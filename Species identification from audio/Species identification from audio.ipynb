{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70203,"databundleVersionId":8068726,"sourceType":"competition"},{"sourceId":8108072,"sourceType":"datasetVersion","datasetId":4789213},{"sourceId":8319412,"sourceType":"datasetVersion","datasetId":4941521},{"sourceId":8478505,"sourceType":"datasetVersion","datasetId":5056677}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":72.272366,"end_time":"2024-05-27T08:24:12.863444","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-27T08:23:00.591078","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"087aee61943640ff866fad504d3bb8b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2412ab76a24d443693ab5c99798255d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"3051ba21b6bb4960912b45a267d7954e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b524d1526854b07a9cbd17a164389ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3051ba21b6bb4960912b45a267d7954e","placeholder":"​","style":"IPY_MODEL_087aee61943640ff866fad504d3bb8b7","value":""}},"603297b6cf5a4516b17db45b449d8bcd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aee52a129532428f97b4ffa466195640","placeholder":"​","style":"IPY_MODEL_98758643ec6f4277946a0b2523b33a62","value":" 0/0 [00:00&lt;?, ?it/s]"}},"7de4c22f5bde4876bd7045b85b99ed48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b524d1526854b07a9cbd17a164389ac","IPY_MODEL_919bacdd323e48c4a80b2ef6b3bdc060","IPY_MODEL_603297b6cf5a4516b17db45b449d8bcd"],"layout":"IPY_MODEL_85db54c088da49578b1a07af47f9565e"}},"85db54c088da49578b1a07af47f9565e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"919bacdd323e48c4a80b2ef6b3bdc060":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2412ab76a24d443693ab5c99798255d6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd98675913fb42bca53f7133abd10dae","value":0}},"98758643ec6f4277946a0b2523b33a62":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aee52a129532428f97b4ffa466195640":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd98675913fb42bca53f7133abd10dae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\"> 1 | Installing Libraries </p>","metadata":{"papermill":{"duration":0.023488,"end_time":"2024-05-27T08:23:03.890039","exception":false,"start_time":"2024-05-27T08:23:03.866551","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\n    !pip install /kaggle/input/onnxruntime/humanfriendly-10.0-py2.py3-none-any.whl --no-index --find-links /kaggle/input/onnxruntime\n    !pip install /kaggle/input/onnxruntime/coloredlogs-15.0.1-py2.py3-none-any.whl --no-index --find-links /kaggle/input/onnxruntime\n    !pip install /kaggle/input/onnxruntime/onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl --no-index --find-links /kaggle/input/onnxruntime","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:23:03.939451Z","iopub.status.busy":"2024-05-27T08:23:03.938996Z","iopub.status.idle":"2024-05-27T08:23:51.840866Z","shell.execute_reply":"2024-05-27T08:23:51.839035Z"},"papermill":{"duration":47.930075,"end_time":"2024-05-27T08:23:51.84393","exception":false,"start_time":"2024-05-27T08:23:03.913855","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">2 | Importing Libraries </p>","metadata":{"papermill":{"duration":0.025691,"end_time":"2024-05-27T08:23:51.894478","exception":false,"start_time":"2024-05-27T08:23:51.868787","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport glob\nimport time\nimport shutil\nimport random\nimport ast\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\nimport onnx\nimport onnxruntime as ort\nimport wandb\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedGroupKFold\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom tqdm.notebook import tqdm\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom torch.cuda import amp\nimport torch\nprint(f\"pytorch version is {torch.__version__}\")\nimport torch.nn as nn\nfrom torch.cuda import amp\n\nisTrain = False\nname = 'bird2024exp1057'\n\nimport torchvision\nfrom torchvision.transforms import v2 as transforms\n\nimport librosa\nimport torchaudio\nimport torchaudio.transforms as audioT\n\nimport timm","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:23:51.946896Z","iopub.status.busy":"2024-05-27T08:23:51.946357Z","iopub.status.idle":"2024-05-27T08:24:05.352159Z","shell.execute_reply":"2024-05-27T08:24:05.350805Z"},"papermill":{"duration":13.435679,"end_time":"2024-05-27T08:24:05.355498","exception":false,"start_time":"2024-05-27T08:23:51.919819","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">3 | Configuration Class </p>\n\nThe `config` class is going to be use  to centralize and manage all the configuration settings needed for the training and evaluation of the model. This approach makes it easier to adjust parameters and ensures consistency across different parts of the code.\n\n```python\nclass config:\n    dir = \"/kaggle/input/birdclef-2024/\"\n```\n\n- **dir**: Specifies the directory where the dataset is located. This allows the code to locate and load the dataset efficiently.\n\n```python\n    wave_path = \"original_waves/second_30/\"\n```\n\n- **wave_path**: The sub-directory within the main directory where the preprocessed audio wave files are stored. This helps in organizing the data and facilitates easy access during data loading.\n\n```python\n    model_name = 'tf_efficientnet_b0'\n```\n\n- **model_name**: The name of the model architecture to be used. Here, `tf_efficientnet_b0` is chosen, which is a variant of EfficientNet. This setting allows for easy switching between different model architectures.\n\n```python\n    pool_type = 'avg'\n```\n\n- **pool_type**: Specifies the type of pooling layer to use, in this case, average pooling. Pooling layers are used to reduce the spatial dimensions of the feature maps.\n\n```python\n    train_duration = 30\n```\n\n- **train_duration**: The duration (in seconds) of the audio samples used for training. This sets a fixed length for all training samples, ensuring uniformity.\n\n```python\n    slice_duration = 5\n```\n\n- **slice_duration**: The duration (in seconds) of the slices used for Short-Time Fourier Transform (STFT) during training. This defines the length of audio segments input to the model.\n\n```python\n    test_duration = 5\n```\n\n- **test_duration**: The duration (in seconds) of the audio samples used for testing. This ensures that the test data is processed in the same way as the training data.\n\n```python\n    train_drop_duration = 1\n```\n\n- **train_drop_duration**: Duration (in seconds) of audio to be randomly dropped out during training for augmentation purposes. This helps in making the model robust to missing data.\n\n### Spectrogram Parameters\n\n```python\n    sr = 32000\n    fmin = 20\n    fmax = 15000\n    n_mels = 128\n    n_fft = n_mels*8\n    size_x = 512\n```\n\n- **sr**: Sample rate, the number of samples per second in the audio files.\n- **fmin**: Minimum frequency for the Mel spectrogram.\n- **fmax**: Maximum frequency for the Mel spectrogram.\n- **n_mels**: Number of Mel bands to generate.\n- **n_fft**: Number of FFT components, calculated as eight times the number of Mel bands.\n- **size_x**: The size of the resulting spectrogram along the time axis.\n\n```python\n    hop_length = int(sr*slice_duration / size_x)\n    test_hop_length = int(sr*test_duration / size_x)\n```\n\n- **hop_length**: Number of audio samples between adjacent frames for the training spectrogram.\n- **test_hop_length**: Number of audio samples between adjacent frames for the testing spectrogram.\n\n### Miscellaneous Configuration\n\n```python\n    bins_per_octave = 12\n```\n\n- **bins_per_octave**: Number of frequency bins per octave in the spectrogram.\n\n```python\n    nfolds = 5\n    inference_folds = [4]\n```\n\n- **nfolds**: Number of folds for cross-validation.\n- **inference_folds**: Specific folds to use for inference.\n\n```python\n    enable_amp = True\n    train_batchsize = 32\n    valid_batchsize = 1\n```\n\n- **enable_amp**: Enables automatic mixed precision to speed up training.\n- **train_batchsize**: Batch size for training.\n- **valid_batchsize**: Batch size for validation.\n\n```python\n    loss_type = \"BCEFocalLoss\"\n```\n\n- **loss_type**: Type of loss function used during training. Here, Binary Cross-Entropy with Focal Loss is used to handle class imbalance.\n\n```python\n    lr = 1.0e-03\n    optimizer = 'adan'\n    weight_decay = 1.0e-02\n    es_patience = 5\n    deterministic = True\n    enable_amp = True\n```\n\n- **lr**: Learning rate for the optimizer.\n- **optimizer**: Optimizer type, here 'adan' is chosen.\n- **weight_decay**: Weight decay for regularization.\n- **es_patience**: Early stopping patience, the number of epochs with no improvement after which training will be stopped.\n- **deterministic**: Ensures deterministic behavior for reproducibility.\n- **enable_amp**: (Repeated) Enables automatic mixed precision to speed up training.\n\n```python\n    max_epoch = 9\n    aug_epoch = 6\n```\n\n- **max_epoch**: Maximum number of epochs for training.\n- **aug_epoch**: Number of epochs for data augmentation.\n\n### Secondary Label Handling\n\n```python\n    useSecondary = True\n    secondary_label_value = 0.5\n```\n\n- **useSecondary**: Whether to use secondary labels in training.\n- **secondary_label_value**: Value assigned to secondary labels during training.\n\n### Oversampling\n\n```python\n    oversample = False\n    oversample_threthold = 60\n```\n\n- **oversample**: Whether to oversample minority classes.\n- **oversample_threthold**: Threshold for oversampling.\n\n### Random Seed and Logging\n\n```python\n    seed = 42\n    wandb = True\n```\n\n- **seed**: Seed for random number generators to ensure reproducibility.\n- **wandb**: Whether to use Weights and Biases for experiment tracking.\n\n### Data Augmentation Flags\n\n```python\n    aug_noise = 0.\n    aug_gain = 0.0\n    aug_wave_pitchshift = 0.0\n    aug_wave_shift = 0.\n    aug_spec_xymasking = 0.\n    aug_spec_coarsedrop = 0.\n    aug_spec_hflip = 0.\n```\n\n- **aug_noise, aug_gain, aug_wave_pitchshift, aug_wave_shift**: Flags for different types of wave augmentations.\n- **aug_spec_xymasking, aug_spec_coarsedrop, aug_spec_hflip**: Flags for different types of spectrogram augmentations.\n\n### Mixup Parameters\n\n```python\n    aug_wave_mixup = 1.0\n    aug_spec_mixup = 0.0\n    aug_spec_mixup_prob = 0.5\n    alpha = 0.95\n```\n\n- **aug_wave_mixup**: Flag for wave mixup augmentation.\n- **aug_spec_mixup**: Flag for spectrogram mixup augmentation.\n- **aug_spec_mixup_prob**: Probability of applying spectrogram mixup.\n- **alpha**: Parameter for the Beta distribution used in mixup.\n\n```python\n    smoothing_value = 0.0\n```\n\n- **smoothing_value**: Label smoothing value.\n\n### Initializing Configuration and Device\n\n```python\ncfg = config()\n```\n\n- **cfg**: Instantiates the configuration class, making all settings accessible through `cfg`.\n\n```python\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\n```\n\n- **device**: Sets the device for computation to GPU if available, otherwise CPU. This ensures that the code can leverage GPU acceleration if available.\n- **print(device)**: Prints the selected device to verify the computation environment.\n\n","metadata":{"papermill":{"duration":0.026074,"end_time":"2024-05-27T08:24:05.406693","exception":false,"start_time":"2024-05-27T08:24:05.380619","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"---","metadata":{"papermill":{"duration":0.024724,"end_time":"2024-05-27T08:24:05.457874","exception":false,"start_time":"2024-05-27T08:24:05.43315","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### **Key Concepts and Terms of CFG class**\n\n#### Audio Processing\n\n1. **Sample Rate (sr)**:\n   - **Definition**: The number of samples of audio carried per second, measured in Hz (samples per second).\n   - **In Context**: `sr = 32000` means the audio is sampled at 32,000 samples per second.\n\n2. **Waveform**:\n   - **Definition**: A graphical representation of the audio signal's amplitude over time.\n   - **In Context**: The raw audio data before any processing like creating spectrograms.\n\n3. **Spectrogram**:\n   - **Definition**: A visual representation of the spectrum of frequencies in a signal as it varies with time.\n   - **In Context**: Used to convert audio signals into a format that can be fed into convolutional neural networks (CNNs).\n\n4. **Short-Time Fourier Transform (STFT)**:\n   - **Definition**: A method to analyze the frequency content of a signal by dividing it into short segments and performing Fourier transform on each segment.\n   - **In Context**: The `slice_duration` and `hop_length` parameters are used in STFT to create segments of audio and overlap them to analyze the frequency content.\n\n5. **Mel Spectrogram**:\n   - **Definition**: A spectrogram where the frequency axis is converted to the Mel scale, which is more closely aligned with human hearing.\n   - **In Context**: Parameters like `n_mels`, `fmin`, and `fmax` are used to create Mel spectrograms.\n\n#### Spectrogram Parameters\n\n1. **fmin and fmax**:\n   - **Definition**: The minimum and maximum frequencies for the Mel spectrogram.\n   - **In Context**: `fmin = 20` and `fmax = 15000` define the frequency range of interest for the Mel spectrogram.\n\n2. **n_mels**:\n   - **Definition**: The number of Mel bands to generate in the spectrogram.\n   - **In Context**: `n_mels = 128` means the spectrogram will have 128 Mel frequency bands.\n\n3. **n_fft**:\n   - **Definition**: The number of data points used in each block for the FFT (Fast Fourier Transform).\n   - **In Context**: `n_fft = n_mels * 8` means each block will use 1024 points.\n\n4. **hop_length**:\n   - **Definition**: The number of samples between successive frames.\n   - **In Context**: Determines the overlap between adjacent STFT segments, calculated based on `slice_duration` and `size_x`.\n\n5. **bins_per_octave**:\n   - **Definition**: Number of frequency bins per octave in a spectrogram.\n   - **In Context**: `bins_per_octave = 12` is often used in music and audio analysis.\n\n#### Training Parameters\n\n1. **train_duration and test_duration**:\n   - **Definition**: Duration of the audio clips used for training and testing, respectively.\n   - **In Context**: Sets a fixed length for audio samples to ensure uniformity during training and testing.\n\n2. **train_drop_duration**:\n   - **Definition**: Duration of audio to be randomly dropped during training for data augmentation.\n   - **In Context**: `train_drop_duration = 1` indicates dropping 1 second of audio to make the model robust to missing data.\n\n3. **nfolds and inference_folds**:\n   - **Definition**: Number of folds for cross-validation and specific folds used for inference.\n   - **In Context**: `nfolds = 5` and `inference_folds = [4]` specify a 5-fold cross-validation with the fourth fold used for inference.\n\n4. **train_batchsize and valid_batchsize**:\n   - **Definition**: Number of samples per batch during training and validation.\n   - **In Context**: `train_batchsize = 32` and `valid_batchsize = 1` set the batch sizes for these phases.\n\n5. **loss_type**:\n   - **Definition**: The type of loss function used during training.\n   - **In Context**: `loss_type = \"BCEFocalLoss\"` is chosen to handle class imbalance.\n\n6. **lr (learning rate)**:\n   - **Definition**: The step size at each iteration while moving toward a minimum of the loss function.\n   - **In Context**: `lr = 1.0e-03` is the learning rate for the optimizer.\n\n7. **optimizer**:\n   - **Definition**: The algorithm used to adjust the weights of the network.\n   - **In Context**: `optimizer = 'adan'` specifies the type of optimizer used.\n\n8. **weight_decay**:\n   - **Definition**: A regularization technique to reduce overfitting by penalizing large weights.\n   - **In Context**: `weight_decay = 1.0e-02` sets the strength of the penalty.\n\n9. **es_patience**:\n   - **Definition**: Number of epochs with no improvement after which training will be stopped early.\n   - **In Context**: `es_patience = 5` means the training will stop if there is no improvement for 5 consecutive epochs.\n\n10. **max_epoch and aug_epoch**:\n    - **Definition**: Maximum number of epochs for training and number of epochs with augmentation.\n    - **In Context**: `max_epoch = 9` and `aug_epoch = 6` define the training schedule.\n\n#### **Data Augmentation**\n\n1. **Augmentation Flags**:\n   - **Definition**: Various flags that control different types of data augmentation techniques.\n   - **In Context**: Flags like `aug_noise`, `aug_gain`, `aug_wave_pitchshift`, etc., control the types and extents of augmentations applied to the audio data.\n\n2. **Mixup Parameters**:\n   - **Definition**: Parameters for mixup data augmentation technique.\n   - **In Context**: `aug_wave_mixup = 1.0` and `aug_spec_mixup = 0.0` control the application of mixup on waveforms and spectrograms.\n\n#### **Label Handling and Logging**\n\n1. **useSecondary and secondary_label_value**:\n   - **Definition**: Whether to use secondary labels and their assigned value.\n   - **In Context**: `useSecondary = True` and `secondary_label_value = 0.5` handle secondary labels during training.\n\n2. **wandb**:\n   - **Definition**: Flag to enable or disable Weights and Biases for experiment tracking.\n   - **In Context**: `wandb = True` enables logging of training runs and results.\n\n3. **seed**:\n   - **Definition**: Random seed to ensure reproducibility.\n   - **In Context**: `seed = 42` ensures consistent results across different runs.\n\n","metadata":{"papermill":{"duration":0.025133,"end_time":"2024-05-27T08:24:05.508027","exception":false,"start_time":"2024-05-27T08:24:05.482894","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class config:\n    dir = \"/kaggle/input/birdclef-2024/\"\n\n\n    wave_path = \"original_waves/second_30/\"\n\n    model_name = 'tf_efficientnet_b0'\n\n    pool_type = 'avg'\n\n    \n    train_duration = 30 \n    slice_duration = 5 \n\n    test_duration = 5\n\n    train_drop_duration = 1\n    \n    # spectrogram parameters\n    sr = 32000\n    fmin = 20\n    fmax = 15000\n\n    n_mels = 128\n    n_fft = n_mels*8\n    size_x = 512\n    \n    hop_length = int(sr*slice_duration / size_x)\n    test_hop_length = int(sr*test_duration / size_x)\n    \n    bins_per_octave = 12\n\n    nfolds = 5\n    inference_folds = [4]\n    \n    enable_amp = True\n    train_batchsize = 32\n    valid_batchsize = 1\n\n    # loss_type = \"BCEWithLogitsLoss\"\n    loss_type = \"BCEFocalLoss\"\n\n    lr = 1.0e-04 \n\n\n    optimizer='adan'\n    weight_decay = 1.0e-02\n    es_patience =  5\n    deterministic = True\n    enable_amp = True\n\n    max_epoch = 9\n    aug_epoch = 6\n    \n\n    useSecondary =True\n    secondary_label_value = 0.5\n    oversample =False\n    oversample_threthold = 60\n    \n    seed = 42\n\n    wandb = True\n\n    ###augmentation flags\n    aug_noise            = 0.\n    aug_gain             = 0.0\n    aug_wave_pitchshift  = 0.0\n    aug_wave_shift       = 0.\n\n    aug_spec_xymasking   = 0.\n    aug_spec_coarsedrop  = 0.\n    aug_spec_hflip       = 0.\n\n    ##mixup param\n    aug_wave_mixup       = 1.0\n    aug_spec_mixup       = 0.0\n    aug_spec_mixup_prob  = 0.5 \n    alpha=0.95\n\n    smoothing_value      = 0.0\n    # spec_mix_mask_percent = 20\n    \ncfg = config()\n\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n\nprint(device)","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:05.562829Z","iopub.status.busy":"2024-05-27T08:24:05.562197Z","iopub.status.idle":"2024-05-27T08:24:05.576651Z","shell.execute_reply":"2024-05-27T08:24:05.575294Z"},"papermill":{"duration":0.045294,"end_time":"2024-05-27T08:24:05.579365","exception":false,"start_time":"2024-05-27T08:24:05.534071","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">4 | Data Augmentation Pipeline For Training </p>","metadata":{"papermill":{"duration":0.024909,"end_time":"2024-05-27T08:24:05.629426","exception":false,"start_time":"2024-05-27T08:24:05.604517","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"\n\n### Normal Augmentation\n\nAugmentations applied directly to the audio waveform.\n\n#### `Compose`\n- Combines multiple augmentation transformations into a single pipeline. Each transformation in the list is applied in sequence.\n\n#### `OneOf`\n- Randomly selects one of the provided augmentations to apply. This is useful for introducing variety without overloading the data with too many augmentations at once.\n\n#### `Gain` and `GainTransition`\n- `Gain`: Changes the volume of the audio by a random amount between -15 dB and 15 dB.\n- `GainTransition`: Changes the volume gradually over a duration, making the audio louder or quieter over time.\n- `p=cfg.aug_gain`: Probability of applying the gain augmentation, controlled by `cfg.aug_gain`.\n\n#### `AddGaussianNoise` and `AddColorNoise`\n- `AddGaussianNoise`: Adds random Gaussian noise to the audio.\n- `AddColorNoise`: Adds colored noise with specific signal-to-noise ratio (SNR) and decay properties.\n- `p=cfg.aug_noise`: Probability of applying noise augmentation, controlled by `cfg.aug_noise`.\n\n#### `PitchShift`\n- Changes the pitch of the audio by a random amount between -1 and 1 semitones.\n- `p=cfg.aug_wave_pitchshift`: Probability of applying pitch shift, controlled by `cfg.aug_wave_pitchshift`.\n\n#### `Shift`\n- Shifts the audio in time, effectively moving it forward or backward.\n- `p=cfg.aug_wave_shift`: Probability of applying time shift, controlled by `cfg.aug_wave_shift`.\n\n### Albumentations Transform\nAugmentations that are applied to the spectrograms, which are visual representations of the audio.\n\n#### `albumentations.XYMasking`\n- Randomly masks parts of the spectrogram, blocking out sections in both time and frequency dimensions.\n- `num_masks_x=2`, `num_masks_y=1`: Number of masks in x (time) and y (frequency) directions.\n- `mask_x_length`, `mask_y_length`: Size of the masks.\n- `p=cfg.aug_spec_xymasking`: Probability of applying XY masking, controlled by `cfg.aug_spec_xymasking`.\n\n#### `albumentations.CoarseDropout`\n- Randomly drops larger sections of the spectrogram, simulating occlusions or missing data.\n- `min_holes=20`, `max_holes=50`: Number of holes to drop in the spectrogram.\n- `p=cfg.aug_spec_coarsedrop`: Probability of applying coarse dropout, controlled by `cfg.aug_spec_coarsedrop`.\n\n#### `albumentations.HorizontalFlip`\n- Flips the spectrogram horizontally. This can help the model learn features invariant to time reversal.\n- `p=cfg.aug_spec_hflip`: Probability of applying horizontal flip, controlled by `cfg.aug_spec_hflip`.\n\n### Compose Albumentations\n```python\nalbumentations_augment = albumentations.Compose(alb_transform)\n```\nCombines the defined spectrogram augmentations into a single pipeline, so they can be applied in sequence.\n\n### Summary\n- **Purpose**: These augmentations are applied to the training data to increase its diversity and help the model generalize better to unseen data.\n- **Waveform Augmentations**: Applied directly to the raw audio signal to introduce variations in volume, noise, pitch, and timing.\n- **Spectrogram Augmentations**: Applied to the visual representation of the audio to simulate occlusions and learn invariant features.\n\n","metadata":{"papermill":{"duration":0.024953,"end_time":"2024-05-27T08:24:05.680616","exception":false,"start_time":"2024-05-27T08:24:05.655663","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if isTrain== True:\n\n    normal_augment = Compose([\n        OneOf([\n            Gain(min_gain_in_db=-15, max_gain_in_db=15, p=1.0),\n            GainTransition(min_gain_in_db=-24.0, max_gain_in_db=6.0,\n                           min_duration=0.2, max_duration=6.0,  p=1.0)\n        ], p=cfg.aug_gain),\n        \n        OneOf([\n            AddGaussianNoise(p=1),\n            AddColorNoise(p=1, min_snr_db=5, max_snr_db=20, min_f_decay=-3.01, max_f_decay=-3.01)\n        ],p=cfg.aug_noise),\n\n    \n        PitchShift(min_semitones=-1, max_semitones=1, p=cfg.aug_wave_pitchshift),\n        Shift(p=cfg.aug_wave_shift)\n    ])\n    alb_transform = [\n        albumentations.XYMasking(num_masks_x=2, num_masks_y=1, \n                                 mask_x_length=cfg.size_x//30, mask_y_length=cfg.n_mels//30,\n                                 fill_value=0, mask_fill_value=0, p=cfg.aug_spec_xymasking),\n        albumentations.CoarseDropout(fill_value=0, min_holes=20, max_holes=50, p=cfg.aug_spec_coarsedrop),\n        albumentations.HorizontalFlip(p=cfg.aug_spec_hflip)    \n    ]\n    albumentations_augment = albumentations.Compose(alb_transform)","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:05.733294Z","iopub.status.busy":"2024-05-27T08:24:05.732908Z","iopub.status.idle":"2024-05-27T08:24:05.743951Z","shell.execute_reply":"2024-05-27T08:24:05.742793Z"},"papermill":{"duration":0.041072,"end_time":"2024-05-27T08:24:05.746803","exception":false,"start_time":"2024-05-27T08:24:05.705731","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">5 |  Mixup Data Augmentation Function </p>\n\n\n### Function: `mixup`\n\n**Purpose**:  \nThe `mixup` function is used for data augmentation. It combines two examples in the dataset to create a new example. This technique helps the model generalize better by providing it with more varied training examples.\n\n#### Parameters:\n- `data`: The input data (usually a batch of samples).\n- `targets`: The corresponding labels for the input data.\n- `alpha`: A parameter for the Beta distribution, controlling the degree of interpolation between examples.\n- `mode`: Specifies how to select the data for mixing. Options are \"same_wave\" or \"other_wave\".\n---\n#### Concepts:\n\n1. **Mixup**: \n   - Mixup is a data augmentation technique where two samples and their labels are linearly interpolated to create a new sample and label.\n   - This helps improve the robustness of the model by preventing it from overfitting to the original training data.\n\n2. **Beta Distribution**:\n   - `np.random.beta(alpha, alpha)`: Generates a lambda (λ) value from the Beta distribution. This λ determines the mix ratio between the two samples.\n   - A higher `alpha` means samples will be more similar to the original, while a lower `alpha` will result in more variation.\n\n3. **Interpolation**:\n   - The new data is created by interpolating between the original and shuffled data using the lambda (λ) value.\n\n---\n\n### Code Explanation:\n\n```python\ndef mixup(data, targets, alpha, mode=\"same_wave\"):\n```\n- Defines the function with parameters `data`, `targets`, `alpha`, and `mode`.\n\n```python\n    if mode == \"same_wave\":\n        data = torch.tensor(data)\n        indices = torch.randperm(data.size(0))\n        shuffled_data = data[indices]\n\n        lam = np.random.beta(alpha, alpha)\n        new_data = data * lam + shuffled_data * (1 - lam)\n        return new_data.numpy()\n```\n- **Mode: \"same_wave\"**:\n  - Converts `data` to a tensor.\n  - `torch.randperm(data.size(0))` generates a random permutation of indices.\n  - `shuffled_data = data[indices]` shuffles the data based on the random indices.\n  - `lam = np.random.beta(alpha, alpha)` generates a lambda value.\n  - `new_data = data * lam + shuffled_data * (1 - lam)` creates the new mixed data by combining the original and shuffled data.\n  - Returns the new mixed data as a numpy array.\n\n```python\n    elif mode == \"other_wave\":\n        indices = torch.randperm(data.size(0))\n        shuffled_data = data[indices]\n        shuffled_targets = targets[indices]\n    \n        lam = np.random.beta(alpha, alpha)\n        new_data = data * lam + shuffled_data * (1 - lam)\n        new_targets = targets * lam + shuffled_targets * (1 - lam)\n    \n        return new_data, new_targets\n```\n- **Mode: \"other_wave\"**:\n  - `indices = torch.randperm(data.size(0))` generates a random permutation of indices.\n  - `shuffled_data = data[indices]` and `shuffled_targets = targets[indices]` shuffle the data and targets.\n  - `lam = np.random.beta(alpha, alpha)` generates a lambda value.\n  - `new_data = data * lam + shuffled_data * (1 - lam)` and `new_targets = targets * lam + shuffled_targets * (1 - lam)` create new mixed data and labels.\n  - Returns the new mixed data and labels.\n\n","metadata":{"papermill":{"duration":0.024922,"end_time":"2024-05-27T08:24:05.797788","exception":false,"start_time":"2024-05-27T08:24:05.772866","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def mixup(data, targets, alpha, mode=\"same_wave\"):\n    \n    if mode == \"same_wave\":\n        data = torch.tensor(data)\n        indices = torch.randperm(data.size(0))\n        shuffled_data = data[indices]\n\n        lam = np.random.beta(alpha, alpha)\n        new_data = data * lam + shuffled_data * (1 - lam)\n        return new_data.numpy()\n        \n    elif mode == \"other_wave\":\n        indices = torch.randperm(data.size(0))\n        shuffled_data = data[indices]\n        shuffled_targets = targets[indices]\n    \n        lam = np.random.beta(alpha, alpha)\n        new_data = data * lam + shuffled_data * (1 - lam)\n        new_targets = targets * lam + shuffled_targets * (1 - lam)\n    \n        return new_data, new_targets","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:05.852511Z","iopub.status.busy":"2024-05-27T08:24:05.852046Z","iopub.status.idle":"2024-05-27T08:24:05.861938Z","shell.execute_reply":"2024-05-27T08:24:05.860506Z"},"papermill":{"duration":0.040647,"end_time":"2024-05-27T08:24:05.864611","exception":false,"start_time":"2024-05-27T08:24:05.823964","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">6 | Spectral Mixup Function</p>\n\n\n1. **SpecXYMasking**:\n   - `spec_xymasking` is an instance of the `XYMasking` transformation from the Albumentations library, which applies masking to an image.\n   - The parameters specify the number of masks along the x and y axes, the lengths of these masks, and the fill values for the mask and the image.\n   - This transformation is applied with a probability of 1 (i.e., always) during data augmentation.\n\n2. **Function `spec_mixup`**:\n   - This function takes `data` (input spectrogram data) and `targets` (corresponding labels).\n   - `type = data.dtype`: Stores the data type of the input data.\n   - `indices = torch.randperm(data.size(0))`: Generates random permutations of indices.\n   - `shuffled_data` and `shuffled_targets` are created by shuffling `data` and `targets` based on the generated indices.\n   - `data_transposed` is created by transposing the input data to a different shape suitable for applying `spec_xymasking`.\n   - `diff` calculates the difference between the original data and the transposed data.\n   - `mask` is created to identify the differences between the original and transposed data.\n   - `shuffled_data_masked` applies the mask to the shuffled data.\n   - `new_data` combines the transposed data and the masked shuffled data.\n   - `lam` calculates the mix ratio based on the number of non-zero elements in the mask.\n   - `new_targets` are calculated by mixing the original targets with the shuffled targets based on the mix ratio.\n\n3. **Return**:\n   - The function returns `new_data` and `new_targets`.\n\n","metadata":{"papermill":{"duration":0.024581,"end_time":"2024-05-27T08:24:05.914281","exception":false,"start_time":"2024-05-27T08:24:05.8897","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"---","metadata":{"papermill":{"duration":0.027032,"end_time":"2024-05-27T08:24:05.966477","exception":false,"start_time":"2024-05-27T08:24:05.939445","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### **Spectral mixup**\n\nA data augmentation technique commonly used in the field of audio signal processing, especially in tasks such as audio classification or sound event detection. This technique is an extension of the original \"mixup\" technique, which was initially proposed for image classification tasks.\n\nIn the context of audio data, spectral mixup involves mixing two or more audio signals' spectrograms to create new synthetic examples. Here's how it works:\n\n1. **Spectrogram Generation**:\n   - Audio signals are converted into spectrograms, which are visual representations of the audio signal's frequency content over time. Spectrograms are created using techniques such as Short-Time Fourier Transform (STFT).\n\n2. **Mixing Spectrograms**:\n   - Spectrograms from different audio samples are linearly combined. This combination involves taking a weighted sum of the spectrograms, where the weights are randomly sampled from a beta distribution.\n   - The purpose of mixing spectrograms is to create new training examples that lie on the line segment connecting two original spectrograms in the feature space.\n\n3. **Label Mixing**:\n   - Along with mixing spectrograms, the corresponding labels (or target outputs) are also mixed based on the same weights used for mixing the spectrograms. This ensures that the labels for the synthetic examples are also a linear combination of the original labels.\n\n4. **Training**:\n   - The synthetic spectrograms and their mixed labels are then used for training a neural network model.\n   - During training, the model learns from both the original and synthetic examples, effectively expanding the diversity of the training data and improving the model's ability to generalize to unseen data.\n\nSpectral mixup helps in regularizing the model and reducing overfitting by providing a smoother and more continuous distribution of training examples in the feature space. It encourages the model to learn more robust and generalized representations of the input data, leading to improved performance on unseen data.","metadata":{"papermill":{"duration":0.02476,"end_time":"2024-05-27T08:24:06.016507","exception":false,"start_time":"2024-05-27T08:24:05.991747","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if isTrain== True:\n    spec_xymasking = albumentations.XYMasking(num_masks_x=2, num_masks_y=1, \n                                              mask_x_length=cfg.size_x // 10, mask_y_length=cfg.n_mels // 10,\n                                              fill_value=0, mask_fill_value=0, p=1)\n\ndef spec_mixup(data, targets):\n    type = data.dtype\n\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_targets = targets[indices]\n\n    data = np.array(data)\n    data_transposed = np.transpose(data, (2, 3, 1, 0))\n    data_transposed = spec_xymasking(image=data_transposed)[\"image\"]\n    data_transposed = np.transpose(data_transposed, (3, 2, 0, 1))  \n\n    diff = data - data_transposed\n    mask = (diff != 0).astype(int)\n\n    shuffled_data_masked = (shuffled_data * mask)\n\n    new_data = torch.tensor(data_transposed, dtype=type) + torch.tensor(shuffled_data_masked, dtype=type)\n\n    lam = mask.sum() / len(data) / (cfg.n_mels*cfg.size_x)\n    new_targets = targets * (1-lam) + shuffled_targets *lam\n\n    return new_data, new_targets","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:06.070029Z","iopub.status.busy":"2024-05-27T08:24:06.069594Z","iopub.status.idle":"2024-05-27T08:24:06.081403Z","shell.execute_reply":"2024-05-27T08:24:06.08004Z"},"papermill":{"duration":0.042092,"end_time":"2024-05-27T08:24:06.084224","exception":false,"start_time":"2024-05-27T08:24:06.042132","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">7 | Mel Spectrogram Generation </p>\n\n#### Purpose:\nThis cell is all about creating Mel spectrograms from audio data. Mel spectrograms are like pictures of sound, and they're crucial for training and testing models to deal with audio, like recognizing bird sounds.\n\n#### Explanation:\n1. **spec_layer**: Think of this as the tool we use to make Mel spectrograms during training. It's like having a special camera that takes pictures of sound waves. We set it up with details like how fast it should take pictures (sample rate), how often it should take them (hop length), and how clear the pictures should be (number of FFT points and Mel bins). We also tell it the lowest and highest sounds it should pay attention to. Then, we put this camera on the device we want to use, like a computer's brain (GPU) if it's strong or just the regular computer (CPU) if it's not.\n\n2. **valid_spec_layer**: This is similar to the first one, but we use it when we're checking how good our model is during validation. It's like having another camera, but this one takes pictures a bit differently to save time. It's still useful for making sure our model is learning well.\n\n3. **test_spec_layer**: This is yet another camera, but we use it when we're done training and want to see how well our model works on new sounds. We put this camera on the regular computer (CPU) because we don't need it to be super fast for this part.\n\n#### Concepts:\n- **Mel Spectrogram**: It's like a photo album of sound waves, showing how loud different pitches are over time.\n- **torchaudio.transforms.MelSpectrogram**: It's like a magic tool that turns sound into Mel spectrograms.\n- **Sample Rate**: How quickly the magic tool takes pictures of sound.\n- **Hop Length**: How often the magic tool takes pictures, kind of like frames in a movie.\n- **FFT (Fast Fourier Transform) Points**: The clearer the pictures, the better we can see details in the sound waves.\n- **Mel Bins**: Imagine the magic tool dividing sound into buckets to see how much sound there is at different pitches.\n- **Minimum and Maximum Frequency**: The lowest and highest pitches the magic tool pays attention to.\n- **Mel Scale**: It's like adjusting the colors in the photo album to match how humans hear sound.\n- **Centering and Padding**: Tricks to make sure the pictures look nice, even at the edges.\n","metadata":{"papermill":{"duration":0.02796,"end_time":"2024-05-27T08:24:06.137985","exception":false,"start_time":"2024-05-27T08:24:06.110025","status":"completed"},"tags":[]}},{"cell_type":"code","source":"spec_layer = torchaudio.transforms.MelSpectrogram(\n    sample_rate=cfg.sr, hop_length=cfg.hop_length, n_fft=cfg.n_fft,\n    n_mels=cfg.n_mels,f_min=cfg.fmin,f_max=cfg.fmax,mel_scale='slaney',center=True, pad_mode='reflect'\n).to(device)\n\nvalid_spec_layer = torchaudio.transforms.MelSpectrogram(\n    sample_rate=cfg.sr, hop_length=cfg.test_hop_length, n_fft=cfg.n_fft,\n    n_mels=cfg.n_mels,f_min=cfg.fmin,f_max=cfg.fmax,mel_scale='slaney',center=True, pad_mode='reflect'\n).to(device)\n\ntest_spec_layer = torchaudio.transforms.MelSpectrogram(\n    sample_rate=cfg.sr, hop_length=cfg.test_hop_length, n_fft=cfg.n_fft,\n    n_mels=cfg.n_mels,f_min=cfg.fmin,f_max=cfg.fmax,mel_scale='slaney',center=True, pad_mode='reflect'\n).cpu()","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:06.191739Z","iopub.status.busy":"2024-05-27T08:24:06.191261Z","iopub.status.idle":"2024-05-27T08:24:06.342328Z","shell.execute_reply":"2024-05-27T08:24:06.34098Z"},"papermill":{"duration":0.181583,"end_time":"2024-05-27T08:24:06.34538","exception":false,"start_time":"2024-05-27T08:24:06.163797","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">8 | Data Preparation Steps</p>\n\n#### 1. Reading Sample Submission Data:\n   - **Purpose**: Understanding submission file format.\n   - **Action**: Reads the sample submission file.\n  \n\n#### 2. Defining Labels:\n   - **Purpose**: Storing label information.\n   - **Action**: Extracts column names from the sample submission file.\n  \n\n#### 3. Reading Training Metadata:\n   - **Purpose**: Understanding training data details.\n   - **Action**: Reads the training metadata file.\n \n\n#### 4. Creating New Target Column:\n   - **Purpose**: Preparing target variable for training.\n   - **Action**: Combines primary and secondary labels into a single column.\n\n\n#### 5. Calculating Length of New Target:\n   - **Purpose**: Analyzing label distribution.\n   - **Action**: Counts the number of labels per sample.\n \n\n#### 6. Visualizing Label Distribution:\n   - **Purpose**: Visualizing label distribution for analysis.\n   - **Action**: Generates a bar plot of label distribution.\n  \n\n#### 7. Cleaning Duplicated Filenames:\n   - **Purpose**: Ensuring data integrity.\n   - **Action**: Identifies and removes rows with duplicated filenames.\n   - **Steps**:\n     - Extracts filenames from paths and removes file extensions.\n     - Identifies duplicated filenames.\n     - Removes rows with duplicated filenames.\n  \n\n#### 8. Resetting DataFrame Index:\n   - **Purpose**: Ensuring sequential index after data manipulation.\n   - **Action**: Resets the index of the DataFrame.\n \n","metadata":{"papermill":{"duration":0.024773,"end_time":"2024-05-27T08:24:06.395921","exception":false,"start_time":"2024-05-27T08:24:06.371148","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sample_submission = pd.read_csv(cfg.dir+\"sample_submission.csv\")\nLABELS = list(sample_submission.set_index(\"row_id\").columns)\nLABELS[:5]\ntrain_csv = pd.read_csv(cfg.dir+\"train_metadata.csv\")\ntrain_csv['new_target'] = train_csv['primary_label'] + ' ' + train_csv['secondary_labels'].map(lambda x: ' '.join(ast.literal_eval(x)))\ntrain_csv['len_new_target'] =train_csv['new_target'].map(lambda x: len(x.split()))\ntrain_csv[\"len_new_target\"].value_counts().plot(kind=\"bar\", figsize=(4,2))\ntrain_csv[\"filename_tmp\"] = train_csv[\"filename\"].map(lambda x:x.split(\"/\")[1][:-4])\nduplicated_filenames = train_csv[\"filename_tmp\"].value_counts()[train_csv[\"filename_tmp\"].value_counts() > 1].index\ntrain_csv = train_csv[~train_csv[\"filename_tmp\"].isin(duplicated_filenames)]\ntrain_csv = train_csv.reset_index(drop=True)","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:06.513905Z","iopub.status.busy":"2024-05-27T08:24:06.513429Z","iopub.status.idle":"2024-05-27T08:24:07.428826Z","shell.execute_reply":"2024-05-27T08:24:07.427161Z"},"papermill":{"duration":0.945215,"end_time":"2024-05-27T08:24:07.431523","exception":false,"start_time":"2024-05-27T08:24:06.486308","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\"> 9 | BirdCLEF Dataset Preparation </p>\n\n#### Purpose:\nPrepare the BirdCLEF dataset for training, validation, and testing purposes. It involves defining a custom dataset class, performing necessary data preprocessing steps, and generating Mel spectrograms from audio recordings.\n\n#### Explanation:\n\n\n1. **`__init__` method**:\n    - This method initializes the dataset instance. It takes in a DataFrame (`df`), a boolean flag for augmentation (`augmentation`), and a mode (`mode`). The DataFrame likely contains information about the dataset, such as file paths and labels. The mode determines whether the dataset is used for training, validation, testing, or cleaning.\n\n2. **`__len__` method**:\n    - This method returns the total number of samples in the dataset.\n\n3. **`normalize` method**:\n    - This method normalizes the input data. It replaces `-inf` values with the mean of valid values, then scales the data between 0 and 1.\n\n4. **`wave_tile_and_cutoff` method**:\n    - This method preprocesses the audio data. It adjusts the length of the audio data to match the desired duration for training by either truncating or tiling the data.\n\n5. **`label_smoothing` method**:\n    - This method applies label smoothing to the target labels. It generates smoothed labels to reduce overfitting and improve generalization performance.\n\n6. **`__getitem__` method**:\n    - This method retrieves a sample from the dataset based on the provided index (`idx`). It loads audio data, preprocesses it, applies augmentation (if enabled), generates spectrograms, and returns the spectrogram along with its corresponding target label.\n\n    - For training mode:\n        - It reads the audio file, applies augmentation techniques (if enabled), generates spectrograms, and applies label smoothing.\n\n    - For validation mode:\n        - It reads the audio file, generates spectrograms, and returns them along with the target labels.\n\n    - For test mode:\n        - It reads the audio file, generates spectrograms, and returns them.\n\n    - For clean mode:\n        - It reads the audio file, generates spectrograms, and returns them along with the file path.\n\n\n\n#### Concepts:\n- **Custom Dataset Class**: A specialized class tailored for specific dataset handling.\n- **Mel Spectrogram**: A visual representation of sound frequency spectrum used for audio analysis.\n- **Label Smoothing**: Technique to prevent model overconfidence and improve generalization.\n- **Data Preprocessing**: Manipulating data to make it suitable for analysis or model training.\n- **Data Augmentation**: Techniques to increase dataset diversity for better model training.\n- **Indexing**: Accessing specific elements from the dataset based on their index.","metadata":{"papermill":{"duration":0.026002,"end_time":"2024-05-27T08:24:07.483224","exception":false,"start_time":"2024-05-27T08:24:07.457222","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class BirdCLEF_Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, augmentation=False, mode='train'):\n        if mode == 'train':\n            self.df = df.reset_index(drop=True)\n        elif mode == 'valid':\n            self.df = df.reset_index(drop=True)\n        else:\n            self.df = df\n        self.mode = mode\n        self.augmentation = augmentation\n    \n    def __len__(self):\n        return len(self.df)\n\n    def normalize(self, x):\n        valid_values = x[x != float('-inf')]\n        mean_value = np.mean(valid_values)\n        x[x == float('-inf')] = mean_value\n        \n\n        x = x - x.min()\n        x = x / x.max()\n        return x\n\n    def wave_tile_and_cutoff(self, data):\n      \n        drop_duration = cfg.sr*cfg.train_drop_duration\n        use_duration  = cfg.sr*cfg.train_duration\n        \n        if len(data[0]) > drop_duration: \n            data = data[:,drop_duration:]\n\n        if len(data[0]) < use_duration:\n            iter = 1 + (use_duration) // len(data[0])\n            data = np.tile(data, (1, iter))\n\n        data = data[:,:use_duration]\n        return data\n\n    def label_smoothing(self, idx, target):\n    \n        secondary_target = target * cfg.secondary_label_value\n    \n        out_of_target_noise_intensity = cfg.smoothing_value/(len(LABELS)-1) \n        out_of_target_noise_array = torch.ones(target.shape) * out_of_target_noise_intensity\n        \n        secondary_target_with_noise = secondary_target + out_of_target_noise_array\n        secondary_target_with_noise = torch.clip(secondary_target_with_noise, min=0, max=cfg.secondary_label_value)\n    \n        primary_target = np.isin(LABELS, self.df.loc[idx, \"primary_label\"]).astype(int)\n        primary_target = torch.tensor(primary_target, dtype=torch.float32)\n\n        primary_and_secondary_target_with_noise = primary_target + secondary_target_with_noise\n        new_target = torch.clip(primary_and_secondary_target_with_noise, min=0, max=1)\n    \n        new_target = new_target - primary_target * cfg.smoothing_value\n    \n        return new_target\n\n    \n    def __getitem__(self, idx):\n\n        if self.mode == 'train':\n\n          \n            if cfg.useSecondary == True:\n                target = np.isin(LABELS, self.df.loc[idx, \"new_target\"].split()).astype(int)\n            else:\n                target = np.isin(LABELS, self.df.loc[idx, \"primary_label\"].split()).astype(int)\n            target = torch.tensor(target, dtype=torch.float32)\n          \n            target = self.label_smoothing(idx, target)\n            \n            fileID = self.df.loc[idx, 'fileID'] \n            \n            path = f\"{cfg.wave_path}{fileID}.npy\"\n            wave = np.load(path)\n            \n\n      \n            wave = self.wave_tile_and_cutoff(data=wave)\n\n            \n            input_duration = cfg.sr * cfg.slice_duration\n            \n            \n            if self.augmentation == True:\n               \n                if cfg.aug_wave_mixup > np.random.random():\n                    #train_duration -> slice_duration\n                    wave_reshape = wave.reshape(-1, input_duration)\n                    wave = mixup(data=wave_reshape, targets=target, alpha=cfg.alpha, mode=\"same_wave\")\n                    wave = wave[:1,:]\n                else:\n                    wave = wave[:, :input_duration]\n                \n     \n                wave = normal_augment(samples=wave, sample_rate=cfg.sr)\n\n    \n                wave = torch.tensor(wave).to(device)\n                mel_spec = spec_layer(wave)\n                mel_spec = np.array(mel_spec.cpu())\n\n                mel_spec = np.log(mel_spec)\n                for i in range(len(mel_spec)):\n                    mel_spec[i] = self.normalize(mel_spec[i])\n                mel_spec = torch.tensor(mel_spec)\n                mel_spec = mel_spec[:,:,:cfg.size_x]\n\n     \n                mel_spec = np.array(mel_spec.cpu())\n                mel_spec = np.transpose(mel_spec, (1, 2, 0))                \n                mel_spec = albumentations_augment(image=mel_spec)[\"image\"]\n                mel_spec = np.transpose(mel_spec, (2, 0, 1))\n\n\n                \n            else:\n                wave = wave[:, :input_duration]\n                \n                wave = torch.tensor(wave).to(device)\n                mel_spec = spec_layer(wave)\n                mel_spec = np.array(mel_spec.cpu())\n\n                mel_spec = np.log(mel_spec)\n\n                for i in range(len(mel_spec)):\n                    mel_spec[i] = self.normalize(mel_spec[i])\n                    \n\n                mel_spec = torch.tensor(mel_spec)\n                mel_spec = mel_spec[:,:,:cfg.size_x]\n\n            \n            mel_spec = torch.tensor(mel_spec)\n\n            \n            return mel_spec, target\n\n        elif self.mode == 'valid':\n            \n\n            if cfg.useSecondary == True:\n                target = np.isin(LABELS, self.df.loc[idx, \"new_target\"].split()).astype(int)\n            else:\n                target = np.isin(LABELS, self.df.loc[idx, \"primary_target\"].split()).astype(int)\n            target = torch.tensor(target, dtype=torch.float32)\n            \n            fileID = self.df.loc[idx, 'fileID'] \n            \n            path = f\"{cfg.wave_path}{fileID}.npy\"\n            wave = np.load(path)\n\n            wave = self.wave_tile_and_cutoff(data=wave)\n\n            input_duration = cfg.sr*cfg.test_duration\n            wave_reshape = wave.reshape(-1, input_duration)\n\n            wave_reshape = torch.tensor(wave_reshape).to(device)\n            mel_specs = valid_spec_layer(wave_reshape)\n            mel_specs = mel_specs.cpu().numpy()\n\n            mel_specs = np.log(mel_specs)\n            for i in range(len(mel_specs)):\n                mel_specs[i] = self.normalize(mel_specs[i])\n            mel_specs = torch.tensor(mel_specs)\n            \n            mel_specs = mel_specs[:,:,:cfg.size_x]\n\n            targets = torch.tile(target, dims=(mel_specs.shape[0],1))\n            return mel_specs, targets\n\n        elif self.mode == 'test':\n\n            filepath = self.df[idx]\n            wave, _  = torchaudio.load(filepath)\n            wave = wave[:,:60*4*32000]\n\n            wave_reshaped = wave.reshape(-1, 1, cfg.test_duration*cfg.sr)\n            \n            mel_spec = test_spec_layer(wave_reshaped)\n            mel_spec = np.log(mel_spec)\n\n            mel_spec = np.array(mel_spec)\n            for i in range(len(mel_spec)):\n                mel_spec[i] = self.normalize(mel_spec[i])\n            mel_spec = torch.tensor(mel_spec)\n\n            mel_spec = mel_spec[:,:,:cfg.size_x]\n            return mel_spec\n\n        elif self.mode == 'clean':\n\n            filepath = self.df[idx]\n            wave, _  = torchaudio.load(filepath)\n\n            wave = wave[:, :6*cfg.test_duration*cfg.sr]\n\n            chunk_length = len(wave[0]) // (cfg.test_duration*cfg.sr)\n            \n            wave = wave[:,:chunk_length*cfg.test_duration*cfg.sr]\n\n            wave_reshaped = wave.reshape(-1, 1, cfg.test_duration*cfg.sr)\n            \n            mel_spec = test_spec_layer(wave_reshaped)\n            mel_spec = np.log(mel_spec)\n\n            mel_spec = np.array(mel_spec)\n            for i in range(len(mel_spec)):\n                mel_spec[i] = self.normalize(mel_spec[i])\n            mel_spec = torch.tensor(mel_spec)\n\n            return mel_spec, filepath","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:07.538213Z","iopub.status.busy":"2024-05-27T08:24:07.53776Z","iopub.status.idle":"2024-05-27T08:24:07.585211Z","shell.execute_reply":"2024-05-27T08:24:07.583971Z"},"papermill":{"duration":0.078512,"end_time":"2024-05-27T08:24:07.588078","exception":false,"start_time":"2024-05-27T08:24:07.509566","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Explaination**\n\n1. **`if isTrain:`**: \n    - This conditional block ensures that the following code is executed only when the variable `isTrain` is `True`.\n\n2. **`print(\"train data\")`**:\n    - This line simply prints \"train data\" to indicate that the following visualization is related to training data.\n\n3. **`dataset = BirdCLEF_Dataset(df=train_csv, augmentation=True,  mode=\"train\")`**:\n    - This line creates an instance of the `BirdCLEF_Dataset` class for training data. It uses the training DataFrame `train_csv`, enables augmentation, and sets the mode to \"train\".\n\n4. **`data, target = dataset[270]`**:\n    - This line retrieves a specific sample (index 270 in this case) from the dataset. It returns the data (likely a spectrogram) and its corresponding target label.\n\n5. **`fig, ax = plt.subplots(figsize=(6,4))`**:\n    - This line creates a figure and axes object for plotting.\n\n6. **`plt.imshow(data[0], cmap=\"jet\", origin=\"lower\")`**:\n    - This line plots the spectrogram of the first channel of the data using `imshow` function from matplotlib. It sets the colormap to \"jet\" and origin to \"lower\".\n\n7. **`plt.show()`**:\n    - This line displays the plotted spectrogram.\n\n8. **`print(\"validation data\")`**:\n    - This line prints \"validation data\" to indicate that the following visualization is related to validation data.\n\n9. **`dataset = BirdCLEF_Dataset(df=train_csv, augmentation=True,  mode=\"valid\")`**:\n    - This line creates an instance of the `BirdCLEF_Dataset` class for validation data. It uses the same training DataFrame `train_csv`, enables augmentation, and sets the mode to \"valid\".\n\n10. **`data, target = dataset[270]`**:\n    - Similar to before, this line retrieves a specific sample (index 270) from the dataset. It returns the data (likely a spectrogram) and its corresponding target label.\n\n11. **`fig, axes = plt.subplots(figsize=(12,8), nrows=len(data), tight_layout=True)`**:\n    - This line creates a figure and axes object for plotting multiple spectrograms. It sets the figure size, number of rows (equal to the length of `data`), and enables tight layout to prevent overlap.\n\n12. **`for idx, ax in enumerate(axes.ravel()):`**:\n    - This line iterates over the flattened axes array.\n\n13. **`ax.imshow(data[idx], cmap=\"jet\", origin=\"lower\")`**:\n    - Inside the loop, it plots each spectrogram from the `data` array onto the corresponding axis.\n\n","metadata":{"papermill":{"duration":0.025775,"end_time":"2024-05-27T08:24:07.639775","exception":false,"start_time":"2024-05-27T08:24:07.614","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if isTrain:\n    print(\"train data\")\n    dataset = BirdCLEF_Dataset(df=train_csv, augmentation=True,  mode=\"train\")\n    data, target = dataset[270]\n    fig, ax = plt.subplots(figsize=(6,4))\n    plt.imshow(data[0], cmap=\"jet\", origin=\"lower\")\n    plt.show()\n    \n    print(\"validation data\")\n    dataset = BirdCLEF_Dataset(df=train_csv, augmentation=True,  mode=\"valid\")\n    data, target = dataset[270]\n    fig, axes = plt.subplots(figsize=(12,8), nrows=len(data), tight_layout=True)\n    for idx, ax in enumerate(axes.ravel()):\n        ax.imshow(data[idx], cmap=\"jet\", origin=\"lower\")","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:07.69362Z","iopub.status.busy":"2024-05-27T08:24:07.692937Z","iopub.status.idle":"2024-05-27T08:24:07.703011Z","shell.execute_reply":"2024-05-27T08:24:07.701672Z"},"papermill":{"duration":0.040422,"end_time":"2024-05-27T08:24:07.705757","exception":false,"start_time":"2024-05-27T08:24:07.665335","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">10 | BirdModel : Flexible Pooling Architecture For Bird Sound Classification </p>\n\nDefine a PyTorch neural network model called `BirdModel`, which is tailored for bird sound classification tasks. \n\n1. **Initialization**:\n    - The `BirdModel` is initialized with parameters like `model_name`, `pretrained`, `in_channels`, `num_classes`, and `pool`, enabling users to customize the architecture according to their needs.\n    - `model_name`: Specifies the backbone model architecture.\n    - `pretrained`: Boolean flag indicating whether to use pre-trained weights for the backbone.\n    - `in_channels`: Number of input channels.\n    - `num_classes`: Number of output classes.\n    - `pool`: Specifies the pooling strategy (\"default\", \"max\", \"avg\", or \"both\").\n\n2. **Backbone**:\n    - The backbone is selected using `timm.create_model`, allowing the flexibility to choose from various pre-trained models.\n    - Depending on the `pool` parameter, the global pooling behavior of the backbone is customized. If `pool` is set to \"default\", the default global pooling behavior of the backbone is retained; otherwise, the global pooling is overridden with an empty string.\n\n3. **Pooling Layers**:\n    - Three types of pooling layers are employed based on the chosen strategy:\n        - `max_pooling`: Adaptive max pooling followed by flattening, useful for capturing the most salient features.\n        - `avg_pooling`: Adaptive average pooling followed by flattening, useful for capturing overall patterns.\n        - `both_pooling_neck`: A combination of max and average pooling followed by a linear layer, allowing the model to learn complementary representations from both strategies.\n\n4. **Head**:\n    - The head of the model consists of fully connected layers, responsible for mapping the features extracted by the backbone and pooling layers to the output space.\n    - It includes batch normalization, activation functions (e.g., Hardswish), and dropout for regularization, ensuring robustness and preventing overfitting.\n    - The final linear layer outputs logits for each class, with the number of units equal to the number of output classes.\n\n5. **Forward Pass**:\n    - The `forward` method defines the forward pass of the model.\n    - It applies input normalization, passes the input through the backbone model, applies the specified pooling operation (if applicable), and feeds the features to the head.\n    - If pooling is set to \"both\", it combines features extracted by both max and average pooling layers before passing them to the head.\n    - The output of the model is the predicted probabilities for each class, obtained by applying the sigmoid activation function.\n\n","metadata":{"papermill":{"duration":0.025839,"end_time":"2024-05-27T08:24:07.757093","exception":false,"start_time":"2024-05-27T08:24:07.731254","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Some Related concepts\n\n1. **Normalization (`transforms.Normalize`)**:\n   - Normalization is a preprocessing technique used to scale numerical data to a standard range. In this case, it standardizes the input data to have a mean of `[0.485, 0.456, 0.406]` and a standard deviation of `[0.229, 0.224, 0.225]`.\n   - `transforms.Normalize` is a transformation provided by PyTorch's `torchvision.transforms` module.\n\n2. **Backbone Model**:\n   - The backbone model is the core architecture of the neural network responsible for feature extraction.\n   - `timm.create_model` is a function from the `timm` (pytorch-image-models) library used to create various neural network architectures.\n   - `model_name` specifies the name of the backbone model architecture, and `pretrained` indicates whether to use pre-trained weights.\n\n3. **Pooling Layers**:\n   - Pooling layers reduce the spatial dimensions of feature maps while retaining important information.\n   - `torch.nn.AdaptiveMaxPool2d` and `torch.nn.AdaptiveAvgPool2d` are adaptive pooling layers that dynamically adjust their output size based on the input size.\n   - `torch.nn.Flatten` converts multi-dimensional data into a one-dimensional tensor.\n   - `torch.nn.BatchNorm1d` is a batch normalization layer applied to the features before feeding them to the fully connected layers.\n\n4. **Head**:\n   - The head of the model typically consists of fully connected layers responsible for classification.\n   - `torch.nn.Linear` defines a linear transformation from input to output features.\n   - `torch.nn.Hardswish` is an activation function that is a smooth approximation of the ReLU function.\n   - `torch.nn.Dropout` applies dropout regularization to prevent overfitting by randomly dropping input units during training.\n\n5. **Forward Pass (`forward` method)**:\n   - The `forward` method defines how input data flows through the neural network layers during inference.\n   - It applies input normalization, passes data through the backbone model, applies pooling if specified, and feeds the features to the head for classification.\n   - The output of the model is the predicted probabilities for each class.\n","metadata":{"papermill":{"duration":0.025798,"end_time":"2024-05-27T08:24:07.809519","exception":false,"start_time":"2024-05-27T08:24:07.783721","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class BirdModel(torch.nn.Module):\n    def __init__(self, model_name, pretrained, in_channels, num_classes, pool=\"default\"):\n        super().__init__()\n\n        self.pool = pool\n        self.normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        \n        if pool == \"default\":\n            self.backbone = timm.create_model(\n                model_name=model_name, pretrained=pretrained,\n                num_classes=0, in_chans=3)\n        else:\n            self.backbone = timm.create_model(\n                model_name=model_name, pretrained=pretrained,\n                num_classes=0, in_chans=3, global_pool=\"\")\n\n        in_features = self.backbone.num_features\n\n\n\n        self.max_pooling = torch.nn.Sequential(torch.nn.AdaptiveMaxPool2d(1),\n                                               torch.nn.Flatten(start_dim=1, end_dim=-1))\n        self.avg_pooling = torch.nn.Sequential(torch.nn.AdaptiveAvgPool2d(1),\n                                               torch.nn.Flatten(start_dim=1, end_dim=-1))\n        self.both_pooling_neck = torch.nn.Sequential(torch.nn.BatchNorm1d(2*in_features),\n                                                     torch.nn.Linear(in_features=2*in_features, out_features=in_features))\n        \n        self.head = torch.nn.Sequential(\n            torch.nn.BatchNorm1d(in_features),\n            torch.nn.Linear(in_features=in_features, out_features=256),\n            torch.nn.Hardswish(inplace=True),torch.nn.Dropout(0.1),\n            torch.nn.Linear(in_features=256, out_features=len(LABELS))  \n        )\n\n\n\n        self.active = torch.nn.Sigmoid()\n    def forward(self, x):\n        x = x.expand(-1, 3, -1, -1)\n        x = self.normalize(x)\n        x = self.backbone(x)\n\n        if self.pool == \"max\":\n            x = self.max_pooling(x)\n        elif self.pool == \"avg\":\n            x = self.avg_pooling(x)\n        elif self.pool == \"both\":\n            x_max = self.max_pooling(x)\n            x_avg = self.avg_pooling(x)\n            x = x_max + x_avg\n            # x = torch.cat([x_max, x_avg], dim=1)\n            # x = self.both_pooling_neck(x)\n            \n        x = self.head(x)\n        # x = self.active(x)\n        return x","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:07.86407Z","iopub.status.busy":"2024-05-27T08:24:07.863673Z","iopub.status.idle":"2024-05-27T08:24:07.88091Z","shell.execute_reply":"2024-05-27T08:24:07.879613Z"},"papermill":{"duration":0.048113,"end_time":"2024-05-27T08:24:07.883643","exception":false,"start_time":"2024-05-27T08:24:07.83553","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">11 | Stratified k-Fold Cross-Validation and Random Seed Setting 🐦</p>\n\n\n1. **Stratified K-Fold Cross-Validation**:\n   - Stratified K-Fold cross-validation is a technique used to evaluate the performance of a machine learning model. It ensures that each fold of the dataset has approximately the same proportion of samples from each class, which is particularly useful for imbalanced datasets.\n   - `StratifiedKFold` is a class from the scikit-learn library that splits a dataset into K folds while preserving the percentage of samples for each class.\n   - In the `for` loop:\n       - `skf.split(train_csv, train_csv['primary_label'])` splits the dataset (`train_csv`) into train and validation sets for each fold, ensuring that each fold maintains the same distribution of classes as the original dataset.\n       - `train_index` and `valid_index` contain the indices of samples for the training and validation sets for the current fold.\n       - `enumerate(skf.split(...))` iterates over each fold, providing the fold index (`fold`) and the corresponding train/validation indices.\n       - `train_csv.loc[valid_index, 'fold'] = int(fold)` assigns the fold index to the validation samples in the `fold` column of the DataFrame `train_csv`, indicating which fold each sample belongs to.\n\n2. **Random Seed Setting**:\n   - Setting random seeds ensures reproducibility of results in machine learning experiments. It initializes the random number generators with a fixed seed, so the same sequence of random numbers is generated every time the code is run.\n   - `set_random_seed` is a function defined to set the random seed across different random number generators used in the experiment.\n   - Inside the function:\n       - `random.seed(seed)`, `np.random.seed(seed)`, and `os.environ[\"PYTHONHASHSEED\"] = str(seed)` set the random seed for the Python built-in random number generator, NumPy, and hash randomization, respectively.\n       - `torch.manual_seed(seed)` sets the random seed for the PyTorch library for CPU operations.\n       - `torch.cuda.manual_seed(seed)` sets the random seed for GPU operations in PyTorch.\n       - `torch.backends.cudnn.deterministic = deterministic` ensures deterministic behavior of CuDNN (CUDA Deep Neural Network library) for GPU operations in PyTorch, which can affect the performance but ensures reproducibility.\n\n","metadata":{"papermill":{"duration":0.025698,"end_time":"2024-05-27T08:24:07.936261","exception":false,"start_time":"2024-05-27T08:24:07.910563","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nskf = StratifiedKFold(n_splits=cfg.nfolds, shuffle=True, random_state=cfg.seed)\nfor fold, (train_index, valid_index) in enumerate(skf.split(train_csv, train_csv['primary_label'])):\n    train_csv.loc[valid_index, 'fold'] = int(fold)\n    \n    \nif isTrain:\n    train_csv.groupby(\"fold\", as_index=False)[\"primary_label\"].value_counts()   \n    \n    \ndef set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = deterministic  # type: ignore    ","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:07.991368Z","iopub.status.busy":"2024-05-27T08:24:07.99097Z","iopub.status.idle":"2024-05-27T08:24:08.043162Z","shell.execute_reply":"2024-05-27T08:24:08.041934Z"},"papermill":{"duration":0.082794,"end_time":"2024-05-27T08:24:08.04628","exception":false,"start_time":"2024-05-27T08:24:07.963486","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\"> 12 | BCEFocalLoss: Binary Cross-Entropy Focal Loss 🐦</p>\n\n\n1. **BCEFocalLoss Class**:\n    - This class defines a custom loss function called `BCEFocalLoss` for binary classification tasks.\n    - It inherits from `nn.Module`, indicating that it's a PyTorch module.\n\n2. **Initialization**:\n    - The `__init__` method initializes the loss function with two parameters: `alpha` and `gamma`.\n    - `alpha` (default value: 0.25) controls the balance between positive and negative class samples in the loss calculation.\n    - `gamma` (default value: 2.0) controls the degree of focus on hard-to-classify examples.\n\n3. **Forward Method**:\n    - The `forward` method computes the loss given model predictions (`preds`) and ground truth labels (`targets`).\n    - It first calculates the binary cross-entropy (BCE) loss using `nn.BCEWithLogitsLoss` with the option `reduction='none'` to compute the loss per sample without averaging.\n    - `probas = torch.sigmoid(preds)` computes the sigmoid activation of the model predictions to obtain probabilities.\n\n4. **Focal Loss Components**:\n    - Focal loss introduces two additional components: focal term (`tmp`) and smooth term (`smp`).\n    - `tmp` calculates the focal loss for positive class samples, where the focus is increased for misclassified samples (`(1. - probas)**self.gamma` increases the loss for hard-to-classify examples).\n    - `smp` calculates the focal loss for negative class samples, focusing on correctly classified samples (`probas**self.gamma` increases the loss for hard-to-classify examples).\n    - Both `tmp` and `smp` are multiplied by the BCE loss to incorporate the original loss calculation.\n\n5. **Final Loss Calculation**:\n    - The final loss is calculated as the sum of `tmp` and `smp`, followed by taking the mean over all samples.\n    - This mean loss value is returned as the output of the `forward` method.\n\n","metadata":{"papermill":{"duration":0.025938,"end_time":"2024-05-27T08:24:08.098639","exception":false,"start_time":"2024-05-27T08:24:08.072701","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class BCEFocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, preds, targets):\n        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(preds, targets)\n        probas = torch.sigmoid(preds)\n\n        \n\n        tmp = targets * self.alpha * (1. - probas)**self.gamma * bce_loss\n        smp = (1. - targets) * probas**self.gamma * bce_loss\n        \n        loss = tmp + smp\n        loss = loss.mean()\n        return loss","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:08.153501Z","iopub.status.busy":"2024-05-27T08:24:08.153081Z","iopub.status.idle":"2024-05-27T08:24:08.161809Z","shell.execute_reply":"2024-05-27T08:24:08.160499Z"},"papermill":{"duration":0.039605,"end_time":"2024-05-27T08:24:08.164291","exception":false,"start_time":"2024-05-27T08:24:08.124686","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">13 | Initialization Function For Training </p>\n\n1. **Model Initialization**:\n   - The function initializes the neural network model using the `BirdModel` class, which is customized for bird sound classification.\n   - It sets parameters such as the model architecture (`model_name`), whether to use pre-trained weights (`pretrained`), number of input channels (`in_channels`), number of output classes (`num_classes`), and pooling type (`pool`).\n\n2. **Optimizer Selection**:\n   - Depending on the configuration (`cfg.optimizer`), the function selects the optimizer for training.\n   - If `cfg.optimizer` is set to `'adan'`, it uses the custom optimizer `Adan` with specific parameters like learning rate (`lr`), betas, and weight decay.\n   - Otherwise, it uses the standard AdamW optimizer from PyTorch with parameters such as learning rate (`lr`) and weight decay.\n\n3. **Learning Rate Scheduler**:\n   - The function initializes a learning rate scheduler using `torch.optim.lr_scheduler.OneCycleLR`.\n   - This scheduler adjusts the learning rate during training, starting from an initial value (`cfg.lr`), and following a one-cycle policy with specified parameters such as the maximum number of epochs (`cfg.max_epoch`), percentage of epochs to increase/decrease learning rate (`pct_start`), and step size (`steps_per_epoch`).\n\n4. **Gradient Scaler (Automatic Mixed Precision)**:\n   - Automatic Mixed Precision (AMP) is a technique that combines single and half-precision floating-point arithmetic to speed up training while maintaining numerical stability.\n   - The function initializes a gradient scaler using `amp.GradScaler` with the option to enable or disable AMP based on the configuration (`cfg.enable_amp`).\n\n5. **Loss Function Initialization**:\n   - Depending on the loss type specified in the configuration (`cfg.loss_type`), the function initializes the loss function.\n   - If `cfg.loss_type` is set to `\"BCEWithLogitsLoss\"`, it uses the binary cross-entropy loss with logits (`torch.nn.BCEWithLogitsLoss`).\n   - If `cfg.loss_type` is set to `\"BCEFocalLoss\"`, it uses the custom focal loss function `BCEFocalLoss` with a specified `alpha` value.\n\n6. **Returning Initialized Components**:\n   - The function returns the initialized model, optimizer, scheduler, scaler, and loss function, all moved to the appropriate device (`device`), typically GPU.\n\n","metadata":{"papermill":{"duration":0.025553,"end_time":"2024-05-27T08:24:08.215738","exception":false,"start_time":"2024-05-27T08:24:08.190185","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def initialization():\n    model = BirdModel(model_name=cfg.model_name, pretrained=True, in_channels=3, num_classes=len(LABELS), pool=cfg.pool_type)\n    \n    if cfg.optimizer=='adan':\n        optimizer = Adan(model.parameters(), lr=cfg.lr, betas=(0.02, 0.08, 0.01), weight_decay=cfg.weight_decay)\n    else:\n        optimizer = torch.optim.AdamW(params=model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n    \n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer=optimizer, epochs=cfg.max_epoch,\n        pct_start=0.0, steps_per_epoch=len(train_dataloader),\n        max_lr=cfg.lr, div_factor=25, final_div_factor=4.0e-01\n    )\n    \n    scaler = amp.GradScaler(enabled=cfg.enable_amp)\n    if cfg.loss_type == \"BCEWithLogitsLoss\":\n        loss_func = torch.nn.BCEWithLogitsLoss()\n    elif cfg.loss_type == \"BCEFocalLoss\":\n        loss_func = BCEFocalLoss(alpha=1)\n    \n    \n\n\n    return model.to(device), optimizer, scheduler, scaler, loss_func.to(device)","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:08.269869Z","iopub.status.busy":"2024-05-27T08:24:08.269438Z","iopub.status.idle":"2024-05-27T08:24:08.28066Z","shell.execute_reply":"2024-05-27T08:24:08.279422Z"},"papermill":{"duration":0.041677,"end_time":"2024-05-27T08:24:08.283638","exception":false,"start_time":"2024-05-27T08:24:08.241961","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">14 | Training And Evaluation Functions 🐦</p>\n\n1. **`train_one_loop` Function**:\n   - This function performs one epoch of training.\n   - It iterates through the training data (`dataloader`) and updates the model parameters based on the calculated loss.\n   - Within each iteration:\n     - The data and labels are moved to the appropriate device (`device`).\n     - The gradients are zeroed using `optimizer.zero_grad()` to clear the previous gradients.\n     - Inside the training loop, gradient scaling is applied using AMP (Automatic Mixed Precision) to improve numerical stability and speed up training.\n     - The loss is computed using the specified loss function (`loss_fn`) and backpropagated through the network.\n     - The optimizer's learning rate is adjusted using the scheduler (`scheduler.step()`).\n     - The loss value is accumulated for monitoring training progress.\n   - After processing all batches, the average training loss is calculated and logged (if using Weights & Biases for logging).\n\n2. **`mixup_one_loop` Function**:\n   - This function performs one epoch of training with mixup augmentation.\n   - Mixup is a data augmentation technique that blends pairs of examples and their corresponding labels.\n   - It follows a similar structure to `train_one_loop`, but before feeding the data to the model, it applies mixup augmentation based on a probability threshold (`cfg.aug_spec_mixup_prob`).\n   - Mixup can be applied either on different waveforms (`\"other_wave\"`) or on spectrograms (`\"spec_mixup\"`).\n   - The rest of the process, including loss computation and optimization, remains the same.\n\n3. **`evaluate_validation` Function**:\n   - This function evaluates the model on the validation dataset (`dataloader`) after each epoch of training.\n   - It calculates validation loss and various evaluation metrics such as AUC (Area Under the ROC Curve), F1-score, precision, and more.\n   - Inside the evaluation loop:\n     - The model makes predictions on the validation data.\n     - Predictions are compared with the ground truth labels to compute the evaluation metrics.\n     - The validation loss is computed using the specified loss function.\n   - The function returns the validation loss and evaluation metrics, which can be used for monitoring the model's performance during training.\n\nThese functions collectively handle the training and evaluation process of the bird sound classification model, including data processing, model training, and performance evaluation. Additionally, they provide flexibility in choosing different training strategies such as mixup augmentation and support for monitoring training progress using Weights & Biases.","metadata":{"papermill":{"duration":0.026164,"end_time":"2024-05-27T08:24:08.335485","exception":false,"start_time":"2024-05-27T08:24:08.309321","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def train_one_loop(model, optimizer, scaler, scheduler, dataloader, loss_fn):\n    trainloss = 0; model.train()\n\n    count = 0\n    for idx, (data, label) in enumerate(tqdm(dataloader,leave=False ,desc=\"[train]\")):\n        # label = label.reshape(-1, len(LABELS))\n        \n        data, label = data.to(device), label.to(device)\n        \n        optimizer.zero_grad()\n        with amp.autocast(cfg.enable_amp, dtype=torch.bfloat16):\n        # with amp.autocast(cfg.enable_amp):\n            pred = model.forward(data)\n            loss = loss_fn(pred, label)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        scheduler.step()\n        \n        trainloss += loss.item()\n        # print(idx, loss.item())\n        # if cfg.wandb == True:\n        #     wandb.log({f\"train_loss\": loss.item(), f\"lr\":scheduler.get_lr()[0]})\n        del data, label, loss\n        count += 1\n        # if count == 300:\n        # break\n    trainloss /= len(dataloader)\n    if cfg.wandb == True:\n        wandb.log({f\"train_loss\": trainloss, f\"lr\":scheduler.get_lr()[0]})\n    return model, optimizer, scaler, scheduler, trainloss\n\n\ndef mixup_one_loop(model, optimizer, scaler, scheduler, dataloader, loss_fn):\n    trainloss = 0; model.train()\n\n    count = 0\n    for idx, (data, label) in enumerate(tqdm(dataloader,leave=False ,desc=\"[train]\")):\n        if np.random.random()>cfg.aug_spec_mixup_prob:\n            data, label = mixup(data=data, targets=label, alpha=cfg.alpha, mode=\"other_wave\")\n        else:\n            data, label = spec_mixup(data=data, targets=label)\n        data, label = data.to(device), label.to(device)\n        \n        optimizer.zero_grad()\n        with amp.autocast(cfg.enable_amp, dtype=torch.bfloat16):\n        # with amp.autocast(cfg.enable_amp):\n            pred = model.forward(data)\n            loss = loss_fn(pred, label)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        scheduler.step()\n        \n        trainloss += loss.item()\n        # print(idx, loss.item())\n        # if cfg.wandb == True:\n        #     wandb.log({f\"lr\":scheduler.get_lr()[0]})\n        del data, label, loss\n        count += 1\n        # if count == 300:\n        # break\n    trainloss /= len(dataloader)\n    if cfg.wandb == True:\n        wandb.log({f\"train_loss\": trainloss, f\"lr\":scheduler.get_lr()[0]})\n    return model, optimizer, scaler, scheduler, trainloss\n\n\ndef evaluate_validation(model, dataloader, loss_fn):\n    validloss=0\n    model.eval()\n\n    preds, trues, targets = [], [], []\n    \n    for idx, (data, label) in enumerate(tqdm(dataloader,leave=False ,desc=\"[valid]\")):\n        # label = label.reshape(-1, len(LABELS))\n\n        d = data[0].unsqueeze(1)\n        label = label[0]\n        \n        d = d.to(device)\n        # with amp.autocast(cfg.enable_amp):\n        pred = model.forward(d)\n\n        preds.extend(pred.detach().cpu())\n        trues.extend(label)\n        targets.extend(label.argmax(axis=1))\n        \n    #======================== metrics ========================#\n    # y_preds = torch.stack(preds)\n    t = torch.stack(preds)\n    t = torch.sigmoid(t)\n    targets = torch.tensor(targets)\n    y_trues = torch.stack(trues)\n\n\n    validloss = loss_fn(torch.stack(preds), torch.stack(trues))\n    #     # print(idx, loss)\n    #     # wandb.log({\"valid_loss\": loss})\n\n    # validloss /= len(dataloader)\n    \n    # sk_f1 = metrics.f1_score(np.array(y_trues), np.array(t), average=\"micro\")\n    sk_f1_30 = metrics.f1_score(np.array(y_trues), np.array(t) > 0.30, average=\"micro\")\n    sk_f1_50 = metrics.f1_score(np.array(y_trues), np.array(t) > 0.50, average=\"micro\")\n    \n    auc = multiclass_auroc(input=t, target=targets, num_classes=len(LABELS),\n                           average=\"macro\").item()\n\n    # auc_micro = multiclass_auroc(input=t, target=targets, num_classes=len(LABELS),\n    #                        average=\"none\").item()\n\n    prec = multiclass_precision(input=t, target=targets, num_classes=len(LABELS),\n                           average=\"macro\").item()\n    # rec = multiclass_recall(input=t, target=targets, num_classes=len(LABELS),\n    #                        average=\"macro\").item()\n    \n    # acc = multilabel_accuracy(input=t, target=targets).item()\n\n    f1 = multiclass_f1_score(input=t, target=torch.tensor(targets), num_classes=len(LABELS),\n                             average=\"micro\").item()\n\n    f1_macro = multiclass_f1_score(input=t, target=torch.tensor(targets), num_classes=len(LABELS),\n                             average=\"macro\").item()\n\n    t_03 = (t>0.3).int()\n    t_03 = torch.tensor(t_03, dtype=torch.int64)\n    f1_03 = multiclass_f1_score(input=t_03, target=torch.tensor(targets), num_classes=len(LABELS), \n                                average=\"micro\").item()\n\n    t_05 = (t>0.5).int()\n    t_05 = torch.tensor(t_05, dtype=torch.int64)\n    f1_05 = multiclass_f1_score(input=t_05, target=torch.tensor(targets), num_classes=len(LABELS), \n                                average=\"micro\").item()\n\n    if cfg.wandb == True:\n        wandb.log({f\"valid_loss\": validloss,\n                   f\"AUC\":auc,\n                   # \"auc_micro\":auc_micro,\n                   \"precision\":prec, \n                   # \"recall\":rec, \n                   # \"accuracy\":acc,\n                   f\"F1\":f1,\n                   \"F1_macro\":f1_macro,\n                   f\"F1 30%\":f1_03,\n                   f\"F1 50%\":f1_05})\n    return validloss, auc, f1, f1_03, f1_05, sk_f1_30, sk_f1_50","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:08.389349Z","iopub.status.busy":"2024-05-27T08:24:08.388922Z","iopub.status.idle":"2024-05-27T08:24:08.422666Z","shell.execute_reply":"2024-05-27T08:24:08.421121Z"},"papermill":{"duration":0.063951,"end_time":"2024-05-27T08:24:08.425685","exception":false,"start_time":"2024-05-27T08:24:08.361734","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Explanation:**\n\n\n\n1. **Creating Temporary Parameters Dictionary (`tmp_params`):**\n   - This code initializes a dictionary named `tmp_params` by copying the content of the `config` object's variables.\n   - It then removes certain reserved attributes (`__module__`, `__dict__`, `__weakref__`, `__doc__`) from the dictionary.\n   - This operation essentially extracts all the parameters from the `config` object and stores them in `tmp_params` for further processing or logging.\n\n2. **`get_oversampled_df` Function:**\n   - This function is designed to oversample the dataframe `df` to address class imbalance, a common issue in machine learning where certain classes have significantly fewer samples than others.\n   - It takes a dataframe `df` as input.\n   - It initializes an empty list `new_df` to store the oversampled dataframes.\n   - It identifies classes (birds in this case) with a low number of samples based on a threshold (`cfg.oversample_threthold`) specified in the configuration.\n   - For each class with fewer samples than the threshold, it replicates the data to increase the sample count.\n   - The replication process involves creating multiple copies (tiles) of the original data until the sample count reaches the threshold. This is achieved by concatenating the original dataframe multiple times.\n   - Finally, it concatenates all the oversampled dataframes into a single dataframe and returns it.\n\n3. **Explanation of `get_oversampled_df` Function (continued):**\n   - The function first identifies bird classes with low sample counts (`low_sample_birds`).\n   - For each identified class, it replicates the data (`tile_df`) to reach the oversampling threshold (`cfg.oversample_threthold`).\n   - It then concatenates the replicated dataframes (`tile_df`) along with a piece of data (`piece`) from the original dataframe that couldn't be tiled entirely.\n   - After processing all classes, it concatenates all oversampled dataframes (`new_df`) into a single dataframe, which is returned as the oversampled dataset.\n\nThis function serves the purpose of oversampling the dataset to address class imbalance, ensuring better model performance, especially for classes with fewer samples. ","metadata":{"papermill":{"duration":0.025848,"end_time":"2024-05-27T08:24:08.477845","exception":false,"start_time":"2024-05-27T08:24:08.451997","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if isTrain == True:\n    tmp_params = dict(vars(config))\n    del tmp_params['__module__'],tmp_params['__dict__'],tmp_params['__weakref__'],tmp_params['__doc__']\n\ndef get_oversampled_df(df):\n    \n    new_df = [df]\n\n    low_sample_birds = df[\"primary_label\"].value_counts()[df[\"primary_label\"].value_counts() < cfg.oversample_threthold].index\n    for bird in low_sample_birds:\n        tmp = df[df[\"primary_label\"] == bird]\n        data_num = len(tmp)\n    \n        tiles = 1 + cfg.oversample_threthold // data_num\n    \n        tile_df = []\n        for i in range(tiles):\n            tile_df.append(tmp)\n    \n        tiled_df = pd.concat(tile_df)\n        piece = tiled_df[data_num:cfg.oversample_threthold]\n        new_df.append(piece)\n    \n    return pd.concat(new_df)","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:08.533093Z","iopub.status.busy":"2024-05-27T08:24:08.532682Z","iopub.status.idle":"2024-05-27T08:24:08.543168Z","shell.execute_reply":"2024-05-27T08:24:08.54158Z"},"papermill":{"duration":0.042146,"end_time":"2024-05-27T08:24:08.546191","exception":false,"start_time":"2024-05-27T08:24:08.504045","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Explanation:**\n\n1. **Condition Check (`isTrain == True`):**\n   - This line checks if the `isTrain` variable is `True`, indicating that the code is running in training mode.\n\n2. **Setting Random Seed:**\n   - The `set_random_seed` function is called to set a random seed for reproducibility. It ensures that random operations in the code generate the same results across different runs when the same seed is used.\n\n3. **Initializing Weights and Biases (W&B) Logging:**\n   - If W&B logging is enabled (`cfg.wandb == True`), the `wandb.init` function is called to initialize a W&B run for logging the training process. The project name and run name are specified, along with the configuration parameters (`tmp_params`).\n   \n4. **Fold-wise Training:**\n   - The training process is performed for each fold in the cross-validation setup (`for fold in cfg.inference_folds:`).\n   - For each fold:\n     - The training and validation data for the current fold are separated.\n     - If oversampling is enabled (`cfg.oversample == True`), the training data is oversampled using the `get_oversampled_df` function to address class imbalance.\n     - Training and validation datasets are created using the `BirdCLEF_Dataset` class, with or without augmentation based on the mode.\n     - The model, optimizer, scheduler, scaler, and loss function are initialized using the `initialization` function.\n     - The training loop runs for a specified number of epochs (`cfg.max_epoch`), with different behaviors for the initial epochs (`cfg.aug_epoch`).\n     - During the training loop, the model is trained on the training dataset using either standard training or mixup training based on a probability condition (`cfg.aug_spec_mixup`).\n     - After each epoch, the model's performance is evaluated on the validation dataset, and if the validation loss improves, the model is saved.\n     - Training progress and performance metrics are logged using print statements.\n     - At the end of training for each fold, the best-performing model is saved, and memory is cleaned up.\n\n5. **W&B Logging of Best Metrics:**\n   - If W&B logging is enabled, the best validation loss, F1 score, and AUC score are logged using the `wandb.log` function.\n\n6. **Memory Cleanup:**\n   - After training each fold, memory is cleaned up to release GPU memory using `del`, `gc.collect()`, and `torch.cuda.empty_cache()`.\n\n\n","metadata":{"papermill":{"duration":0.027092,"end_time":"2024-05-27T08:24:08.599556","exception":false,"start_time":"2024-05-27T08:24:08.572464","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if isTrain == True:\n    set_random_seed(seed=42)\n    \n    \n    if cfg.wandb == True:\n        wandb.init(project='BirdCLEF_cv_ver2', name=f\"{name}\",\n                   config=tmp_params)\n        \n    # for fold in range(cfg.nfolds):\n    for fold in cfg.inference_folds:\n        train_ = train_csv.loc[train_csv[\"fold\"]!=fold]\n\n        if cfg.oversample == True:\n            train = get_oversampled_df(df=train_)\n        else:\n            train = train_\n        \n        augme_dataset = BirdCLEF_Dataset(df=train, augmentation=True, mode='train')\n        augme_dataloader = torch.utils.data.DataLoader(dataset=augme_dataset, batch_size=cfg.train_batchsize, shuffle=True)\n\n        train_dataset = BirdCLEF_Dataset(df=train, augmentation=False, mode='train')\n        train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=cfg.train_batchsize, shuffle=True)\n        \n        valid = train_csv.loc[train_csv[\"fold\"]==fold]\n        valid_dataset = BirdCLEF_Dataset(df=valid, augmentation=False, mode='valid')\n        valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=cfg.valid_batchsize, shuffle=False)\n    \n        model, optimizer, scheduler, scaler, loss_func =  initialization()\n    \n    \n        best_f1 = 0\n        best_auc = 0\n        best_loss = 1.00000\n        for e in range(cfg.max_epoch):\n            start_time = time.time()\n            if e < cfg.aug_epoch:\n                if cfg.aug_spec_mixup > np.random.random():\n                    model, optimizer, scaler, shcheduler, train_loss = mixup_one_loop(model=model,optimizer=optimizer,scaler=scaler, \n                                                                                          scheduler=scheduler,dataloader=augme_dataloader, loss_fn=loss_func)\n                else:\n                    model, optimizer, scaler, shcheduler, train_loss = train_one_loop(model=model,optimizer=optimizer,scaler=scaler, \n                                                                                          scheduler=scheduler,dataloader=augme_dataloader, loss_fn=loss_func)\n\n            else:\n                model, optimizer, scaler, shcheduler, train_loss = train_one_loop(model=model,optimizer=optimizer,scaler=scaler, \n                                                                                          scheduler=scheduler,dataloader=train_dataloader, loss_fn=loss_func)\n            \n            valid_loss, auc, f1, f1_03, f1_05, sk_f1_30, sk_f1_50 = evaluate_validation(model=model, dataloader=valid_dataloader, loss_fn=loss_func)\n            # print(f\"epoch {e} , train_loss is {train_loss}, valid_loss is {valid_loss}\")\n            \n            if best_loss > valid_loss:\n                end_time = time.time()\n                print(f\"[epoch {str(e).zfill(2)}] AUC{auc: .4f}, F1{f1: .4f}, F1_03{f1_03: .4f}, F1_05{f1_05: .4f}\")\n                print(f\"[epoch {str(e).zfill(2)}] SKF1_03{sk_f1_30: .4f}, SKF1_05{sk_f1_50: .4f}\")\n                print(f\"[epoch {str(e).zfill(2)}] valid_loss {valid_loss: .6f}\")\n                print(f\"[epoch {str(e).zfill(2)}] update loss {best_loss: .6f} --> {valid_loss: .6f} {(end_time - start_time): .1f}[s]\")\n                print(f\"[epoch {str(e).zfill(2)}] update auc score {best_auc: .6f} --> {auc: .6f} {(end_time - start_time): .1f}[s]\")\n                model_name = f'{name}/checkpoint/fold_{fold}_snapshot_epoch_{str(e).zfill(2)}.pth'\n                best_model = model\n                best_loss = valid_loss\n                best_auc = auc\n                best_f1 = f1\n            else:\n                end_time = time.time()\n                print(f\"[epoch {str(e).zfill(2)}] NOT update loss {best_loss: .6f} <-- {valid_loss: .6f} {(end_time - start_time): .1f}[s]\")\n                print(f\"[epoch {str(e).zfill(2)}] NOT update score {best_auc: .6f} <-- {auc: .6f} {(end_time - start_time): .1f}[s]\")\n\n        if cfg.wandb == True:\n            wandb.log({f\"best_loss\": best_loss,\n                       f\"best_f1\": best_f1,\n                       f\"best_auc\":best_auc})\n\n        torch.save(best_model.state_dict(), model_name)\n        \n        del model, best_model\n        gc.collect()\n        torch.cuda.empty_cache()\n        print(\"--\")\n","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:08.654707Z","iopub.status.busy":"2024-05-27T08:24:08.654154Z","iopub.status.idle":"2024-05-27T08:24:08.680714Z","shell.execute_reply":"2024-05-27T08:24:08.679192Z"},"papermill":{"duration":0.05756,"end_time":"2024-05-27T08:24:08.683435","exception":false,"start_time":"2024-05-27T08:24:08.625875","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">15 | Model Loading For Inference 🐦</p>\n\n1. **Initializing Dictionaries:**\n   - Two dictionaries, `models` and `models_names`, are initialized to store loaded models and their corresponding names, respectively.\n\n2. **Iterating Over Folds:**\n   - The code iterates over each fold for which inference is required (`for fold in cfg.inference_folds:`).\n\n3. **Loading Trained Model:**\n   - The path to the best-performing model checkpoint for the current fold is obtained using `glob.glob`.\n   - The `BirdModel` class is instantiated to create a new model instance with the same architecture as the trained model.\n   - The model's state dictionary is loaded from the saved checkpoint file using `torch.load`.\n   - The loaded model is set to evaluation mode using `model.eval()`.\n\n4. **Storing Loaded Models and Names:**\n   - The loaded model is stored in the `models` dictionary with the fold index as the key.\n   - The name of the ONNX file for the model is generated from the checkpoint file path and stored in the `models_names` dictionary.\n\n5. **Printing Model Path and ONNX Name:**\n   - The path of the loaded model checkpoint file and the corresponding ONNX file name are printed for verification.\n\n","metadata":{"papermill":{"duration":0.025863,"end_time":"2024-05-27T08:24:08.735311","exception":false,"start_time":"2024-05-27T08:24:08.709448","status":"completed"},"tags":[]}},{"cell_type":"code","source":"models = dict()\nmodels_names = dict()\n# for fold in range(cfg.nfolds):\nfor fold in cfg.inference_folds:\n    bestmodel_path = sorted(glob.glob(f\"/kaggle/input/{name}/checkpoint/fold_{fold}*.pth\"))[-1]\n\n    print(bestmodel_path)\n    model = BirdModel(model_name=cfg.model_name, pretrained=False, in_channels=1, num_classes=len(LABELS))\n    model.load_state_dict(torch.load(bestmodel_path, map_location=torch.device('cpu')))\n    model = model.eval()\n    models[fold] = model\n\n    models_names[fold] = bestmodel_path.split(\".\")[0]+\".onnx\"\n    print(models_names[fold])","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:08.789039Z","iopub.status.busy":"2024-05-27T08:24:08.788628Z","iopub.status.idle":"2024-05-27T08:24:09.372657Z","shell.execute_reply":"2024-05-27T08:24:09.371146Z"},"papermill":{"duration":0.614286,"end_time":"2024-05-27T08:24:09.375315","exception":false,"start_time":"2024-05-27T08:24:08.761029","status":"completed"},"scrolled":true,"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Explanation:**\n\n\n1. **Test Audio Directory and File List:**\n   - The directory path where the test audio files are stored is defined using `cfg.dir`.\n   - The list of test audio file paths is obtained using `glob.glob` by searching for files with the `.ogg` extension in the test audio directory.\n   - The list of file paths is sorted alphabetically.\n\n2. **Test Dataset and DataLoader Setup:**\n   - An instance of the `BirdCLEF_Dataset` class is created for the test dataset using the list of file paths obtained in the previous step.\n   - A `DataLoader` object is initialized for the test dataset with a batch size of 1 and shuffle set to False. This loader will be used to iterate over the test data during inference.\n\n3. **Input and Output Tensor Configuration:**\n   - An example input tensor is initialized with random values to specify the input shape expected by the ONNX models. The shape is defined as `(48, 1, cfg.n_mels, cfg.size_x+1)`.\n\n4. **Model File Paths Retrieval:**\n   - The file paths of the ONNX models for each fold are retrieved using `glob.glob`.\n   - The paths are sorted to ensure consistency in model loading.\n\n5. **ONNX Session Initialization:**\n   - For each fold, the corresponding ONNX model is loaded using `onnx.load`, and the model's graph is accessed.\n   - An ONNX inference session is created using `ort.InferenceSession` by passing the serialized model obtained from `onnx_model.SerializeToString()`.\n   - The initialized ONNX sessions are stored in the `onnx_sessions` dictionary.\n\n**Concepts Used:**\n- **File Path Manipulation:** File paths are obtained using `glob.glob` to locate test audio files and ONNX model files.\n- **Dataset and DataLoader:** The test dataset is prepared using `BirdCLEF_Dataset`, and a `DataLoader` is initialized for iterating over the test data.\n- **Input and Output Configuration:** Example input and output tensor shapes are defined to configure the ONNX models during inference.\n- **ONNX Model Loading:** ONNX models are loaded using `onnx.load`, and inference sessions are created using `ort.InferenceSession`.\n- **Data Storage:** The file paths of ONNX models and the initialized ONNX sessions are stored in dictionaries for easy access during inference.\n\n","metadata":{"papermill":{"duration":0.025798,"end_time":"2024-05-27T08:24:09.426918","exception":false,"start_time":"2024-05-27T08:24:09.40112","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\ntest_audio_dir = f\"{cfg.dir}test_soundscapes/\"\nfile_list = glob.glob(test_audio_dir+\"*.ogg\")\nfile_list = sorted(file_list)\n\n\ntest_dataset = BirdCLEF_Dataset(df=file_list, mode=\"test\")\ntest_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                              batch_size=1, \n                                              shuffle=False)\n\ninput_tensor = torch.randn((48, 1, cfg.n_mels, cfg.size_x+1))  # input shape\noutput_names=['output']\ninput_names=[\"x\"]\n\n\n# models_names = []\nmodels_names = dict()\n# for fold in range(cfg.nfolds):\nfor fold in cfg.inference_folds:\n    onnxmodel_path = sorted(glob.glob(f\"/kaggle/input/{name}/checkpoint/fold_{fold}*.onnx\"))[-1]\n\n    print(onnxmodel_path)\n\n    models_names[fold] = onnxmodel_path\n    \n    \nonnx_sessions = dict()\n# for fold in range(cfg.nfolds):\nfor fold in cfg.inference_folds:\n\n    onnx_model = onnx.load(models_names[fold])\n    onnx_model_graph = onnx_model.graph\n    onnx_session = ort.InferenceSession(onnx_model.SerializeToString())\n\n    onnx_sessions[fold] = onnx_session    ","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:09.480999Z","iopub.status.busy":"2024-05-27T08:24:09.480586Z","iopub.status.idle":"2024-05-27T08:24:09.899475Z","shell.execute_reply":"2024-05-27T08:24:09.89809Z"},"papermill":{"duration":0.449231,"end_time":"2024-05-27T08:24:09.902461","exception":false,"start_time":"2024-05-27T08:24:09.45323","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Explanation:**\n\n\n1. **Start Time Measurement:**\n   - The current time is recorded using `time.time()` to measure the duration of the inference process.\n\n2. **Inference Loop:**\n   - The code iterates over the test data using the `test_dataloader`.\n   - For each batch of data:\n     - Predictions are collected for each fold using the ONNX models loaded in the `onnx_sessions`.\n     - The predictions are obtained by running the ONNX session (`session.run`) with the input data (`data[0].numpy()`), where `data[0]` contains the audio data converted to a numpy array.\n     - The output predictions are then converted to PyTorch tensors, sigmoid is applied, and appended to the `preds` list.\n   - The average prediction across folds for each data batch is computed by taking the mean along the fold axis (`axis=0`) and stored in `preds_per_batch`.\n   - These batch-wise predictions are collected in the `predictions` list.\n\n3. **Prediction Processing:**\n   - Once all predictions are collected, they are stacked along the batch dimension to form a single tensor using `torch.stack`. If `predictions` is empty, it remains as is.\n   \n4. **End Time Measurement:**\n   - The current time is recorded again using `time.time()` after the inference loop completes.\n\n5. **Time Calculation:**\n   - The total time taken for the inference process is calculated by subtracting the start time from the end time.\n\n**Concepts Used:**\n- **Inference Loop:** Iterating over the test data using a `DataLoader` and performing inference for each batch.\n- **ONNX Model Inference:** Utilizing the loaded ONNX models to make predictions on the test data.\n- **Batch Processing:** Collecting and processing predictions in batches to optimize memory usage.\n- **Time Measurement:** Recording start and end times to calculate the duration of the inference process.\n\n","metadata":{"papermill":{"duration":0.025657,"end_time":"2024-05-27T08:24:09.955282","exception":false,"start_time":"2024-05-27T08:24:09.929625","status":"completed"},"tags":[]}},{"cell_type":"code","source":"start_time = time.time()\n\npredictions = []\nfor data in tqdm(test_dataloader):\n    \n    preds = []\n    \n#     for fold, session in enumerate(onnx_sessions):\n    for fold in cfg.inference_folds:\n        session = onnx_sessions[fold]\n        pred = session.run(output_names, {input_names[0]: data[0].numpy()})[0]\n        \n        pred = torch.sigmoid(torch.tensor(pred))\n        preds.append(pred)\n    preds_per_batch = torch.stack(preds, axis=0).mean(axis=0)\n    \n    predictions.extend(preds_per_batch)\n    \nif len(predictions)>0:\n    predictions = torch.stack(predictions)\nelse:\n    predictions = predictions\nend_time = time.time()\nuse_time = end_time - start_time","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:10.010819Z","iopub.status.busy":"2024-05-27T08:24:10.010383Z","iopub.status.idle":"2024-05-27T08:24:10.043582Z","shell.execute_reply":"2024-05-27T08:24:10.042225Z"},"papermill":{"duration":0.069935,"end_time":"2024-05-27T08:24:10.05298","exception":false,"start_time":"2024-05-27T08:24:09.983045","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">16 | Final Submission 📝</p>\n\nCreates a submission file (`submission.csv`) containing the predictions for the test data in the format required for submission. \n\n1. **Column Selection:**\n   - The column names representing bird species from the `sample_submission` DataFrame are extracted using `sample_submission.columns[1:]`. These columns represent the target labels.\n\n2. **DataFrame Initialization:**\n   - A new DataFrame `df` is initialized with columns named `'row_id'` and the bird species names.\n\n3. **Row ID Generation:**\n   - For each audio file in the `file_list` (representing test audio files), a unique `'row_id'` is generated for each 5-second segment of the audio. This is done by iterating over each file and creating row IDs in the format `<audio_file_name>_<segment_number>`, where the segment number ranges from 1 to the total number of 5-second segments in the audio file.\n\n4. **DataFrame Population:**\n   - The generated row IDs are assigned to the `'row_id'` column of the DataFrame.\n   - If there are predictions available (`len(predictions) >= 1`), the predictions are assigned to the corresponding bird species columns in the DataFrame. Otherwise, this step is skipped.\n     - This step involves populating the DataFrame with the predictions obtained from the previous inference step.\n\n5. **Saving to CSV:**\n   - The DataFrame `df` is saved to a CSV file named `\"submission.csv\"` using `to_csv()`, with the `index` set to `False` to exclude row indices from the output file.\n","metadata":{"papermill":{"duration":0.025943,"end_time":"2024-05-27T08:24:10.105111","exception":false,"start_time":"2024-05-27T08:24:10.079168","status":"completed"},"tags":[]}},{"cell_type":"code","source":"bird_cols = sample_submission.columns[1:]\ndf = pd.DataFrame(columns=['row_id']+list(bird_cols))\n\n\nrow_list = []\nfor file in file_list:\n    dataname = file.split(\"/\")[-1][:-4]\n    for i in range(int(4*60/5)):\n        row = f\"{dataname}_{(i+1)*5}\"\n        row_list.append(row)\n        \n        \n        \ndf['row_id'] = row_list        \n\nif len(predictions) < 1:\n    pass\nelse:\n    df[bird_cols] = predictions\n    \n    \ndf.to_csv(\"submission.csv\", index=False)     ","metadata":{"execution":{"iopub.execute_input":"2024-05-27T08:24:10.159839Z","iopub.status.busy":"2024-05-27T08:24:10.159378Z","iopub.status.idle":"2024-05-27T08:24:10.183943Z","shell.execute_reply":"2024-05-27T08:24:10.182797Z"},"papermill":{"duration":0.055183,"end_time":"2024-05-27T08:24:10.186739","exception":false,"start_time":"2024-05-27T08:24:10.131556","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}