{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3842332,"sourceType":"datasetVersion","datasetId":2286778},{"sourceId":3849825,"sourceType":"datasetVersion","datasetId":2290351}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# FakeSpeech Detection\n> Detect if an audio sample is **Real/Genuine** or **Fake/Spoof**.\n\n<div align=center><img src=\"https://i.ibb.co/f4LYTyY/fake-speech-logo.png\" width=500></div>","metadata":{}},{"cell_type":"markdown","source":"# A. Background ðŸŽ¦\nRemember the scene from [**Terminator**](https://www.youtube.com/watch?v=MT_u9Rurrqg) moviie where two Terminators were trying to trick each other by cloning human's voice? Nowadays, we can copy another person's voice within few mintues. Hence it is cruicial to detect **Fake Speech** to prevent many malicious work.\n\n<div align=center><img src=\"https://i.ibb.co/pP4bwqY/terminator-img.png\" width=800></div>","metadata":{}},{"cell_type":"markdown","source":"# B. Methodology  ðŸŽ¯\n* This notebook will demonstrate **Fake Speech Detection** with `TensorFlow`. \n* This notebook will also show how to use `tf.data`, `tfrecord` for **Audio Processing** task.\n* This notebook also demonstrates how to extract **Spectrogram** features from **Raw Audio** using TensorFlow.\n* This notebook will also show how to set up **Augmentation Pipeline** for audio data. It will also implement some cool augmentations such as **CutMix** and **MixUp** for **Audio** data.\n* This notebook also shows how we can use **Automatic Speech Recognition (ASR)** model for **Audio Classification** task to take leverage of **Relevant Transfer Learning** and **avoid ImageNet** pretraining.\n* In this notebook, I'll be showing the usage of **[Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/pdf/2005.08100.pdf)** Model by Google using Tensorflow.\n* This notebook will also show how to use TensorFlow [**audio_classification_models**](https://github.com/awsaf49/audio_classification_models) library to load similar models with just one line of code for a similar task.\n* **ASVspoof2019** dataset has been chosen for **Fake Speech Detection** task. This dataset comes with **dev/valid** & **eval/test** data. Hence we can directly use them for comparing model's performance. \n* TFRecord dataset for this **Audio Processing** task is created using [ASVspoof 2019 tfrecord Data](https://www.kaggle.com/code/awsaf49/asvspoof-2019-tfrecord-data) notebook.\n* TFRecord files are created using **StratifiedFold** to stratify each `.tfrec` file for avoiding imbalance in batch during training.\n* This notebook is compatible with both **GPU** and **TPU**. Device is automatically selected so you won't have to do anything to allocate device.","metadata":{}},{"cell_type":"markdown","source":"# C. Notebooks ðŸ“’\nðŸ“Œ **Data/Dataset**:\n* TFRecord: [ASVspoof 2019 tfrecord Data](https://www.kaggle.com/code/awsaf49/asvspoof-2019-tfrecord-data) ","metadata":{}},{"cell_type":"markdown","source":"# 1. Install Libraries ðŸ› ","metadata":{}},{"cell_type":"code","source":"# tensorflow utility\n!pip install -q tensorflow_io==0.21.0\n!pip install -q tensorflow_probability==0.14.1\n!pip install -q tensorflow_addons==0.15.0\n\n# audio classification models\n!pip install -q audio_classification_models\n\n# weights & biases\n!pip install -qU wandb","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-24T23:02:49.751875Z","iopub.execute_input":"2022-06-24T23:02:49.752639Z","iopub.status.idle":"2022-06-24T23:04:15.400211Z","shell.execute_reply.started":"2022-06-24T23:02:49.752543Z","shell.execute_reply":"2022-06-24T23:04:15.399074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Import Libraries ðŸ“š\nLet's imoport necessary libraries.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport os\nimport shutil\n\nimport cv2\nimport matplotlib.pyplot as plt\nplt.rcParams[\"font.family\"] = 'DejaVu Sans'\nimport seaborn as sns\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\n\nimport tensorflow as tf, re, math\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nimport tensorflow_io as tfio\nimport tensorflow_addons as tfa\nimport tensorflow_probability as tfp\n\nimport yaml\nfrom IPython import display as ipd\nimport json\nfrom datetime import datetime\n\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nfrom kaggle_datasets import KaggleDatasets\nimport sklearn\nfrom sklearn.metrics import confusion_matrix, precision_recall_fscore_support\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom IPython import display as ipd\n\nimport itertools\nimport scipy\nimport warnings\n\n# Show less log messages\ntf.get_logger().setLevel('ERROR')\ntf.autograph.set_verbosity(0)\n\n# Set true to show less logging messages\nos.environ[\"WANDB_SILENT\"] = \"true\"\nimport wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-24T23:04:15.402865Z","iopub.execute_input":"2022-06-24T23:04:15.403263Z","iopub.status.idle":"2022-06-24T23:04:23.494363Z","shell.execute_reply.started":"2022-06-24T23:04:15.403223Z","shell.execute_reply":"2022-06-24T23:04:23.493262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Configuration âš™ï¸","metadata":{}},{"cell_type":"code","source":"class CFG:\n    wandb = True\n    project = \"fake-speech-detection\"\n    debug = False\n    exp_name = \"v0\"\n    comment = \"Conformer-128x80-cosine-no_aug-no_fc\"\n\n    # Use verbose=0 for silent, 1 for interactive\n    verbose = 0\n    display_plot = True\n\n    # Device for training\n    device = None  # device is automatically selected\n\n    # Model & Backbone\n    model_name = \"Conformer\"\n\n    # Seeding for reproducibility\n    seed = 101\n\n    # Audio params\n    sample_rate = 16000\n    duration = 3.5 # duration in second\n    audio_len = int(sample_rate * duration)\n    normalize = True\n\n    # Spectrogram params\n    spec_freq = 128 # freq axis\n    n_fft = 2048\n    spec_time = 256 # time axis\n    hop_len = audio_len//(spec_time - 1) # non-overlap region\n    fmin = 20\n    fmax = sample_rate//2 # max frequency\n    spec_shape = [spec_time, spec_freq] # output spectrogram shape\n    \n    # Audio Augmentation\n    timeshift_prob = 0.0\n    gn_prob = 0.0\n    \n    # Spectrogram Augmentation\n    time_mask = 20\n    freq_mask = 10\n    cutmix_prob = 0.0\n    cutmix_alpha = 2.5\n    mixup_prob = 0.0\n    mixup_alpha = 2.5\n\n    # Batch Size & Epochs\n    batch_size = 32\n    drop_remainder = False\n    epochs = 12\n    steps_per_execution = None\n\n    # Loss & Optimizer & LR Scheduler\n    loss = \"binary_crossentropy\"\n    optimizer = \"Adam\"\n    lr = 1e-4\n    lr_schedule = \"cosine\"\n\n    # Augmentation\n    augment = False\n\n    # Clip values to [0, 1]\n    clip = False","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:04:23.49642Z","iopub.execute_input":"2022-06-24T23:04:23.497512Z","iopub.status.idle":"2022-06-24T23:04:23.508661Z","shell.execute_reply.started":"2022-06-24T23:04:23.497473Z","shell.execute_reply":"2022-06-24T23:04:23.507724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Reproducibility â™»ï¸\nSets value for random seed to produce similar result in each run.","metadata":{}},{"cell_type":"code","source":"def seeding(SEED):\n    \"\"\"\n    Sets all random seeds for the program (Python, NumPy, and TensorFlow).\n    \"\"\"\n    np.random.seed(SEED)\n    random.seed(SEED)\n    os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n#     os.environ[\"TF_CUDNN_DETERMINISTIC\"] = str(SEED)\n    tf.random.set_seed(SEED)\n    print(\"seeding done!!!\")\n\n\nseeding(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:04:23.511534Z","iopub.execute_input":"2022-06-24T23:04:23.512143Z","iopub.status.idle":"2022-06-24T23:04:23.537494Z","shell.execute_reply.started":"2022-06-24T23:04:23.512106Z","shell.execute_reply":"2022-06-24T23:04:23.536411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Set Up Device ðŸ“±\nFollowing codes automatically detects hardware(tpu or gpu or cpu). ","metadata":{}},{"cell_type":"code","source":"def configure_device():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()  # connect to tpu cluster\n        strategy = tf.distribute.TPUStrategy(tpu) # get strategy for tpu\n        print('> Running on TPU ', tpu.master(), end=' | ')\n        print('Num of TPUs: ', strategy.num_replicas_in_sync)\n        device='TPU'\n    except: # otherwise detect GPUs\n        tpu = None\n        gpus = tf.config.list_logical_devices('GPU') # get logical gpus\n        ngpu = len(gpus)\n        if ngpu: # if number of GPUs are 0 then CPU\n            strategy = tf.distribute.MirroredStrategy(gpus) # single-GPU or multi-GPU\n            print(\"> Running on GPU\", end=' | ')\n            print(\"Num of GPUs: \", ngpu)\n            device='GPU'\n        else:\n            print(\"> Running on CPU\")\n            strategy = tf.distribute.get_strategy() # connect to single gpu or cpu\n            device='CPU'\n    return strategy, device, tpu","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:04:23.53937Z","iopub.execute_input":"2022-06-24T23:04:23.539767Z","iopub.status.idle":"2022-06-24T23:04:23.550171Z","shell.execute_reply.started":"2022-06-24T23:04:23.539729Z","shell.execute_reply":"2022-06-24T23:04:23.54873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strategy, CFG.device, tpu = configure_device()\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:04:23.551999Z","iopub.execute_input":"2022-06-24T23:04:23.553136Z","iopub.status.idle":"2022-06-24T23:04:26.436541Z","shell.execute_reply.started":"2022-06-24T23:04:23.553095Z","shell.execute_reply":"2022-06-24T23:04:26.435545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Meta Data ðŸ“–\nFor Fake Speech Detection task, this notebook uses **ASVspoof 2019** dataset. It contains `train`,`valid` and `test` data for both `training` and `evaluating` model's performance. This dataset contains **Fake/Spoof** speech, created from $19$ system which is denoted as `A10` to `A19`. As the dataset is huge will use only a **balanced_fraction** of the actual dataset.\n* Columns:\n    * `speaker_id` : \t\tLA_****, a 4-digit speaker ID\n    * `filename` : \tLA_****, name of the audio file\n    * `system_id` : \t\tID of the speech spoofing system `(A01 - A19)`,  or, for **real** speech SYSTEM-ID is left blank ('-')\n    * `class_name` : \t\t**bonafide** for genuine speech, or, **spoof** for fake/spoof speech\n    * `target` : `1` for **fake/spoof**  and `0` for **real/genuine**","metadata":{}},{"cell_type":"code","source":"BASE_PATH = '../input/asvpoof-2019-dataset/LA/LA'\n\n# Train \ntrain_df = pd.read_csv(f'{BASE_PATH}/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt',\n                       sep=\" \", header=None)\ntrain_df.columns =['speaker_id','filename','system_id','null','class_name']\ntrain_df.drop(columns=['null'],inplace=True)\ntrain_df['filepath'] = f'{BASE_PATH}/ASVspoof2019_LA_train/flac/'+train_df.filename+'.flac'\ntrain_df['target'] = (train_df.class_name=='spoof').astype('int32') # set labels 1 for fake and 0 for real\nif True:\n    train_df = train_df.groupby(['target']).sample(2500).reset_index(drop=True)\nprint(f'Train Samples: {len(train_df)}')\ntrain_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:04:26.437605Z","iopub.execute_input":"2022-06-24T23:04:26.437934Z","iopub.status.idle":"2022-06-24T23:04:26.592368Z","shell.execute_reply.started":"2022-06-24T23:04:26.437893Z","shell.execute_reply":"2022-06-24T23:04:26.591433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-24T23:04:26.597242Z","iopub.execute_input":"2022-06-24T23:04:26.599531Z","iopub.status.idle":"2022-06-24T23:04:26.607037Z","shell.execute_reply.started":"2022-06-24T23:04:26.599492Z","shell.execute_reply":"2022-06-24T23:04:26.606032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To run code on **TPU** we need our data to be stored on **Google Cloud Storage**. Hence, we'll be needing **GCS_PATH** of our stored data. Worried about how we will get our data stored on **GCS**? \"Kaggle to the Rescue\" Kaggle provides a **GCS_PATH** for public datasets. Hence we can use it for training our model on **TPU**. Simply we have to use `KaggleDatasets()` to get `GCS_PATH` of our dataset.","metadata":{}},{"cell_type":"code","source":"GCS_PATH = KaggleDatasets().get_gcs_path('asvspoof-2019-tfrecord-dataset')\nTRAIN_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/asvspoof/train*.tfrec')\nVALID_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/asvspoof/valid*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/asvspoof/test*.tfrec')\n\nprint('# NUM TRAIN: {:,}'.format(count_data_items(TRAIN_FILENAMES)))\nprint('# NUM VALID: {:,}'.format(count_data_items(VALID_FILENAMES)))\nprint('# NUM TEST: {:,}'.format(count_data_items(TEST_FILENAMES)))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:04:26.612044Z","iopub.execute_input":"2022-06-24T23:04:26.614506Z","iopub.status.idle":"2022-06-24T23:04:27.73629Z","shell.execute_reply.started":"2022-06-24T23:04:26.61447Z","shell.execute_reply":"2022-06-24T23:04:27.735319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Data Augmentation ðŸŒˆ\n> **Caution:** Even though we are training audio as an image (spectrogram), we can't use typical computer-vision augmentations such as HorizontalFlip, Rotation, Shear, etc as it may contaminate the information contained within the image (spectrogram). For example, if we rotate a spectrogram then it doesn't make sense anymore as we can't relate this rotated spectrogram to the actual audio. ","metadata":{}},{"cell_type":"markdown","source":"## Used Augmentations\nTwo types of Augmentations are used here, \n1. AudioAug\n2. SpecAug\n\n## AudioAug:\n\n* Random Noise\n<img src=\"https://i.ibb.co/nQYZwry/aug.png\" alt=\"aug\" border=\"0\">\n* Random TimeShift\n<img src=\"https://i.ibb.co/xC2zHsr/aug-timeshift.png\" alt=\"aug-timeshift\" border=\"0\">\n* Random CropOrPad\n<img src=\"https://i.ibb.co/DzkNKPz/aug-crop-or-pad.png\" alt=\"aug-crop-or-pad\" border=\"0\">\n* Audio Trim\n<img src=\"https://i.ibb.co/7CD1Kgk/aug-trim.png\" alt=\"aug-trim\" border=\"0\">\n\n## SpecAug:\n\n* Random TimeMask\n<img src=\"https://i.ibb.co/T0jdwrT/spec-time-mask.png\" alt=\"spec-time-mask\" border=\"0\">\n* Random FreqMask\n<img src=\"https://i.ibb.co/16qk6tn/spec-freq-mask.png\" alt=\"spec-freq-mask\" border=\"0\">\n* CutMix\n<img src=\"https://i.ibb.co/LrsZsKs/spec-cutmix.png\" alt=\"spec-cutmix\" border=\"0\">\n* MixUp\n<img src=\"https://i.ibb.co/DRY0nC2/spec-mixup.png\" alt=\"spec-mixup\" border=\"0\">","metadata":{}},{"cell_type":"markdown","source":"## Utility","metadata":{}},{"cell_type":"code","source":"def random_int(shape=[], minval=0, maxval=1):\n    return tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.int32)\n\n\ndef random_float(shape=[], minval=0.0, maxval=1.0):\n    rnd = tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.float32)\n    return rnd","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-24T23:04:27.74312Z","iopub.execute_input":"2022-06-24T23:04:27.74568Z","iopub.status.idle":"2022-06-24T23:04:27.755026Z","shell.execute_reply.started":"2022-06-24T23:04:27.745641Z","shell.execute_reply":"2022-06-24T23:04:27.753981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## AudioAug\nAudio Augmentation","metadata":{}},{"cell_type":"code","source":"# Trim Audio to ignore silent part in the start and end\ndef TrimAudio(audio, epsilon=0.15):\n    pos  = tfio.audio.trim(audio, axis=0, epsilon=epsilon)\n    audio = audio[pos[0]:pos[1]]\n    return audio\n\n# Crop or Pad audio to keep a fixed length\ndef CropOrPad(audio, target_len, pad_mode='constant'):\n    audio_len = tf.shape(audio)[0]\n    if audio_len < target_len: # if audio_len is smaller than target_len then use Padding\n        diff_len = (target_len - audio_len)\n        pad1 = random_int([], minval=0, maxval=diff_len) # select random location for padding\n        pad2 = diff_len - pad1\n        pad_len = [pad1, pad2]\n        audio = tf.pad(audio, paddings=[pad_len], mode=pad_mode) # apply padding\n    elif audio_len > target_len:  # if audio_len is larger than target_len then use Cropping\n        diff_len = (audio_len - target_len)\n        idx = tf.random.uniform([], 0, diff_len, dtype=tf.int32) # select random location for cropping\n        audio = audio[idx: (idx + target_len)]\n    audio = tf.reshape(audio, [target_len])\n    return audio\n\n# Randomly shift audio -> any sound at <t> time may get shifted to <t+shift> time\ndef TimeShift(audio, prob=0.5):\n    if random_float() < prob:\n        shift = random_int(shape=[], minval=0, maxval=tf.shape(audio)[0])\n        if random_float() < 0.5:\n            shift = -shift\n        audio = tf.roll(audio, shift, axis=0)\n    return audio\n\n# Apply random noise to audio data\ndef GaussianNoise(audio, std=[0.0025, 0.025], prob=0.5):\n    std = random_float([], std[0], std[1])\n    if random_float() < prob:\n        GN = tf.keras.layers.GaussianNoise(stddev=std)\n        audio = GN(audio, training=True) # training=False don't apply noise to data\n    return audio\n\n# Applies augmentation to Audio Signal\ndef AudioAug(audio):\n    audio = TimeShift(audio, prob=CFG.timeshift_prob)\n    audio = GaussianNoise(audio, prob=CFG.gn_prob)\n    return audio\n\ndef Normalize(data):\n    MEAN = tf.math.reduce_mean(data)\n    STD = tf.math.reduce_std(data)\n    data = tf.math.divide_no_nan(data - MEAN, STD)\n    return data","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-24T23:04:27.7569Z","iopub.execute_input":"2022-06-24T23:04:27.75773Z","iopub.status.idle":"2022-06-24T23:04:27.778303Z","shell.execute_reply.started":"2022-06-24T23:04:27.757691Z","shell.execute_reply":"2022-06-24T23:04:27.777286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SpecAug\nSpectrogram Augmentation.\n","metadata":{}},{"cell_type":"code","source":"# Randomly mask data in time and freq axis\ndef TimeFreqMask(spec, time_mask, freq_mask, prob=0.5):\n    if random_float() < prob:\n        spec = tfio.audio.freq_mask(spec, param=freq_mask)\n        spec = tfio.audio.time_mask(spec, param=time_mask)\n    return spec\n\n# Applies augmentation to Spectrogram\ndef SpecAug(spec):\n    spec = TimeFreqMask(spec, time_mask=CFG.time_mask, freq_mask=CFG.freq_mask, prob=0.5)\n    return spec\n\n# Compute MixUp Augmentation for Spectrogram\ndef get_mixup(alpha=0.2, prob=0.5):\n    \"\"\"Apply Spectrogram-MixUp augmentaiton. Apply Mixup to one batch and its shifted version\"\"\"\n    def mixup(specs, labels, alpha=alpha, prob=prob):\n        if random_float() > prob:\n            return specs, labels\n\n        spec_shape = tf.shape(specs)\n        label_shape = tf.shape(labels)\n\n        beta = tfp.distributions.Beta(alpha, alpha) # select lambda from beta distribution\n        lam = beta.sample(1)[0]\n        \n        # It's faster to roll the batch by one instead of shuffling it to create image pairs\n        specs = lam * specs + (1 - lam) * tf.roll(specs, shift=1, axis=0) # mixup = [1, 2, 3]*lam + [3, 1, 2]*(1 - lam)\n        labels = lam * labels + (1 - lam) * tf.roll(labels, shift=1, axis=0)\n\n        specs = tf.reshape(specs, spec_shape)\n        labels = tf.reshape(labels, label_shape)\n        return specs, labels\n    return mixup\n\n\ndef get_cutmix(alpha, prob=0.5):\n    \"\"\"Apply Spectrogram-CutMix augmentaiton which only cuts patch across time axis unline \n    typical Computer-Vision CutMix. Apply CutMix to one batch and its shifted version.\n    \"\"\"\n    def cutmix(specs, labels, alpha=alpha, prob=prob):\n        if random_float() > prob:\n            return specs, labels\n        spec_shape = tf.shape(specs)\n        label_shape = tf.shape(labels)\n        W = tf.cast(spec_shape[1], tf.int32)  # [batch, time, freq, channel]\n\n        # Lambda from beta distribution\n        beta = tfp.distributions.Beta(alpha, alpha)\n        lam = beta.sample(1)[0]\n        \n        # It's faster to roll the batch by one instead of shuffling it to create image pairs\n        specs_rolled = tf.roll(specs, shift=1, axis=0) # specs->[1, 2, 3], specs_rolled->[3, 1, 2]\n        labels_rolled = tf.roll(labels, shift=1, axis=0)\n\n        # Select random patch size\n        r_x = random_int([], minval=0, maxval=W)\n        r = 0.5 * tf.math.sqrt(1.0 - lam)\n        r_w_half = tf.cast(r * tf.cast(W, tf.float32), tf.int32)\n\n        # Select random location in time axis\n        x1 = tf.cast(tf.clip_by_value(r_x - r_w_half, 0, W), tf.int32)\n        x2 = tf.cast(tf.clip_by_value(r_x + r_w_half, 0, W), tf.int32)\n\n        # outer-pad patch -> [0, 0, x, x, 0, 0]\n        patch1 = specs[:, x1:x2, :, :]  # [batch, time, freq, channel]\n        patch1 = tf.pad(\n            patch1, [[0, 0], [x1, W - x2], [0, 0], [0, 0]])  # outer-pad\n\n        # inner-pad-patch -> [y, y, 0, 0, y, y]\n        patch2 = specs_rolled[:, x1:x2, :, :]  # [batch, mel, time, channel]\n        patch2 = tf.pad(\n            patch2, [[0, 0], [x1, W - x2], [0, 0], [0, 0]])  # outer-pad\n        patch2 = specs_rolled - patch2  # inner-pad-patch = img - outer-pad-patch\n        \n        # patch1 -> [0, 0, x, x, 0, 0], patch2 -> [y, y, 0, 0, y, y]\n        # cutmix = (patch1 + patch2) -> [y, y, x, x, y, y]\n        specs = patch1 + patch2  # cutmix img\n\n        # Compute lambda = [1 - (patch_area/image_area)]\n        lam = tf.cast((1.0 - (x2 - x1) / (W)),tf.float32)  # no H term as (y1 - y2) = H\n        labels = lam * labels + (1.0 - lam) * labels_rolled  # cutmix label\n\n        specs = tf.reshape(specs, spec_shape)\n        labels = tf.reshape(labels, label_shape)\n\n        return specs, labels\n    return cutmix","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-24T23:04:27.780196Z","iopub.execute_input":"2022-06-24T23:04:27.780891Z","iopub.status.idle":"2022-06-24T23:04:27.810576Z","shell.execute_reply.started":"2022-06-24T23:04:27.780855Z","shell.execute_reply":"2022-06-24T23:04:27.809348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Feature Extraction ðŸ’¡\n* We'll feed **Mel-Spectrogram** feature to our model as input. We can simply consider **Mel-Spectrogram** as **2D image** which `x-axis `represents `time` and `y-axis` represents `frequency` information. Hence, we can consider this problem as an **Image Classification** Problem.\n* First, **Spectrogram** feature is extracted from audio sample using **Short Time Fourier Transform (STFT)** which is a variant of **Fast Fourier Transform (FFT)**. \n* The fast Fourier transform is a powerful tool that allows us to analyze the frequency content of a signal.\n* For non-periodic signals such as Music and Speech, compute several spectrums by performing FFT on several windowed segments of the signal hence it is called **Short-time Fourier transform**. It is computed on overlapping windowed segments of the signal, and we get what is called the spectrogram.\n* STFT suffers from famous Time-Frequency localization trade-off, which is\n    * `Narrow-Window` results in `Good time resolution` but `Poor frequency resolution`\n    * `Wide-Window` results in `Poor time resolution` but `Good frequency resolution`\n* Second, **Mel-Spectrogram** is computed which is a Spectrogram in **Mel-scale (logarithmic transformation of a signal's frequency)**. Mel spectrograms are better suited for applications that need to model human hearing perception such as **Speech and Music**.\n\n<div align=center><img src=\"https://i.ibb.co/yYkTmF7/Working-of-a-Spectrogram-24-3-D-Visualization-of-a-Spectrogram.jpg\" width=600></div>","metadata":{}},{"cell_type":"code","source":"# Compute Spectrogram from audio\ndef Audio2Spec(audio,spec_shape=[256, 128],sr=16000,nfft=2048,window=2048,fmin=20,fmax=8000):\n    spec_time = spec_shape[0]\n    spec_freq = spec_shape[1]\n    audio_len = tf.shape(audio)[0]\n    hop_length = tf.cast((audio_len // (spec_time - 1)), tf.int32) # compute hop_length to keep desired spec_shape\n    spec = tfio.audio.spectrogram(audio, nfft=nfft, window=window, stride=hop_length) # convert to spectrogram\n    mel_spec = tfio.audio.melscale(spec, rate=sr, mels=spec_freq, fmin=fmin, fmax=fmax) # transform to melscale\n    db_mel_spec = tfio.audio.dbscale(mel_spec, top_db=80) # from power to db (log10) scale\n    if tf.shape(db_mel_spec)[0] > spec_time:  # check if we have desiered shape\n        db_mel_spec = db_mel_spec[:spec_time,:]\n    db_mel_spec = tf.reshape(db_mel_spec, spec_shape)\n    return db_mel_spec\n\n# Convert spectrogram (H,W) to image (H,W,1)\ndef Spec2Img(spec, num_channels=1):\n    # 1 channel image\n    img = spec[..., tf.newaxis]\n    # Copy same image across channel axis\n    if num_channels>1:\n        img = tf.tile(img, [1, 1, num_channels])\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:04:27.815315Z","iopub.execute_input":"2022-06-24T23:04:27.817957Z","iopub.status.idle":"2022-06-24T23:04:27.831495Z","shell.execute_reply.started":"2022-06-24T23:04:27.817919Z","shell.execute_reply":"2022-06-24T23:04:27.830509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Data Pipeline ðŸš","metadata":{}},{"cell_type":"markdown","source":"## Reading TFRecord Data\n**What is TFRecord?**\n\n* The `.tfrecord`/`.tfrec` format is TensorFlow's custom data format which is used for storing a sequence of binary records.\n* TFRecord consumes **less storage on disk**, and has **faster read and write time from the disk**.\n* Apart from that there are several advantages to using TFRecords: \n    * Efficient usage of storage.\n    * Better I/O Speed.\n    * TPUs require that you pass data to them in TFRecord format\n    \n**Why use TFRecord for Audio Processing?**\n\n* For tasks like **Automatic Speaker Recognition** or **Text to Speech** we need to process multi-modal data (audio + text) at the same time. Unlike any other data format in `.tfrecord` we can load these multi-model files at once. In this notebook we can access both **audio** and its **metadata (audio_len,audio_id,speaker_id,etc)** using `.tfrecord`.\n* As `.tfrecord` is very fast, it takes less time to process data hence it reduces **CPU** bottleneck during training hence speeding up the training.\n* As `.tfrecord` takes less space we can save our cost on **Cloud Storage**.\n\n**How TFRecord is created for Audio Processing?**\n\n* Audio data is stored in `tfrecord` as a byte-string. \n* To encode audio data to byte-string we need to use `tf.audio.encode_wav()` function.\n* After loading `.tfrecord` we can parse data to get the byte-string using `example[\"audio\"]`. \n* Then, to decode it to `tf.Tensor` simply we can use `tf.audio.encode_wav()` function.\n* For more information, checout [ASVspoof 2019 tfrecord Data](https://www.kaggle.com/code/awsaf49/asvspoof-2019-tfrecord-data)  notebook.","metadata":{}},{"cell_type":"code","source":"# Decode audio from wav\ndef decode_audio(data, audio_len):\n    # Decode\n    audio, sr = tf.audio.decode_wav(data)\n    audio = tf.reshape(audio, [audio_len]) # explicit size needed for TPU\n    audio = tf.cast(audio,tf.float32)\n    # Normalization\n    if CFG.normalize:\n        audio = Normalize(audio)\n    return audio\n\n# Decode label\ndef decode_label(label):\n    label = tf.cast(label, tf.float32)\n    return label\n    \n# Read tfrecord data & parse it & do augmentation\ndef read_tfrecord(example, augment=True, return_id=False, return_label=True, target_len=CFG.audio_len, spec_shape=CFG.spec_shape):\n    tfrec_format = {\n        \"audio\" : tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\" : tf.io.FixedLenFeature([], tf.string),\n        \"speaker_id\": tf.io.FixedLenFeature([], tf.string),\n        \"system_id\" : tf.io.FixedLenFeature([], tf.string),\n        \"audio_len\" : tf.io.FixedLenFeature([], tf.int64),\n        \"target\" : tf.io.FixedLenFeature([], tf.int64),\n    }\n    # Parses a single example proto.\n    example = tf.io.parse_single_example(\n        example, tfrec_format\n    )\n    # Extract data from example proto\n    audio_id = example[\"id\"]\n    audio_len = example[\"audio_len\"]\n    # Decoding\n    audio = decode_audio(example[\"audio\"], audio_len)  # decode audio from .wav\n    target = decode_label(example[\"target\"]) # decode label -> type cast\n    # Trim Audio\n    audio = TrimAudio(audio)\n    # Crop or Pad audio to keep a fixed length\n    audio = CropOrPad(audio, target_len)\n    if augment:\n        # Apply AudioAug\n        audio = AudioAug(audio)\n    # Compute Spectrogram\n    spec = Audio2Spec(audio, spec_shape=spec_shape)\n    if augment:\n        # Apply SpecAug\n        spec = SpecAug(spec)\n    # Spectrogram (H, W) to Image (H, W, C)\n    img = Spec2Img(spec, num_channels=1) \n    # Clip & Reshape\n    img = tf.clip_by_value(img, 0, 1) if CFG.clip else img\n    img = tf.reshape(img, [*spec_shape, 1])\n    \n    if not return_id:\n        if return_label:\n            return (img, target)\n        else:\n            return img\n    else:\n        if return_label:\n            return (img, target, audio_id)\n        else:\n            return (img, audio_id)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-24T23:04:27.837092Z","iopub.execute_input":"2022-06-24T23:04:27.839675Z","iopub.status.idle":"2022-06-24T23:04:27.857614Z","shell.execute_reply.started":"2022-06-24T23:04:27.839637Z","shell.execute_reply":"2022-06-24T23:04:27.856494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipeline with **tf.data**\nTo build a data pipeline using `tfrecrod/tfrec`, we need to use `tf.data` API.\n\n* We can build complex input pipelines from simple, reusable pieces using`tf.data` API . For example, the pipeline for an audio model might aggregate data from files in a distributed file system, apply random transformation/augmentation to each audio, and merge randomly selected audios into a batch for training.\n* Moreover `tf.data` API provides a `tf.data.Dataset` feature that represents a sequence of components where each component comprises one or more pieces. For instance, in an audio pipeline, a component might be a single training example, with a pair of tensor pieces representing the audio and its label.\n\nCheck out this [doc](https://www.tensorflow.org/guide/data) if you want to learn more about `tf.data`.\n\n## Pipeline\nHere's an overview of our data pipeline,\n\n<img src=\"https://i.ibb.co/VBfmsp8/audio-pipeline.png\" alt=\"audio-pipeline\" border=\"0\">","metadata":{}},{"cell_type":"code","source":"def get_dataset(\n    filenames,\n    shuffle=True,\n    repeat=True,\n    augment=True,\n    cache=True,\n    return_id=False,\n    return_label=True,\n    batch_size=CFG.batch_size * REPLICAS,\n    target_len=CFG.audio_len,\n    spec_shape=CFG.spec_shape,\n    drop_remainder=False,\n    seed=CFG.seed,\n):\n    # Real tfrecord files\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) \n    if cache:\n        dataset = dataset.cache()  # cache data for speedup\n    if repeat:\n        dataset = dataset.repeat()  # repeat the data (for training only)\n    if shuffle:\n        dataset = dataset.shuffle(1024, seed=seed)  # shuffle the data (for training only)\n        options = tf.data.Options()\n        options.experimental_deterministic = False  # order won't be maintained when we shuffle\n        dataset = dataset.with_options(options)\n    # Parse data from tfrecord\n    dataset = dataset.map(lambda x: read_tfrecord(x,\n                                                  augment=augment,\n                                                  return_id=return_id,\n                                                  return_label=return_label,\n                                                  target_len=target_len,),\n                          num_parallel_calls=AUTO,)\n    # Batch Data Samples\n    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n    # MixUp\n    if CFG.mixup_prob and augment and return_label:\n        dataset = dataset.map(get_mixup(alpha=CFG.mixup_alpha,prob=CFG.mixup_prob),num_parallel_calls=AUTO)\n    # CutMix\n    if CFG.cutmix_prob and augment and return_label:\n        dataset = dataset.map(get_cutmix(alpha=CFG.cutmix_alpha,prob=CFG.cutmix_prob),num_parallel_calls=AUTO)\n    # Prefatch data for speedup\n    dataset = dataset.prefetch(AUTO)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:04:27.859453Z","iopub.execute_input":"2022-06-24T23:04:27.859999Z","iopub.status.idle":"2022-06-24T23:04:27.874431Z","shell.execute_reply.started":"2022-06-24T23:04:27.859961Z","shell.execute_reply":"2022-06-24T23:04:27.87349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Visualization ðŸ”­\nTo ensure our pipeline is generating **spectrogram** and its associate **label** correctly, we'll check some samples from a batch.","metadata":{}},{"cell_type":"code","source":"def plot_confusion_matrix(cm, \n                          classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues,\n                          save = False):\n    \"\"\"Plot Confusion Matrix\"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n#     plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    return\n    \ndef plot_history(history):\n    \"\"\"Plot model training history\"\"\"\n    plt.figure(figsize=(15, 5))\n    plt.plot(\n        np.arange(len(history[\"f1_score\"])),\n        history[\"f1_score\"],\n        \"-o\",\n        label=\"Train f1_score\",\n        color=\"#ff7f0e\",\n    )\n    plt.plot(\n        np.arange(len(history[\"f1_score\"])),\n        history[\"val_f1_score\"],\n        \"-o\",\n        label=\"Val f1_score\",\n        color=\"#1f77b4\",\n    )\n    x = np.argmax(history[\"val_f1_score\"])\n    y = np.max(history[\"val_f1_score\"])\n    xdist = plt.xlim()[1] - plt.xlim()[0]\n    ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x, y, s=200, color=\"#1f77b4\")\n    plt.text(x - 0.03 * xdist, y - 0.13 * ydist, \"max f1_score\\n%.2f\" % y, size=14)\n    plt.ylabel(\"f1_score\", size=14)\n    plt.xlabel(\"Epoch\", size=14)\n    plt.legend(loc=2)\n    plt2 = plt.gca().twinx()\n    plt2.plot(\n        np.arange(len(history[\"f1_score\"])),\n        history[\"loss\"],\n        \"-o\",\n        label=\"Train Loss\",\n        color=\"#2ca02c\",\n    )\n    plt2.plot(\n        np.arange(len(history[\"f1_score\"])),\n        history[\"val_loss\"],\n        \"-o\",\n        label=\"Val Loss\",\n        color=\"#d62728\",\n    )\n    x = np.argmin(history[\"val_loss\"])\n    y = np.min(history[\"val_loss\"])\n    ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x, y, s=200, color=\"#d62728\")\n    plt.text(x - 0.03 * xdist, y + 0.05 * ydist, \"min loss\", size=14)\n    plt.ylabel(\"Loss\", size=14)\n    plt.legend(loc=3)\n    plt.savefig(f\"history_plot.png\")\n    plt.show()\n    return\n\ndef display_batch(batch, row=2, col=5):\n    \"Plot one batch data\"\n    imgs, tars = batch\n    plt.figure(figsize=(5.0*col, 3.5*row))\n    for idx in range(row*col):\n        img = imgs[idx].numpy().transpose()[0]\n        tar = tars[idx].numpy()\n        plt.subplot(row, col, idx+1)\n        plt.imshow(img, cmap='coolwarm')\n        text = 'Fake' if tar else 'Real'\n        plt.title(text, fontsize=15, color=('red' if tar else 'green'))\n    plt.tight_layout();\n    plt.grid(False)\n    plt.show();\n    return","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-24T23:04:27.876028Z","iopub.execute_input":"2022-06-24T23:04:27.876744Z","iopub.status.idle":"2022-06-24T23:04:27.908894Z","shell.execute_reply.started":"2022-06-24T23:04:27.876706Z","shell.execute_reply":"2022-06-24T23:04:27.907676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Without Augmentation","metadata":{}},{"cell_type":"code","source":"ds = get_dataset(TRAIN_FILENAMES[:2], augment=False, cache=False, repeat=False).take(1)\nbatch = next(iter(ds.unbatch().batch(20)))\nimgs, tars = batch\nprint(f'image_shape: {imgs.shape} target_shape:{tars.shape}')\nprint(f'image_dtype: {imgs.dtype} target_dtype:{tars.dtype}')\ndisplay_batch(batch, row=3, col=3)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-24T23:04:27.910551Z","iopub.execute_input":"2022-06-24T23:04:27.911304Z","iopub.status.idle":"2022-06-24T23:04:34.508703Z","shell.execute_reply.started":"2022-06-24T23:04:27.911266Z","shell.execute_reply":"2022-06-24T23:04:34.50783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## With Augmentation","metadata":{}},{"cell_type":"code","source":"ds = get_dataset(TRAIN_FILENAMES[:2], augment=True, cache=False, repeat=False).take(1)\nbatch = next(iter(ds.unbatch().batch(20)))\nimgs, tars = batch\nprint(f'image_shape: {imgs.shape} target_shape:{tars.shape}')\nprint(f'image_dtype: {imgs.dtype} target_dtype:{tars.dtype}')\ndisplay_batch(batch, row=3, col=3)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-24T23:04:34.509765Z","iopub.execute_input":"2022-06-24T23:04:34.510126Z","iopub.status.idle":"2022-06-24T23:04:39.440231Z","shell.execute_reply.started":"2022-06-24T23:04:34.510092Z","shell.execute_reply":"2022-06-24T23:04:39.438992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Loss & Metric ðŸ“‰\nWe will use `binary_crossentropy` loss as our loss_function and `f1_score` as our primary metric. We will also use `precision`, `recall` & `accuracy` to compare performance of our model.\n> **Note:** One of the key difference between Loss and Metric is that loss must be **differentiable**. Hence, loss is directly used to optimize our model. This is the reason why we can't use `f1_score` or `accuracy` as metric. Hence, we keep an eye on the metrics to compare model's performance. \n\n* Precision\n$$ \nPrecision = \\frac{TP}{TP + FP}\n$$\n* Recall\n$$ \nRecall = \\frac{TP}{TP + FN}\n$$\n* F1_Score\n$$\nF1\\_Score = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}\n$$\n* Accuracy\n$$\nAccuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n$$","metadata":{}},{"cell_type":"code","source":"def get_metrics():\n    acc = tf.keras.metrics.BinaryAccuracy()\n    f1_score = tfa.metrics.F1Score(num_classes=1, threshold=0.5, average='macro')\n    precision = tf.keras.metrics.Precision()\n    recall = tf.keras.metrics.Recall()\n    return [acc, precision, recall, f1_score]","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:04:39.442258Z","iopub.execute_input":"2022-06-24T23:04:39.443021Z","iopub.status.idle":"2022-06-24T23:04:39.449334Z","shell.execute_reply.started":"2022-06-24T23:04:39.442982Z","shell.execute_reply":"2022-06-24T23:04:39.448533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12. LR Schedule âš“\n* Learning Rate scheduler for transfer learning. \n* The learning rate starts from `lr_start`, then decreases to a`lr_min` using different methods namely,\n    * **step**: Reduce lr step wise like stair.\n    * **cosine**: Follow Cosine graph to reduce lr.\n    * **exp**: Reduce lr exponentially.","metadata":{}},{"cell_type":"code","source":"import math\n\ndef get_lr_callback(mode='exp', batch_size=64, epochs=30, plot=False):\n    \"\"\"adapted from @cdeotte\"\"\"\n    lr_start = 5e-5\n    lr = 0.001 # base_lr\n    lr_max = 5e-4 # max lr - will be multiplied by batch_size\n    lr_min = 0.1e-4 # min lr\n    lr_ramp_ep = 4 # warming up epochs\n    lr_sus_ep = 0 # sustain epochs lr after warming up\n    lr_decay = 0.8 # decay rate\n\n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n\n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n\n        elif mode == 'exp':\n            lr = (lr_max - lr_min) * lr_decay**(epoch - \\\n                  lr_ramp_ep - lr_sus_ep) + lr_min\n\n        elif mode == 'step':\n            lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n\n        elif mode == 'cosine':\n            decay_total_epochs = epochs - lr_ramp_ep - lr_sus_ep + 3\n            decay_epoch_index = epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index / decay_total_epochs\n            cosine_decay = 0.5 * (1 + math.cos(phase))\n            lr = (lr_max - lr_min) * cosine_decay + lr_min\n        return lr\n\n    if plot:\n        plt.figure(figsize=(10, 5))\n        plt.plot(\n            np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n        plt.xlabel('epoch')\n        plt.ylabel('learnig rate')\n        plt.title('Learning Rate Scheduler')\n    return tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-24T23:04:39.450951Z","iopub.execute_input":"2022-06-24T23:04:39.451584Z","iopub.status.idle":"2022-06-24T23:04:39.46598Z","shell.execute_reply.started":"2022-06-24T23:04:39.451548Z","shell.execute_reply":"2022-06-24T23:04:39.465026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_callback = get_lr_callback(mode=CFG.lr_schedule,epochs=30,plot=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:05:02.697278Z","iopub.execute_input":"2022-06-24T23:05:02.697867Z","iopub.status.idle":"2022-06-24T23:05:02.9589Z","shell.execute_reply.started":"2022-06-24T23:05:02.697815Z","shell.execute_reply":"2022-06-24T23:05:02.957972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 13. Modeling ðŸ¤–\nBefore diving deep into modeling let's first set our agenda,\n* Q1: As this problem can be considered as an `Image Classification Task` why not use `ImageNet` pretrain models?\n    - **Ans:** `ImageNet` pretrain models are experts in extracting image features not audio features. As **ImageNet** models don't have much idea about audio data they can't reach optimal performance. Hence `Relevant Transfer Learning` or `Transfer Learning` from `Audio` dataset can boost our score.\n* Q2: How can we find pretrain models which are pretrain on large **Audio** dataset?\n    - **Ans:** We can simply use **Automatic Speech Recognition (ASR)** models as pretrain models which are pretrained on large audio datasets like **LibriSpeech** dataset.\n* Q3: As `Automatic Speech Recognition (ASR)` models are built for text generation from audio, how can we use them for fake speech detection (audio classification)?\n    - **Ans:** As ASR models are `Encoder-Decoder` models, we can't use them directly for audio classification task. But we can use a simple trick and try using `only Encoder` part of the model utilizing its feature vector for audio classification.","metadata":{}},{"cell_type":"markdown","source":"## Conformer \n> [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/pdf/2005.08100.pdf)\n\nIn this work, this model combines **Convolutional Neural Networks (CNN)** and **Transformers** to get the best of both worlds by modeling both local and global features of an audio sequence in a parameter-efficient way. As we'll be using only the **Encoder** part of the model, we'll focus on that part here.\nSo, the main components of **Conformer-Block** are,\n\n1. **Feed-forward module:** Simple fully connected layer to map $n$ inputs to $m$ outputs\n2. **Self-attention module:** Compute globally coherent features.\n3. **Convolution module:** Compute locally coherent features using `pointwise conv` and `1d depth-wise conv`.\n4. **Layer normalization module:** Unlike batch normalization, it directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer.\n\n<!-- > **Codes below are adapted from [here](https://github.com/kenza-bouzid/TransUnet)** -->\n\n<img src=\"https://i.ibb.co/61R58b1/conformer-01.png\" alt=\"conformer-01\" width=600>\n\n<!-- <img src=\"https://i.ibb.co/x7jyJVZ/conformer-04.png\" alt=\"conformer-04\" border=\"0\">\n<img src=\"https://i.ibb.co/F83NnTt/conformer-03.png\" alt=\"conformer-03\" border=\"0\">\n<img src=\"https://i.ibb.co/wzHzXQx/conformer-02.png\" alt=\"conformer-02\" border=\"0\"> -->\n","metadata":{}},{"cell_type":"markdown","source":"## Code\n* Re-designing ASR models from scratch is difficult and time-consuming hence I created [Audio Classification Models](https://github.com/awsaf49/audio_classification_models) library to speed up the process. It also contains models like **ContextNet**. \n* This library can directly be used for **audio classification** task. Below I have shown how to build a simple model using **ASR Encoder**.\n* We can load **Conformer** model directly using simple two lines of code,\n```py\nimport audio_classification_models as acm\nmodel = acm.Conformer(input_shape=(128,80,1), pretrain=True)\n```","metadata":{}},{"cell_type":"code","source":"import audio_classification_models as acm\n\nURL = 'https://github.com/awsaf49/audio_classification_models/releases/download/v1.0.8/conformer-encoder.h5'\n\ndef Conformer(input_shape=(128, 80, 1),num_classes=1, final_activation='sigmoid', pretrain=True):\n    \"\"\"Souce Code: https://github.com/awsaf49/audio_classification_models\"\"\"\n    inp = tf.keras.layers.Input(shape=input_shape)\n    backbone = acm.ConformerEncoder()\n    out = backbone(inp)\n    if pretrain:\n        acm.utils.weights.load_pretrain(backbone, url=URL)\n    out = tf.keras.layers.GlobalAveragePooling1D()(out)\n#     out = tf.keras.layers.Dense(32, activation='selu')(out)\n    out = tf.keras.layers.Dense(num_classes, activation=final_activation)(out)\n    model = tf.keras.models.Model(inp, out)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:11:47.797966Z","iopub.execute_input":"2022-06-24T23:11:47.798951Z","iopub.status.idle":"2022-06-24T23:11:47.806905Z","shell.execute_reply.started":"2022-06-24T23:11:47.798904Z","shell.execute_reply":"2022-06-24T23:11:47.805914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Model\n* Build complete model.\n* Select Loss, LR_Scheduling, Metrics and so on.\n* Compile model for training.","metadata":{}},{"cell_type":"code","source":"def get_model(name=CFG.model_name, loss=CFG.loss,):\n    model = Conformer(input_shape=[*CFG.spec_shape,1],pretrain=True)\n    lr = CFG.lr\n    if CFG.optimizer == \"Adam\":\n        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n    elif CFG.optimizer == \"AdamW\":\n        opt = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=lr)\n    elif CFG.optimizer == \"RectifiedAdam\":\n        opt = tfa.optimizers.RectifiedAdam(learning_rate=lr)\n    else:\n        raise ValueError(\"Wrong Optimzer Name\")\n    model.compile(\n        optimizer=opt,\n        loss=loss,\n        steps_per_execution=CFG.steps_per_execution, # to reduce idle time\n        metrics=get_metrics()\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-24T23:11:48.268524Z","iopub.execute_input":"2022-06-24T23:11:48.268875Z","iopub.status.idle":"2022-06-24T23:11:48.276952Z","shell.execute_reply.started":"2022-06-24T23:11:48.268846Z","shell.execute_reply":"2022-06-24T23:11:48.275962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()\nmodel.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-24T23:11:48.870603Z","iopub.execute_input":"2022-06-24T23:11:48.871246Z","iopub.status.idle":"2022-06-24T23:11:53.323068Z","shell.execute_reply.started":"2022-06-24T23:11:48.87121Z","shell.execute_reply":"2022-06-24T23:11:53.322038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 14. WandB ðŸª„\n<img src=\"https://camo.githubusercontent.com/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\" width=\"400\" alt=\"Weights & Biases\" />\n\nTo track model's training I'll be using **Weights & Biases** tool. Weights & Biases (W&B) is MLOps platform for tracking our experiemnts. We can use it to Build better models faster with experiment tracking, dataset versioning, and model management.","metadata":{}},{"cell_type":"code","source":"if CFG.wandb:\n    \"login in wandb otherwise run anonymously\"\n    try:\n        # Addo-ons > Secrets > WANDB\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        api_key = user_secrets.get_secret(\"WANDB\")\n        wandb.login(key=api_key)\n        anonymous = None\n    except:\n        anonymous = \"must\"\n\n\ndef wandb_init():\n    \"initialize project on wandb\"\n    id_ = wandb.util.generate_id() # generate random id\n    config = {k: v for k, v in dict(vars(CFG)).items() if \"__\" not in k} # convert class to dict\n    config[\"id\"] = id_\n    run = wandb.init(\n        id=id_,\n        project=\"fake-speech-detection\",\n        name=f\"dim-{CFG.spec_shape[0]}x{CFG.spec_shape[1]}|model-{CFG.model_name}\",\n        config=config,\n        anonymous=anonymous,\n        group=CFG.comment,\n        reinit=True,\n        resume=\"allow\",\n    )\n    return run","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-23T21:36:52.035769Z","iopub.execute_input":"2022-06-23T21:36:52.036124Z","iopub.status.idle":"2022-06-23T21:36:53.390422Z","shell.execute_reply.started":"2022-06-23T21:36:52.036093Z","shell.execute_reply":"2022-06-23T21:36:53.389461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 15. Training ðŸš…\nOur model will be trained on the `train` data and `valid` data will be used to save `checkpoint`. Finally, we'll check our model's performance on `test` data.","metadata":{}},{"cell_type":"code","source":"# Initialize wandb Run\nif CFG.wandb:\n    run = wandb_init()\n    WandbCallback = wandb.keras.WandbCallback(save_model=False)\n    \n# Load gcs_path of train, valid & test\nTRAIN_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/asvspoof/train*.tfrec')\nVALID_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/asvspoof/valid*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/asvspoof/test*.tfrec')\n\n# Take Only 10 Files if run in Debug Mode\nif CFG.debug:\n    TRAIN_FILENAMES = TRAIN_FILENAMES[:2]\n    VALID_FILENAMES = VALID_FILENAMES[:2]\n    TEST_FILENAMES = TEST_FILENAMES[:2]\n\n# Shuffle train files\nrandom.shuffle(TRAIN_FILENAMES)\n\n# Count train and valid samples\nNUM_TRAIN = count_data_items(TRAIN_FILENAMES)\nNUM_VALID = count_data_items(VALID_FILENAMES)\nNUM_TEST = count_data_items(TEST_FILENAMES)\n\n# Compute batch size & steps_per_epoch\nBATCH_SIZE = CFG.batch_size * REPLICAS\nSTEPS_PER_EPOCH = NUM_TRAIN // BATCH_SIZE\n\nprint(\"#\" * 60)\nprint(\"#### IMAGE_SIZE: (%i, %i) | BATCH_SIZE: %i | EPOCHS: %i\"% (CFG.spec_shape[0],\n                                                                  CFG.spec_shape[1],\n                                                                  BATCH_SIZE,\n                                                                  CFG.epochs))\nprint(\"#### MODEL: %s | LOSS: %s\"% (CFG.model_name, CFG.loss))\nprint(\"#### NUM_TRAIN: {:,} | NUM_VALID: {:,}\".format(NUM_TRAIN, NUM_VALID))\nprint(\"#\" * 60)\n\n# Log in w&B before training\nif CFG.wandb:\n    wandb.log(\n        {\n            \"num_train\": NUM_TRAIN,\n            \"num_valid\": NUM_VALID,\n            \"num_test\": NUM_TEST,\n        }\n    )\n\n# Build model in device\nK.clear_session()\nwith strategy.scope():\n    model = get_model(name=CFG.model_name,loss=CFG.loss)\n\n# Callbacks\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    \"/kaggle/working/ckpt.h5\",\n    verbose=CFG.verbose,\n    monitor=\"val_f1_score\",\n    mode=\"max\",\n    save_best_only=True,\n    save_weights_only=True,\n)\ncallbacks = [checkpoint, get_lr_callback(mode=CFG.lr_schedule,epochs=CFG.epochs)]\n\nif CFG.wandb:\n    # Include w&b callback if WANDB is True\n    callbacks.append(WandbCallback)\n\n# Create train & valid dataset\ntrain_ds = get_dataset(\n    TRAIN_FILENAMES,\n    augment=CFG.augment,\n    batch_size=BATCH_SIZE,\n    cache=False,\n    drop_remainder=False,\n)\nvalid_ds = get_dataset(\n    VALID_FILENAMES,\n    shuffle=False,\n    augment=False,\n    repeat=False,\n    batch_size=BATCH_SIZE,\n    cache=False,\n    drop_remainder=False,\n)\n\n# Train model\nhistory = model.fit(\n    train_ds,\n    epochs=CFG.epochs if not CFG.debug else 2,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    callbacks=callbacks,\n    validation_data=valid_ds,\n    #         validation_steps = NUM_VALID/BATCH_SIZE,\n    verbose=CFG.verbose,\n)\n\n# Convert dict history to df history\nhistory = pd.DataFrame(history.history)\n\n# Load best weights\nmodel.load_weights(\"/kaggle/working/ckpt.h5\")\n\n# Plot Training History\nif CFG.display_plot:\n    plot_history(history)","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-23T22:54:58.966091Z","iopub.execute_input":"2022-06-23T22:54:58.966856Z","iopub.status.idle":"2022-06-23T23:39:13.86084Z","shell.execute_reply.started":"2022-06-23T22:54:58.966822Z","shell.execute_reply":"2022-06-23T23:39:13.85988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Log\n### [Click Here âž¡ï¸](https://wandb.ai/awsaf49/fake-speech-detection) to check training log in **WandB** dashboard.\n\n<img src=\"https://i.ibb.co/sJGTNWq/wandb-dash-board.png\">","metadata":{}},{"cell_type":"markdown","source":"# 16. Performance ðŸŽ­\nWe'll compute **Accuracy**, **Precision**, **Recall** and **F1_Score** for both `valid` and `test` data. Finally, we'l plot `confusion_matrix` to get better insight about model's performance regarding **FP** and **FN**.","metadata":{}},{"cell_type":"code","source":"# Load best weights\nmodel.load_weights(\"/kaggle/working/ckpt.h5\")\n\n# Compute & save best Test result\nprint(\"\\n>> Valid Result:\")\nvalid_result = model.evaluate(\n    get_dataset(\n        VALID_FILENAMES,\n        batch_size=BATCH_SIZE,\n        augment=False,\n        shuffle=False,\n        repeat=False,\n        cache=False,\n    ),\n    return_dict=True,\n    verbose=1,\n)\nprint()\n\n# Compute & save best Test result\nprint(\"\\n>> Test Result:\")\ntest_result = model.evaluate(\n    get_dataset(\n        TEST_FILENAMES,\n        batch_size=BATCH_SIZE,\n        augment=False,\n        shuffle=False,\n        repeat=False,\n        cache=False,\n    ),\n    return_dict=True,\n    verbose=1,\n)\nprint()\n\n# Log in wandb\nif CFG.wandb:\n    best_epoch = np.argmax(history[\"val_f1_score\"]) + 1\n    wandb.log({\"best\": {\"valid\":valid_result,\n                        \"test\":test_result,\n                        \"epoch\":best_epoch}})\n    wandb.run.finish()\n\n# Get Prediction for test data\ntest_ds = get_dataset(TEST_FILENAMES,\n    shuffle=False,\n    augment=False,\n    repeat=False,\n    batch_size=BATCH_SIZE,\n    cache=False,\n    drop_remainder=False,\n    return_id=False,\n    return_label=False,\n)\ntest_preds = model.predict(test_ds, verbose=1, steps=NUM_TEST/BATCH_SIZE)\n\n# Extract test metadata from tfrecord\ntest_ds = get_dataset(TEST_FILENAMES,\n    shuffle=False,\n    augment=False,\n    repeat=False,\n    batch_size=1,\n    cache=False,\n    drop_remainder=False,\n    return_id=True,\n    return_label=True,\n)\ninfo = [(id_.numpy()[0].decode('utf-8'),label.numpy()[0]) for _,label,id_ in tqdm(iter(test_ds),total=NUM_TEST)]\ntest_ids, test_labels = list(zip(*info))\n\n# Plot Confusion Matrix\nprint(\"\\n>> Confusoin Matrix\")\ncm = confusion_matrix(test_labels, test_preds.reshape(-1).round())\nplt.figure(figsize=(6,6))\nplot_confusion_matrix(cm, [\"Real\",\"Fake\"],normalize=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-23T22:41:32.113032Z","iopub.execute_input":"2022-06-23T22:41:32.113743Z","iopub.status.idle":"2022-06-23T22:47:23.257092Z","shell.execute_reply.started":"2022-06-23T22:41:32.113706Z","shell.execute_reply":"2022-06-23T22:47:23.256099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 17. Reference âœï¸\n* [audio_classification_models](https://github.com/awsaf49/audio_classification_models) [TensorFlow]\n* [TensorflowASR](https://github.com/TensorSpeech/TensorFlowASR) [Tensorflow]\n* [Conformer](https://arxiv.org/abs/2005.08100) [Paper]\n* [ASVspoof 2019](https://datashare.ed.ac.uk/handle/10283/3336) [Dataset]","metadata":{}},{"cell_type":"markdown","source":"# 18. Remove Files âœ‚ï¸","metadata":{}},{"cell_type":"code","source":"import shutil\ntry:\n    !rm -r ./wandb\nexcept:\n    pass","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-23T21:32:39.625038Z","iopub.status.idle":"2022-06-23T21:32:39.625506Z","shell.execute_reply.started":"2022-06-23T21:32:39.62527Z","shell.execute_reply":"2022-06-23T21:32:39.625291Z"},"trusted":true},"execution_count":null,"outputs":[]}]}