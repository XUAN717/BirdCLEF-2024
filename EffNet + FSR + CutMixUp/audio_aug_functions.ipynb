{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70203,"databundleVersionId":8068726,"sourceType":"competition"},{"sourceId":1664376,"sourceType":"datasetVersion","datasetId":985270},{"sourceId":5181249,"sourceType":"datasetVersion","datasetId":3012199},{"sourceId":8108072,"sourceType":"datasetVersion","datasetId":4789213},{"sourceId":8319412,"sourceType":"datasetVersion","datasetId":4941521},{"sourceId":8478505,"sourceType":"datasetVersion","datasetId":5056677},{"sourceId":8605414,"sourceType":"datasetVersion","datasetId":5149186}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":72.272366,"end_time":"2024-05-27T08:24:12.863444","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-27T08:23:00.591078","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"087aee61943640ff866fad504d3bb8b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2412ab76a24d443693ab5c99798255d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"3051ba21b6bb4960912b45a267d7954e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b524d1526854b07a9cbd17a164389ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3051ba21b6bb4960912b45a267d7954e","placeholder":"​","style":"IPY_MODEL_087aee61943640ff866fad504d3bb8b7","value":""}},"603297b6cf5a4516b17db45b449d8bcd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aee52a129532428f97b4ffa466195640","placeholder":"​","style":"IPY_MODEL_98758643ec6f4277946a0b2523b33a62","value":" 0/0 [00:00&lt;?, ?it/s]"}},"7de4c22f5bde4876bd7045b85b99ed48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b524d1526854b07a9cbd17a164389ac","IPY_MODEL_919bacdd323e48c4a80b2ef6b3bdc060","IPY_MODEL_603297b6cf5a4516b17db45b449d8bcd"],"layout":"IPY_MODEL_85db54c088da49578b1a07af47f9565e"}},"85db54c088da49578b1a07af47f9565e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"919bacdd323e48c4a80b2ef6b3bdc060":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2412ab76a24d443693ab5c99798255d6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd98675913fb42bca53f7133abd10dae","value":0}},"98758643ec6f4277946a0b2523b33a62":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aee52a129532428f97b4ffa466195640":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd98675913fb42bca53f7133abd10dae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\"> 1 | Installing Libraries </p>","metadata":{"papermill":{"duration":0.023488,"end_time":"2024-05-27T08:23:03.890039","exception":false,"start_time":"2024-05-27T08:23:03.866551","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\n    !pip install /kaggle/input/onnxruntime/humanfriendly-10.0-py2.py3-none-any.whl --no-index --find-links /kaggle/input/onnxruntime\n    !pip install /kaggle/input/onnxruntime/coloredlogs-15.0.1-py2.py3-none-any.whl --no-index --find-links /kaggle/input/onnxruntime\n    !pip install /kaggle/input/onnxruntime/onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl --no-index --find-links /kaggle/input/onnxruntime\n    !pip install albumentations","metadata":{"papermill":{"duration":47.930075,"end_time":"2024-05-27T08:23:51.84393","exception":false,"start_time":"2024-05-27T08:23:03.913855","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-09T10:05:07.037530Z","iopub.execute_input":"2024-06-09T10:05:07.038400Z","iopub.status.idle":"2024-06-09T10:06:20.040030Z","shell.execute_reply.started":"2024-06-09T10:05:07.038364Z","shell.execute_reply":"2024-06-09T10:06:20.038744Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/onnxruntime\nProcessing /kaggle/input/onnxruntime/humanfriendly-10.0-py2.py3-none-any.whl\nhumanfriendly is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nLooking in links: /kaggle/input/onnxruntime\nProcessing /kaggle/input/onnxruntime/coloredlogs-15.0.1-py2.py3-none-any.whl\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs==15.0.1) (10.0)\ncoloredlogs is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nLooking in links: /kaggle/input/onnxruntime\nProcessing /kaggle/input/onnxruntime/onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from onnxruntime==1.17.3) (15.0.1)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime==1.17.3) (23.5.26)\nRequirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from onnxruntime==1.17.3) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from onnxruntime==1.17.3) (21.3)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime==1.17.3) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime==1.17.3) (1.12)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->onnxruntime==1.17.3) (10.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->onnxruntime==1.17.3) (3.1.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime==1.17.3) (1.3.0)\nonnxruntime is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nRequirement already satisfied: albumentations in /opt/conda/lib/python3.10/site-packages (1.4.0)\nRequirement already satisfied: numpy>=1.24.4 in /opt/conda/lib/python3.10/site-packages (from albumentations) (1.26.4)\nRequirement already satisfied: scipy>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from albumentations) (1.11.4)\nRequirement already satisfied: scikit-image>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.22.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from albumentations) (6.0.1)\nRequirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.0.4)\nRequirement already satisfied: opencv-python>=4.9.0 in /opt/conda/lib/python3.10/site-packages (from albumentations) (4.9.0.80)\nRequirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (1.2.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (4.9.0)\nRequirement already satisfied: opencv-python-headless>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (4.9.0.80)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (3.2.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (9.5.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (2.33.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (2023.12.9)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (21.3)\nRequirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image>=0.21.0->albumentations) (3.1.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.2.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">2 | Importing Libraries </p>","metadata":{"papermill":{"duration":0.025691,"end_time":"2024-05-27T08:23:51.894478","exception":false,"start_time":"2024-05-27T08:23:51.868787","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport glob\nimport time\nimport shutil\nimport random\nimport ast\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\nimport onnx\nimport onnxruntime as ort\nimport wandb\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedGroupKFold\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom tqdm.notebook import tqdm\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom torch.cuda import amp\nimport torch\nprint(f\"pytorch version is {torch.__version__}\")\nimport torch.nn as nn\nfrom torch.cuda import amp\n\nisTrain = True\nname = 'bird2024exp1057'\n\nimport torchvision\nfrom torchvision.transforms import v2 as transforms\n\nimport librosa\nimport torchaudio\nimport torchaudio.transforms as audioT\n\nimport timm\nfrom albumentations import Compose,OneOf","metadata":{"papermill":{"duration":13.435679,"end_time":"2024-05-27T08:24:05.355498","exception":false,"start_time":"2024-05-27T08:23:51.919819","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-09T10:08:45.807387Z","iopub.execute_input":"2024-06-09T10:08:45.808270Z","iopub.status.idle":"2024-06-09T10:08:45.819440Z","shell.execute_reply.started":"2024-06-09T10:08:45.808238Z","shell.execute_reply":"2024-06-09T10:08:45.818424Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"pytorch version is 2.1.2+cpu\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">3 | Configuration Class </p>\n\n","metadata":{"papermill":{"duration":0.026074,"end_time":"2024-05-27T08:24:05.406693","exception":false,"start_time":"2024-05-27T08:24:05.380619","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"---","metadata":{"papermill":{"duration":0.024724,"end_time":"2024-05-27T08:24:05.457874","exception":false,"start_time":"2024-05-27T08:24:05.43315","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class config:\n    dir = \"/kaggle/input/birdclef-2024/\"\n\n\n    wave_path = \"original_waves/second_30/\"\n\n    model_name = 'tf_efficientnet_b0'\n\n    pool_type = 'avg'\n\n    \n    train_duration = 30 \n    slice_duration = 5 \n\n    test_duration = 5\n\n    train_drop_duration = 1\n    \n    # spectrogram parameters\n    sr = 32000\n    fmin = 20\n    fmax = 15000\n\n    n_mels = 128\n    n_fft = n_mels*8\n    size_x = 512\n    \n    hop_length = int(sr*slice_duration / size_x)\n    test_hop_length = int(sr*test_duration / size_x)\n    \n    bins_per_octave = 12\n\n    nfolds = 5\n    inference_folds = [4]\n    \n    enable_amp = True\n    train_batchsize = 32\n    valid_batchsize = 1\n\n    # loss_type = \"BCEWithLogitsLoss\"\n    loss_type = \"BCEFocalLoss\"\n\n    lr = 1.0e-04 \n\n\n    optimizer='adan'\n    weight_decay = 1.0e-02\n    es_patience =  5\n    deterministic = True\n    enable_amp = True\n\n    max_epoch = 9\n    aug_epoch = 6\n    \n\n    useSecondary =True\n    secondary_label_value = 0.5\n    oversample =False\n    oversample_threthold = 60\n    \n    seed = 42\n\n    wandb = True\n\n    ###augmentation flags\n    aug_noise            = 0.02        #0.\n    aug_gain             = 0.6         #0.0概率值\n    aug_wave_pitchshift  = 1.0         #0.0\n    aug_wave_shift       = 1.0         #0.\n\n    aug_spec_xymasking   = 0.15        #0.\n    aug_spec_coarsedrop  = 0.02        #0.\n    aug_spec_hflip       = 1           #0.\n\n\n    ##mixup param\n    aug_wave_mixup       = 1.0\n    aug_spec_mixup       = 0.0\n    aug_spec_mixup_prob  = 0.5 \n    alpha=0.95\n\n    smoothing_value      = 0.0\n    # spec_mix_mask_percent = 20\n    \ncfg = config()\n\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n\nprint(device)","metadata":{"papermill":{"duration":0.045294,"end_time":"2024-05-27T08:24:05.579365","exception":false,"start_time":"2024-05-27T08:24:05.534071","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-09T10:08:49.785076Z","iopub.execute_input":"2024-06-09T10:08:49.785726Z","iopub.status.idle":"2024-06-09T10:08:49.798216Z","shell.execute_reply.started":"2024-06-09T10:08:49.785692Z","shell.execute_reply":"2024-06-09T10:08:49.797056Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">4 | Data Augmentation Pipeline For Training </p>","metadata":{"papermill":{"duration":0.024909,"end_time":"2024-05-27T08:24:05.629426","exception":false,"start_time":"2024-05-27T08:24:05.604517","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def Gain(p, y, min_gain_in_db=-15, max_gain_in_db=15):\n    \n#     对音频信号 y 应用一个随机增益。\n    \n#     参数:\n#     y : np.ndarray\n#         音频信号。\n#     min_gain_in_db : float\n#         增益的最小分贝值。\n#     max_gain_in_db : float\n#         增益的最大分贝值。\n#     p : float\n#         应用增益的概率。\n        \n#     返回:\n#     y_gained : np.ndarray\n#         应用了增益的音频信号。\n   \n    if np.random.rand() < p:\n        # 随机选择增益值\n        gain_db = np.random.uniform(min_gain_in_db, max_gain_in_db)\n        # 将分贝转换为线性比例\n        gain = 10 ** (gain_db / 20)\n        # 应用增益\n        y_gained = y * gain\n    else:\n        y_gained = y\n    return y_gained\n   \ndef GainTransition(y, sr, min_gain_in_db=-24.0, max_gain_in_db=6.0, min_duration=0.2, max_duration=6.0, p=1.0):\n    \n#     在指定的时间段内对音频信号 y 逐渐改变增益。\n    \n#     参数:\n#     y : np.ndarray\n#         音频信号。\n#     min_gain_in_db : float\n#         增益的最小分贝值。\n#     max_gain_in_db : float\n#         增益的最大分贝值。\n#     min_duration : float\n#         增益变化的最小持续时间（秒）。\n#     max_duration : float\n#         增益变化的最大持续时间（秒）。\n#     p : float\n#         应用增益转换的概率。\n#     sr : int\n#         采样率。\n        \n#     返回:\n#     y_transitioned : np.ndarray\n#         应用了增益转换的音频信号。\n    \n    if np.random.rand() < p:\n        # 计算增益变化的持续时间（样本数）\n        duration_samples = int(np.random.uniform(min_duration, max_duration) * sr)\n        # 随机选择起始增益和结束增益\n        start_gain_db = np.random.uniform(min_gain_in_db, max_gain_in_db)\n        end_gain_db = np.random.uniform(min_gain_in_db, max_gain_in_db)\n        # 将分贝转换为线性比例\n        start_gain = 10 ** (start_gain_db / 20)\n        end_gain = 10 ** (end_gain_db / 20)\n        \n        # 应用增益转换\n        t = np.arange(len(y))\n        gain = np.interp(t, [0, duration_samples], [start_gain, end_gain])\n        y_transitioned = y * gain\n    else:\n        y_transitioned = y\n    return y_transitioned\n#增加高斯噪音，输入x为波形wave\ndef AddGaussianNoise(x,w=0.004,p=1):\n    if p > random.random():\n        output = x + w * np.random.normal(loc=0, scale=1, size=len(x))\n        return output\n    else:\n        return x\n    \n#\"彩色噪声\" 通常指的是具有特定频率分布的噪声   \ndef AddColorNoise(y, sr, p=1, min_snr_db=5, max_snr_db=20, min_f_decay=0.01, max_f_decay=0.1):\n    \n#     在音频信号 y 中添加彩色噪声。\n    \n#     参数:\n#     y : np.ndarray\n#         音频信号。\n#     sr : int\n#         采样率。\n#     p : float\n#         添加噪声的概率。\n#     min_snr_db : float\n#         最小信噪比，单位为分贝。\n#     max_snr_db : float\n#         最大信噪比，单位为分贝。\n#     min_f_decay : float\n#         最小频率衰减。\n#     max_f_decay : float\n#         最大频率衰减。\n        \n#     返回:\n#     noisy_y : np.ndarray\n#         添加了彩色噪声的音频信号。\n    if np.random.rand() < p:\n        # 随机选择信噪比\n        snr_db = np.random.uniform(min_snr_db, max_snr_db)\n        \n        # 计算信号功率\n        signal_power = np.sum(y**2) / len(y)\n        \n        # 根据信噪比计算噪声功率\n        noise_power = signal_power / 10**(snr_db / 10)\n        \n        # 计算噪声\n        noise = np.random.normal(0, np.sqrt(noise_power), y.shape)\n        \n        # 将噪声添加到信号中\n        y_noisy = y + noise\n        \n        # 频率衰减计算\n        f_decay = np.random.uniform(min_f_decay, max_f_decay)\n        # 计算频率响应\n        freq_response = np.exp(-f_decay * np.arange(0, sr) / sr)\n        \n        # 应用频率响应到噪声上\n        y_noisy = librosa.istft(librosa.stft(y_noisy) * freq_response)\n        \n    else:\n        y_noisy = y\n\n    return y_noisy\n    \ndef PitchShift(x, sr, n_steps, bins_per_octave=12):\n    # sr: 音频采样率\n    # n_steps: 要移动多少步\n    # bins_per_octave: 每个八度音阶(半音)多少步\n    if p > random.random():\n        return librosa.effects.pitch_shift(x, sr, n_steps, bins_per_octave=bins_per_octave)\n    else:\n        return x\n    \n#波形位移,时间单位？\ndef Shift(x,shift_time,p):\n    if p > random.random():\n        return np.roll(x, int(shift_time))\n    else:\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:10:36.576958Z","iopub.execute_input":"2024-06-09T10:10:36.578022Z","iopub.status.idle":"2024-06-09T10:10:36.600940Z","shell.execute_reply.started":"2024-06-09T10:10:36.577961Z","shell.execute_reply":"2024-06-09T10:10:36.599818Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"if isTrain== True:\n\n    #针对音频增强？\n    normal_augment = Compose([\n        OneOf([\n            Gain(samples, p=1.0, min_gain_in_db=-15, max_gain_in_db=15),\n            GainTransition(samples,cfg.sr, min_gain_in_db=-24.0, max_gain_in_db=6.0,\n                           min_duration=0.2, max_duration=6.0,  p=1.0)\n        ], p=cfg.aug_gain),\n        \n        OneOf([\n            AddGaussianNoise(samples,w=0.004,p=1),\n            AddColorNoise(samples, cfg.sr, p=1, min_snr_db=5, max_snr_db=20, min_f_decay=0.01, max_f_decay=0.1)\n        ],p=cfg.aug_noise),\n\n    \n        PitchShift(samples, cfg.sr, n_steps=6, bins_per_octave=12),\n        Shift(samples, shift_time = 2,p=cfg.aug_wave_shift)\n    ])\n    #针对图像增强？\n    alb_transform = [\n        albumentations.XYMasking(num_masks_x=2, num_masks_y=1, \n                                 mask_x_length=cfg.size_x//30, mask_y_length=cfg.n_mels//30,\n                                 fill_value=0, mask_fill_value=0, p=cfg.aug_spec_xymasking),\n        albumentations.CoarseDropout(fill_value=0, min_holes=20, max_holes=50, p=cfg.aug_spec_coarsedrop),\n        albumentations.HorizontalFlip(p=cfg.aug_spec_hflip)    \n    ]\n    albumentations_augment = albumentations.Compose(alb_transform)","metadata":{"papermill":{"duration":0.041072,"end_time":"2024-05-27T08:24:05.746803","exception":false,"start_time":"2024-05-27T08:24:05.705731","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-09T10:10:40.934345Z","iopub.execute_input":"2024-06-09T10:10:40.934730Z","iopub.status.idle":"2024-06-09T10:10:41.138495Z","shell.execute_reply.started":"2024-06-09T10:10:40.934704Z","shell.execute_reply":"2024-06-09T10:10:41.137040Z"},"trusted":true},"execution_count":78,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[78], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isTrain\u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#针对音频增强？\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     normal_augment \u001b[38;5;241m=\u001b[39m Compose([\n\u001b[1;32m      5\u001b[0m         OneOf([\n\u001b[0;32m----> 6\u001b[0m             Gain(\u001b[43msamples\u001b[49m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, min_gain_in_db\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m15\u001b[39m, max_gain_in_db\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m),\n\u001b[1;32m      7\u001b[0m             GainTransition(samples,cfg\u001b[38;5;241m.\u001b[39msr, min_gain_in_db\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m24.0\u001b[39m, max_gain_in_db\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6.0\u001b[39m,\n\u001b[1;32m      8\u001b[0m                            min_duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, max_duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6.0\u001b[39m,  p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m      9\u001b[0m         ], p\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39maug_gain),\n\u001b[1;32m     10\u001b[0m         \n\u001b[1;32m     11\u001b[0m         OneOf([\n\u001b[1;32m     12\u001b[0m             AddGaussianNoise(samples,w\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.004\u001b[39m,p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     13\u001b[0m             AddColorNoise(sampels, cfg\u001b[38;5;241m.\u001b[39msr, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, min_snr_db\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, max_snr_db\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, min_f_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, max_f_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     14\u001b[0m         ],p\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39maug_noise),\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m     \n\u001b[1;32m     17\u001b[0m         PitchShift(samples, cfg\u001b[38;5;241m.\u001b[39msr, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, bins_per_octave\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m),\n\u001b[1;32m     18\u001b[0m         Shift(samples, shift_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m,p\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39maug_wave_shift)\n\u001b[1;32m     19\u001b[0m     ])\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#针对图像增强？\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     alb_transform \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     22\u001b[0m         albumentations\u001b[38;5;241m.\u001b[39mXYMasking(num_masks_x\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_masks_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m     23\u001b[0m                                  mask_x_length\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39msize_x\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m30\u001b[39m, mask_y_length\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mn_mels\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m         albumentations\u001b[38;5;241m.\u001b[39mHorizontalFlip(p\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39maug_spec_hflip)    \n\u001b[1;32m     27\u001b[0m     ]\n","\u001b[0;31mNameError\u001b[0m: name 'samples' is not defined"],"ename":"NameError","evalue":"name 'samples' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">5 |  Mixup Data Augmentation Function </p>\n","metadata":{"papermill":{"duration":0.024922,"end_time":"2024-05-27T08:24:05.797788","exception":false,"start_time":"2024-05-27T08:24:05.772866","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def mixup(data, targets, alpha, mode=\"same_wave\"):\n    \n    if mode == \"same_wave\":\n        data = torch.tensor(data)\n        indices = torch.randperm(data.size(0))\n        shuffled_data = data[indices]\n\n        lam = np.random.beta(alpha, alpha)\n        new_data = data * lam + shuffled_data * (1 - lam)\n        return new_data.numpy()\n        \n    elif mode == \"other_wave\":\n        indices = torch.randperm(data.size(0))\n        shuffled_data = data[indices]\n        shuffled_targets = targets[indices]\n    \n        lam = np.random.beta(alpha, alpha)\n        new_data = data * lam + shuffled_data * (1 - lam)\n        new_targets = targets * lam + shuffled_targets * (1 - lam)\n    \n        return new_data, new_targets","metadata":{"papermill":{"duration":0.040647,"end_time":"2024-05-27T08:24:05.864611","exception":false,"start_time":"2024-05-27T08:24:05.823964","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-09T12:04:28.646249Z","iopub.execute_input":"2024-06-09T12:04:28.646961Z","iopub.status.idle":"2024-06-09T12:04:28.677531Z","shell.execute_reply.started":"2024-06-09T12:04:28.646930Z","shell.execute_reply":"2024-06-09T12:04:28.676756Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">6 | Spectral Mixup Function</p>\n\n\n\n","metadata":{"papermill":{"duration":0.024581,"end_time":"2024-05-27T08:24:05.914281","exception":false,"start_time":"2024-05-27T08:24:05.8897","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"---","metadata":{"papermill":{"duration":0.027032,"end_time":"2024-05-27T08:24:05.966477","exception":false,"start_time":"2024-05-27T08:24:05.939445","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if isTrain== True:\n    spec_xymasking = albumentations.XYMasking(num_masks_x=2, num_masks_y=1, \n                                              mask_x_length=cfg.size_x // 10, mask_y_length=cfg.n_mels // 10,\n                                              fill_value=0, mask_fill_value=0, p=1)\n\ndef spec_mixup(data, targets):\n    type = data.dtype\n\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_targets = targets[indices]\n\n    data = np.array(data)\n    data_transposed = np.transpose(data, (2, 3, 1, 0))\n    data_transposed = spec_xymasking(image=data_transposed)[\"image\"]\n    data_transposed = np.transpose(data_transposed, (3, 2, 0, 1))  \n\n    diff = data - data_transposed\n    mask = (diff != 0).astype(int)\n\n    shuffled_data_masked = (shuffled_data * mask)\n\n    new_data = torch.tensor(data_transposed, dtype=type) + torch.tensor(shuffled_data_masked, dtype=type)\n\n    lam = mask.sum() / len(data) / (cfg.n_mels*cfg.size_x)\n    new_targets = targets * (1-lam) + shuffled_targets *lam\n\n    return new_data, new_targets","metadata":{"papermill":{"duration":0.042092,"end_time":"2024-05-27T08:24:06.084224","exception":false,"start_time":"2024-05-27T08:24:06.042132","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-09T12:04:34.158723Z","iopub.execute_input":"2024-06-09T12:04:34.159382Z","iopub.status.idle":"2024-06-09T12:04:34.468275Z","shell.execute_reply.started":"2024-06-09T12:04:34.159345Z","shell.execute_reply":"2024-06-09T12:04:34.467044Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43misTrain\u001b[49m\u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     spec_xymasking \u001b[38;5;241m=\u001b[39m albumentations\u001b[38;5;241m.\u001b[39mXYMasking(num_masks_x\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_masks_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m      3\u001b[0m                                               mask_x_length\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39msize_x \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m, mask_y_length\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mn_mels \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      4\u001b[0m                                               fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, mask_fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspec_mixup\u001b[39m(data, targets):\n","\u001b[0;31mNameError\u001b[0m: name 'isTrain' is not defined"],"ename":"NameError","evalue":"name 'isTrain' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"\n# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">7 | Mel Spectrogram Generation </p>\n\n","metadata":{"papermill":{"duration":0.02796,"end_time":"2024-05-27T08:24:06.137985","exception":false,"start_time":"2024-05-27T08:24:06.110025","status":"completed"},"tags":[]}},{"cell_type":"code","source":"spec_layer = torchaudio.transforms.MelSpectrogram(\n    sample_rate=cfg.sr, hop_length=cfg.hop_length, n_fft=cfg.n_fft,\n    n_mels=cfg.n_mels,f_min=cfg.fmin,f_max=cfg.fmax,mel_scale='slaney',center=True, pad_mode='reflect'\n).to(device)\n\nvalid_spec_layer = torchaudio.transforms.MelSpectrogram(\n    sample_rate=cfg.sr, hop_length=cfg.test_hop_length, n_fft=cfg.n_fft,\n    n_mels=cfg.n_mels,f_min=cfg.fmin,f_max=cfg.fmax,mel_scale='slaney',center=True, pad_mode='reflect'\n).to(device)\n\ntest_spec_layer = torchaudio.transforms.MelSpectrogram(\n    sample_rate=cfg.sr, hop_length=cfg.test_hop_length, n_fft=cfg.n_fft,\n    n_mels=cfg.n_mels,f_min=cfg.fmin,f_max=cfg.fmax,mel_scale='slaney',center=True, pad_mode='reflect'\n).cpu()","metadata":{"papermill":{"duration":0.181583,"end_time":"2024-05-27T08:24:06.34538","exception":false,"start_time":"2024-05-27T08:24:06.163797","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">8 | Data Preparation Steps</p>\n\n\n \n","metadata":{"papermill":{"duration":0.024773,"end_time":"2024-05-27T08:24:06.395921","exception":false,"start_time":"2024-05-27T08:24:06.371148","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sample_submission = pd.read_csv(cfg.dir+\"sample_submission.csv\")\nLABELS = list(sample_submission.set_index(\"row_id\").columns)\nLABELS[:5]\ntrain_csv = pd.read_csv(cfg.dir+\"train_metadata.csv\")\ntrain_csv['new_target'] = train_csv['primary_label'] + ' ' + train_csv['secondary_labels'].map(lambda x: ' '.join(ast.literal_eval(x)))\ntrain_csv['len_new_target'] =train_csv['new_target'].map(lambda x: len(x.split()))\ntrain_csv[\"len_new_target\"].value_counts().plot(kind=\"bar\", figsize=(4,2))\ntrain_csv[\"filename_tmp\"] = train_csv[\"filename\"].map(lambda x:x.split(\"/\")[1][:-4])\nduplicated_filenames = train_csv[\"filename_tmp\"].value_counts()[train_csv[\"filename_tmp\"].value_counts() > 1].index\ntrain_csv = train_csv[~train_csv[\"filename_tmp\"].isin(duplicated_filenames)]\ntrain_csv = train_csv.reset_index(drop=True)","metadata":{"papermill":{"duration":0.945215,"end_time":"2024-05-27T08:24:07.431523","exception":false,"start_time":"2024-05-27T08:24:06.486308","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\"> 9 | BirdCLEF Dataset Preparation </p>\n\n","metadata":{"papermill":{"duration":0.026002,"end_time":"2024-05-27T08:24:07.483224","exception":false,"start_time":"2024-05-27T08:24:07.457222","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class BirdCLEF_Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, augmentation=False, mode='train'):\n        if mode == 'train':\n            self.df = df.reset_index(drop=True)\n        elif mode == 'valid':\n            self.df = df.reset_index(drop=True)\n        else:\n            self.df = df\n        self.mode = mode\n        self.augmentation = augmentation\n    \n    def __len__(self):\n        return len(self.df)\n\n    def normalize(self, x):\n        valid_values = x[x != float('-inf')]\n        mean_value = np.mean(valid_values)\n        x[x == float('-inf')] = mean_value\n        \n\n        x = x - x.min()\n        x = x / x.max()\n        return x\n\n    def wave_tile_and_cutoff(self, data):\n      \n        drop_duration = cfg.sr*cfg.train_drop_duration\n        use_duration  = cfg.sr*cfg.train_duration\n        \n        if len(data[0]) > drop_duration: \n            data = data[:,drop_duration:]\n\n        if len(data[0]) < use_duration:\n            iter = 1 + (use_duration) // len(data[0])\n            data = np.tile(data, (1, iter))\n\n        data = data[:,:use_duration]\n        return data\n\n    def label_smoothing(self, idx, target):\n    \n        secondary_target = target * cfg.secondary_label_value\n    \n        out_of_target_noise_intensity = cfg.smoothing_value/(len(LABELS)-1) \n        out_of_target_noise_array = torch.ones(target.shape) * out_of_target_noise_intensity\n        \n        secondary_target_with_noise = secondary_target + out_of_target_noise_array\n        secondary_target_with_noise = torch.clip(secondary_target_with_noise, min=0, max=cfg.secondary_label_value)\n    \n        primary_target = np.isin(LABELS, self.df.loc[idx, \"primary_label\"]).astype(int)\n        primary_target = torch.tensor(primary_target, dtype=torch.float32)\n\n        primary_and_secondary_target_with_noise = primary_target + secondary_target_with_noise\n        new_target = torch.clip(primary_and_secondary_target_with_noise, min=0, max=1)\n    \n        new_target = new_target - primary_target * cfg.smoothing_value\n    \n        return new_target\n\n    \n    def __getitem__(self, idx):\n\n        if self.mode == 'train':\n\n          \n            if cfg.useSecondary == True:\n                target = np.isin(LABELS, self.df.loc[idx, \"new_target\"].split()).astype(int)\n            else:\n                target = np.isin(LABELS, self.df.loc[idx, \"primary_label\"].split()).astype(int)\n            target = torch.tensor(target, dtype=torch.float32)\n          \n            target = self.label_smoothing(idx, target)\n            \n            fileID = self.df.loc[idx, 'fileID'] \n            \n            path = f\"{cfg.wave_path}{fileID}.npy\"\n            wave = np.load(path)\n            \n\n      \n            wave = self.wave_tile_and_cutoff(data=wave)\n\n            \n            input_duration = cfg.sr * cfg.slice_duration\n            \n            \n            if self.augmentation == True:\n               \n                if cfg.aug_wave_mixup > np.random.random():\n                    #train_duration -> slice_duration\n                    wave_reshape = wave.reshape(-1, input_duration)\n                    wave = mixup(data=wave_reshape, targets=target, alpha=cfg.alpha, mode=\"same_wave\")\n                    wave = wave[:1,:]\n                else:\n                    wave = wave[:, :input_duration]\n                \n     \n                wave = normal_augment(samples=wave, sample_rate=cfg.sr)\n\n    \n                wave = torch.tensor(wave).to(device)\n                mel_spec = spec_layer(wave)\n                mel_spec = np.array(mel_spec.cpu())\n\n                mel_spec = np.log(mel_spec)\n                for i in range(len(mel_spec)):\n                    mel_spec[i] = self.normalize(mel_spec[i])\n                mel_spec = torch.tensor(mel_spec)\n                mel_spec = mel_spec[:,:,:cfg.size_x]\n\n     \n                mel_spec = np.array(mel_spec.cpu())\n                mel_spec = np.transpose(mel_spec, (1, 2, 0))                \n                mel_spec = albumentations_augment(image=mel_spec)[\"image\"]\n                mel_spec = np.transpose(mel_spec, (2, 0, 1))\n\n\n                \n            else:\n                wave = wave[:, :input_duration]\n                \n                wave = torch.tensor(wave).to(device)\n                mel_spec = spec_layer(wave)\n                mel_spec = np.array(mel_spec.cpu())\n\n                mel_spec = np.log(mel_spec)\n\n                for i in range(len(mel_spec)):\n                    mel_spec[i] = self.normalize(mel_spec[i])\n                    \n\n                mel_spec = torch.tensor(mel_spec)\n                mel_spec = mel_spec[:,:,:cfg.size_x]\n\n            \n            mel_spec = torch.tensor(mel_spec)\n\n            \n            return mel_spec, target\n\n        elif self.mode == 'valid':\n            \n\n            if cfg.useSecondary == True:\n                target = np.isin(LABELS, self.df.loc[idx, \"new_target\"].split()).astype(int)\n            else:\n                target = np.isin(LABELS, self.df.loc[idx, \"primary_target\"].split()).astype(int)\n            target = torch.tensor(target, dtype=torch.float32)\n            \n            fileID = self.df.loc[idx, 'fileID'] \n            \n            path = f\"{cfg.wave_path}{fileID}.npy\"\n            wave = np.load(path)\n\n            wave = self.wave_tile_and_cutoff(data=wave)\n\n            input_duration = cfg.sr*cfg.test_duration\n            wave_reshape = wave.reshape(-1, input_duration)\n\n            wave_reshape = torch.tensor(wave_reshape).to(device)\n            mel_specs = valid_spec_layer(wave_reshape)\n            mel_specs = mel_specs.cpu().numpy()\n\n            mel_specs = np.log(mel_specs)\n            for i in range(len(mel_specs)):\n                mel_specs[i] = self.normalize(mel_specs[i])\n            mel_specs = torch.tensor(mel_specs)\n            \n            mel_specs = mel_specs[:,:,:cfg.size_x]\n\n            targets = torch.tile(target, dims=(mel_specs.shape[0],1))\n            return mel_specs, targets\n\n        elif self.mode == 'test':\n\n            filepath = self.df[idx]\n            wave, _  = torchaudio.load(filepath)\n            wave = wave[:,:60*4*32000]\n\n            wave_reshaped = wave.reshape(-1, 1, cfg.test_duration*cfg.sr)\n            \n            mel_spec = test_spec_layer(wave_reshaped)\n            mel_spec = np.log(mel_spec)\n\n            mel_spec = np.array(mel_spec)\n            for i in range(len(mel_spec)):\n                mel_spec[i] = self.normalize(mel_spec[i])\n            mel_spec = torch.tensor(mel_spec)\n\n            mel_spec = mel_spec[:,:,:cfg.size_x]\n            return mel_spec\n\n        elif self.mode == 'clean':\n\n            filepath = self.df[idx]\n            wave, _  = torchaudio.load(filepath)\n\n            wave = wave[:, :6*cfg.test_duration*cfg.sr]\n\n            chunk_length = len(wave[0]) // (cfg.test_duration*cfg.sr)\n            \n            wave = wave[:,:chunk_length*cfg.test_duration*cfg.sr]\n\n            wave_reshaped = wave.reshape(-1, 1, cfg.test_duration*cfg.sr)\n            \n            mel_spec = test_spec_layer(wave_reshaped)\n            mel_spec = np.log(mel_spec)\n\n            mel_spec = np.array(mel_spec)\n            for i in range(len(mel_spec)):\n                mel_spec[i] = self.normalize(mel_spec[i])\n            mel_spec = torch.tensor(mel_spec)\n\n            return mel_spec, filepath","metadata":{"papermill":{"duration":0.078512,"end_time":"2024-05-27T08:24:07.588078","exception":false,"start_time":"2024-05-27T08:24:07.509566","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if isTrain:\n    print(\"train data\")\n    dataset = BirdCLEF_Dataset(df=train_csv, augmentation=True,  mode=\"train\")\n    data, target = dataset[270]\n    fig, ax = plt.subplots(figsize=(6,4))\n    plt.imshow(data[0], cmap=\"jet\", origin=\"lower\")\n    plt.show()\n    \n    print(\"validation data\")\n    dataset = BirdCLEF_Dataset(df=train_csv, augmentation=True,  mode=\"valid\")\n    data, target = dataset[270]\n    fig, axes = plt.subplots(figsize=(12,8), nrows=len(data), tight_layout=True)\n    for idx, ax in enumerate(axes.ravel()):\n        ax.imshow(data[idx], cmap=\"jet\", origin=\"lower\")","metadata":{"papermill":{"duration":0.040422,"end_time":"2024-05-27T08:24:07.705757","exception":false,"start_time":"2024-05-27T08:24:07.665335","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">10 | BirdModel : Flexible Pooling Architecture For Bird Sound Classification </p>\n\n\n\n","metadata":{"papermill":{"duration":0.025839,"end_time":"2024-05-27T08:24:07.757093","exception":false,"start_time":"2024-05-27T08:24:07.731254","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class BirdModel(torch.nn.Module):\n    def __init__(self, model_name, pretrained, in_channels, num_classes, pool=\"default\"):\n        super().__init__()\n\n        self.pool = pool\n        self.normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        \n        if pool == \"default\":\n            self.backbone = timm.create_model(\n                model_name=model_name, pretrained=pretrained,\n                num_classes=0, in_chans=3)\n        else:\n            self.backbone = timm.create_model(\n                model_name=model_name, pretrained=pretrained,\n                num_classes=0, in_chans=3, global_pool=\"\")\n\n        in_features = self.backbone.num_features\n\n\n\n        self.max_pooling = torch.nn.Sequential(torch.nn.AdaptiveMaxPool2d(1),\n                                               torch.nn.Flatten(start_dim=1, end_dim=-1))\n        self.avg_pooling = torch.nn.Sequential(torch.nn.AdaptiveAvgPool2d(1),\n                                               torch.nn.Flatten(start_dim=1, end_dim=-1))\n        self.both_pooling_neck = torch.nn.Sequential(torch.nn.BatchNorm1d(2*in_features),\n                                                     torch.nn.Linear(in_features=2*in_features, out_features=in_features))\n        \n        self.head = torch.nn.Sequential(\n            torch.nn.BatchNorm1d(in_features),\n            torch.nn.Linear(in_features=in_features, out_features=256),\n            torch.nn.Hardswish(inplace=True),torch.nn.Dropout(0.1),\n            torch.nn.Linear(in_features=256, out_features=len(LABELS))  \n        )\n\n\n\n        self.active = torch.nn.Sigmoid()\n    def forward(self, x):\n        x = x.expand(-1, 3, -1, -1)\n        x = self.normalize(x)\n        x = self.backbone(x)\n\n        if self.pool == \"max\":\n            x = self.max_pooling(x)\n        elif self.pool == \"avg\":\n            x = self.avg_pooling(x)\n        elif self.pool == \"both\":\n            x_max = self.max_pooling(x)\n            x_avg = self.avg_pooling(x)\n            x = x_max + x_avg\n            # x = torch.cat([x_max, x_avg], dim=1)\n            # x = self.both_pooling_neck(x)\n            \n        x = self.head(x)\n        # x = self.active(x)\n        return x","metadata":{"papermill":{"duration":0.048113,"end_time":"2024-05-27T08:24:07.883643","exception":false,"start_time":"2024-05-27T08:24:07.83553","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">11 | Stratified k-Fold Cross-Validation and Random Seed Setting 🐦</p>\n\n\n1. **Stratified K-Fold Cross-Validation**:\n   - Stratified K-Fold cross-validation is a technique used to evaluate the performance of a machine learning model. It ensures that each fold of the dataset has approximately the same proportion of samples from each class, which is particularly useful for imbalanced datasets.\n   - `StratifiedKFold` is a class from the scikit-learn library that splits a dataset into K folds while preserving the percentage of samples for each class.\n   - In the `for` loop:\n       - `skf.split(train_csv, train_csv['primary_label'])` splits the dataset (`train_csv`) into train and validation sets for each fold, ensuring that each fold maintains the same distribution of classes as the original dataset.\n       - `train_index` and `valid_index` contain the indices of samples for the training and validation sets for the current fold.\n       - `enumerate(skf.split(...))` iterates over each fold, providing the fold index (`fold`) and the corresponding train/validation indices.\n       - `train_csv.loc[valid_index, 'fold'] = int(fold)` assigns the fold index to the validation samples in the `fold` column of the DataFrame `train_csv`, indicating which fold each sample belongs to.\n\n2. **Random Seed Setting**:\n   - Setting random seeds ensures reproducibility of results in machine learning experiments. It initializes the random number generators with a fixed seed, so the same sequence of random numbers is generated every time the code is run.\n   - `set_random_seed` is a function defined to set the random seed across different random number generators used in the experiment.\n   - Inside the function:\n       - `random.seed(seed)`, `np.random.seed(seed)`, and `os.environ[\"PYTHONHASHSEED\"] = str(seed)` set the random seed for the Python built-in random number generator, NumPy, and hash randomization, respectively.\n       - `torch.manual_seed(seed)` sets the random seed for the PyTorch library for CPU operations.\n       - `torch.cuda.manual_seed(seed)` sets the random seed for GPU operations in PyTorch.\n       - `torch.backends.cudnn.deterministic = deterministic` ensures deterministic behavior of CuDNN (CUDA Deep Neural Network library) for GPU operations in PyTorch, which can affect the performance but ensures reproducibility.\n\n","metadata":{"papermill":{"duration":0.025698,"end_time":"2024-05-27T08:24:07.936261","exception":false,"start_time":"2024-05-27T08:24:07.910563","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nskf = StratifiedKFold(n_splits=cfg.nfolds, shuffle=True, random_state=cfg.seed)\nfor fold, (train_index, valid_index) in enumerate(skf.split(train_csv, train_csv['primary_label'])):\n    train_csv.loc[valid_index, 'fold'] = int(fold)\n    \n    \nif isTrain:\n    train_csv.groupby(\"fold\", as_index=False)[\"primary_label\"].value_counts()   \n    \n    \ndef set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = deterministic  # type: ignore    ","metadata":{"papermill":{"duration":0.082794,"end_time":"2024-05-27T08:24:08.04628","exception":false,"start_time":"2024-05-27T08:24:07.963486","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\"> 12 | BCEFocalLoss: Binary Cross-Entropy Focal Loss 🐦</p>\n\n\n1. **BCEFocalLoss Class**:\n    - This class defines a custom loss function called `BCEFocalLoss` for binary classification tasks.\n    - It inherits from `nn.Module`, indicating that it's a PyTorch module.\n\n2. **Initialization**:\n    - The `__init__` method initializes the loss function with two parameters: `alpha` and `gamma`.\n    - `alpha` (default value: 0.25) controls the balance between positive and negative class samples in the loss calculation.\n    - `gamma` (default value: 2.0) controls the degree of focus on hard-to-classify examples.\n\n3. **Forward Method**:\n    - The `forward` method computes the loss given model predictions (`preds`) and ground truth labels (`targets`).\n    - It first calculates the binary cross-entropy (BCE) loss using `nn.BCEWithLogitsLoss` with the option `reduction='none'` to compute the loss per sample without averaging.\n    - `probas = torch.sigmoid(preds)` computes the sigmoid activation of the model predictions to obtain probabilities.\n\n4. **Focal Loss Components**:\n    - Focal loss introduces two additional components: focal term (`tmp`) and smooth term (`smp`).\n    - `tmp` calculates the focal loss for positive class samples, where the focus is increased for misclassified samples (`(1. - probas)**self.gamma` increases the loss for hard-to-classify examples).\n    - `smp` calculates the focal loss for negative class samples, focusing on correctly classified samples (`probas**self.gamma` increases the loss for hard-to-classify examples).\n    - Both `tmp` and `smp` are multiplied by the BCE loss to incorporate the original loss calculation.\n\n5. **Final Loss Calculation**:\n    - The final loss is calculated as the sum of `tmp` and `smp`, followed by taking the mean over all samples.\n    - This mean loss value is returned as the output of the `forward` method.\n\n","metadata":{"papermill":{"duration":0.025938,"end_time":"2024-05-27T08:24:08.098639","exception":false,"start_time":"2024-05-27T08:24:08.072701","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class BCEFocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, preds, targets):\n        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(preds, targets)\n        probas = torch.sigmoid(preds)\n\n        \n\n        tmp = targets * self.alpha * (1. - probas)**self.gamma * bce_loss\n        smp = (1. - targets) * probas**self.gamma * bce_loss\n        \n        loss = tmp + smp\n        loss = loss.mean()\n        return loss","metadata":{"papermill":{"duration":0.039605,"end_time":"2024-05-27T08:24:08.164291","exception":false,"start_time":"2024-05-27T08:24:08.124686","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">13 | Initialization Function For Training </p>\n\n1. **Model Initialization**:\n   - The function initializes the neural network model using the `BirdModel` class, which is customized for bird sound classification.\n   - It sets parameters such as the model architecture (`model_name`), whether to use pre-trained weights (`pretrained`), number of input channels (`in_channels`), number of output classes (`num_classes`), and pooling type (`pool`).\n\n2. **Optimizer Selection**:\n   - Depending on the configuration (`cfg.optimizer`), the function selects the optimizer for training.\n   - If `cfg.optimizer` is set to `'adan'`, it uses the custom optimizer `Adan` with specific parameters like learning rate (`lr`), betas, and weight decay.\n   - Otherwise, it uses the standard AdamW optimizer from PyTorch with parameters such as learning rate (`lr`) and weight decay.\n\n3. **Learning Rate Scheduler**:\n   - The function initializes a learning rate scheduler using `torch.optim.lr_scheduler.OneCycleLR`.\n   - This scheduler adjusts the learning rate during training, starting from an initial value (`cfg.lr`), and following a one-cycle policy with specified parameters such as the maximum number of epochs (`cfg.max_epoch`), percentage of epochs to increase/decrease learning rate (`pct_start`), and step size (`steps_per_epoch`).\n\n4. **Gradient Scaler (Automatic Mixed Precision)**:\n   - Automatic Mixed Precision (AMP) is a technique that combines single and half-precision floating-point arithmetic to speed up training while maintaining numerical stability.\n   - The function initializes a gradient scaler using `amp.GradScaler` with the option to enable or disable AMP based on the configuration (`cfg.enable_amp`).\n\n5. **Loss Function Initialization**:\n   - Depending on the loss type specified in the configuration (`cfg.loss_type`), the function initializes the loss function.\n   - If `cfg.loss_type` is set to `\"BCEWithLogitsLoss\"`, it uses the binary cross-entropy loss with logits (`torch.nn.BCEWithLogitsLoss`).\n   - If `cfg.loss_type` is set to `\"BCEFocalLoss\"`, it uses the custom focal loss function `BCEFocalLoss` with a specified `alpha` value.\n\n6. **Returning Initialized Components**:\n   - The function returns the initialized model, optimizer, scheduler, scaler, and loss function, all moved to the appropriate device (`device`), typically GPU.\n\n","metadata":{"papermill":{"duration":0.025553,"end_time":"2024-05-27T08:24:08.215738","exception":false,"start_time":"2024-05-27T08:24:08.190185","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def initialization():\n    model = BirdModel(model_name=cfg.model_name, pretrained=True, in_channels=3, num_classes=len(LABELS), pool=cfg.pool_type)\n    \n    if cfg.optimizer=='adan':\n        optimizer = Adan(model.parameters(), lr=cfg.lr, betas=(0.02, 0.08, 0.01), weight_decay=cfg.weight_decay)\n    else:\n        optimizer = torch.optim.AdamW(params=model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n    \n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer=optimizer, epochs=cfg.max_epoch,\n        pct_start=0.0, steps_per_epoch=len(train_dataloader),\n        max_lr=cfg.lr, div_factor=25, final_div_factor=4.0e-01\n    )\n    \n    scaler = amp.GradScaler(enabled=cfg.enable_amp)\n    if cfg.loss_type == \"BCEWithLogitsLoss\":\n        loss_func = torch.nn.BCEWithLogitsLoss()\n    elif cfg.loss_type == \"BCEFocalLoss\":\n        loss_func = BCEFocalLoss(alpha=1)\n    \n    \n\n\n    return model.to(device), optimizer, scheduler, scaler, loss_func.to(device)","metadata":{"papermill":{"duration":0.041677,"end_time":"2024-05-27T08:24:08.283638","exception":false,"start_time":"2024-05-27T08:24:08.241961","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">14 | Training And Evaluation Functions 🐦</p>\n\n1. **`train_one_loop` Function**:\n   - This function performs one epoch of training.\n   - It iterates through the training data (`dataloader`) and updates the model parameters based on the calculated loss.\n   - Within each iteration:\n     - The data and labels are moved to the appropriate device (`device`).\n     - The gradients are zeroed using `optimizer.zero_grad()` to clear the previous gradients.\n     - Inside the training loop, gradient scaling is applied using AMP (Automatic Mixed Precision) to improve numerical stability and speed up training.\n     - The loss is computed using the specified loss function (`loss_fn`) and backpropagated through the network.\n     - The optimizer's learning rate is adjusted using the scheduler (`scheduler.step()`).\n     - The loss value is accumulated for monitoring training progress.\n   - After processing all batches, the average training loss is calculated and logged (if using Weights & Biases for logging).\n\n2. **`mixup_one_loop` Function**:\n   - This function performs one epoch of training with mixup augmentation.\n   - Mixup is a data augmentation technique that blends pairs of examples and their corresponding labels.\n   - It follows a similar structure to `train_one_loop`, but before feeding the data to the model, it applies mixup augmentation based on a probability threshold (`cfg.aug_spec_mixup_prob`).\n   - Mixup can be applied either on different waveforms (`\"other_wave\"`) or on spectrograms (`\"spec_mixup\"`).\n   - The rest of the process, including loss computation and optimization, remains the same.\n\n3. **`evaluate_validation` Function**:\n   - This function evaluates the model on the validation dataset (`dataloader`) after each epoch of training.\n   - It calculates validation loss and various evaluation metrics such as AUC (Area Under the ROC Curve), F1-score, precision, and more.\n   - Inside the evaluation loop:\n     - The model makes predictions on the validation data.\n     - Predictions are compared with the ground truth labels to compute the evaluation metrics.\n     - The validation loss is computed using the specified loss function.\n   - The function returns the validation loss and evaluation metrics, which can be used for monitoring the model's performance during training.\n\nThese functions collectively handle the training and evaluation process of the bird sound classification model, including data processing, model training, and performance evaluation. Additionally, they provide flexibility in choosing different training strategies such as mixup augmentation and support for monitoring training progress using Weights & Biases.","metadata":{"papermill":{"duration":0.026164,"end_time":"2024-05-27T08:24:08.335485","exception":false,"start_time":"2024-05-27T08:24:08.309321","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def train_one_loop(model, optimizer, scaler, scheduler, dataloader, loss_fn):\n    trainloss = 0; model.train()\n\n    count = 0\n    for idx, (data, label) in enumerate(tqdm(dataloader,leave=False ,desc=\"[train]\")):\n        # label = label.reshape(-1, len(LABELS))\n        \n        data, label = data.to(device), label.to(device)\n        \n        optimizer.zero_grad()\n        with amp.autocast(cfg.enable_amp, dtype=torch.bfloat16):\n        # with amp.autocast(cfg.enable_amp):\n            pred = model.forward(data)\n            loss = loss_fn(pred, label)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        scheduler.step()\n        \n        trainloss += loss.item()\n        # print(idx, loss.item())\n        # if cfg.wandb == True:\n        #     wandb.log({f\"train_loss\": loss.item(), f\"lr\":scheduler.get_lr()[0]})\n        del data, label, loss\n        count += 1\n        # if count == 300:\n        # break\n    trainloss /= len(dataloader)\n    if cfg.wandb == True:\n        wandb.log({f\"train_loss\": trainloss, f\"lr\":scheduler.get_lr()[0]})\n    return model, optimizer, scaler, scheduler, trainloss\n\n\ndef mixup_one_loop(model, optimizer, scaler, scheduler, dataloader, loss_fn):\n    trainloss = 0; model.train()\n\n    count = 0\n    for idx, (data, label) in enumerate(tqdm(dataloader,leave=False ,desc=\"[train]\")):\n        if np.random.random()>cfg.aug_spec_mixup_prob:\n            data, label = mixup(data=data, targets=label, alpha=cfg.alpha, mode=\"other_wave\")\n        else:\n            data, label = spec_mixup(data=data, targets=label)\n        data, label = data.to(device), label.to(device)\n        \n        optimizer.zero_grad()\n        with amp.autocast(cfg.enable_amp, dtype=torch.bfloat16):\n        # with amp.autocast(cfg.enable_amp):\n            pred = model.forward(data)\n            loss = loss_fn(pred, label)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        scheduler.step()\n        \n        trainloss += loss.item()\n        # print(idx, loss.item())\n        # if cfg.wandb == True:\n        #     wandb.log({f\"lr\":scheduler.get_lr()[0]})\n        del data, label, loss\n        count += 1\n        # if count == 300:\n        # break\n    trainloss /= len(dataloader)\n    if cfg.wandb == True:\n        wandb.log({f\"train_loss\": trainloss, f\"lr\":scheduler.get_lr()[0]})\n    return model, optimizer, scaler, scheduler, trainloss\n\n\ndef evaluate_validation(model, dataloader, loss_fn):\n    validloss=0\n    model.eval()\n\n    preds, trues, targets = [], [], []\n    \n    for idx, (data, label) in enumerate(tqdm(dataloader,leave=False ,desc=\"[valid]\")):\n        # label = label.reshape(-1, len(LABELS))\n\n        d = data[0].unsqueeze(1)\n        label = label[0]\n        \n        d = d.to(device)\n        # with amp.autocast(cfg.enable_amp):\n        pred = model.forward(d)\n\n        preds.extend(pred.detach().cpu())\n        trues.extend(label)\n        targets.extend(label.argmax(axis=1))\n        \n    #======================== metrics ========================#\n    # y_preds = torch.stack(preds)\n    t = torch.stack(preds)\n    t = torch.sigmoid(t)\n    targets = torch.tensor(targets)\n    y_trues = torch.stack(trues)\n\n\n    validloss = loss_fn(torch.stack(preds), torch.stack(trues))\n    #     # print(idx, loss)\n    #     # wandb.log({\"valid_loss\": loss})\n\n    # validloss /= len(dataloader)\n    \n    # sk_f1 = metrics.f1_score(np.array(y_trues), np.array(t), average=\"micro\")\n    sk_f1_30 = metrics.f1_score(np.array(y_trues), np.array(t) > 0.30, average=\"micro\")\n    sk_f1_50 = metrics.f1_score(np.array(y_trues), np.array(t) > 0.50, average=\"micro\")\n    \n    auc = multiclass_auroc(input=t, target=targets, num_classes=len(LABELS),\n                           average=\"macro\").item()\n\n    # auc_micro = multiclass_auroc(input=t, target=targets, num_classes=len(LABELS),\n    #                        average=\"none\").item()\n\n    prec = multiclass_precision(input=t, target=targets, num_classes=len(LABELS),\n                           average=\"macro\").item()\n    # rec = multiclass_recall(input=t, target=targets, num_classes=len(LABELS),\n    #                        average=\"macro\").item()\n    \n    # acc = multilabel_accuracy(input=t, target=targets).item()\n\n    f1 = multiclass_f1_score(input=t, target=torch.tensor(targets), num_classes=len(LABELS),\n                             average=\"micro\").item()\n\n    f1_macro = multiclass_f1_score(input=t, target=torch.tensor(targets), num_classes=len(LABELS),\n                             average=\"macro\").item()\n\n    t_03 = (t>0.3).int()\n    t_03 = torch.tensor(t_03, dtype=torch.int64)\n    f1_03 = multiclass_f1_score(input=t_03, target=torch.tensor(targets), num_classes=len(LABELS), \n                                average=\"micro\").item()\n\n    t_05 = (t>0.5).int()\n    t_05 = torch.tensor(t_05, dtype=torch.int64)\n    f1_05 = multiclass_f1_score(input=t_05, target=torch.tensor(targets), num_classes=len(LABELS), \n                                average=\"micro\").item()\n\n    if cfg.wandb == True:\n        wandb.log({f\"valid_loss\": validloss,\n                   f\"AUC\":auc,\n                   # \"auc_micro\":auc_micro,\n                   \"precision\":prec, \n                   # \"recall\":rec, \n                   # \"accuracy\":acc,\n                   f\"F1\":f1,\n                   \"F1_macro\":f1_macro,\n                   f\"F1 30%\":f1_03,\n                   f\"F1 50%\":f1_05})\n    return validloss, auc, f1, f1_03, f1_05, sk_f1_30, sk_f1_50","metadata":{"papermill":{"duration":0.063951,"end_time":"2024-05-27T08:24:08.425685","exception":false,"start_time":"2024-05-27T08:24:08.361734","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if isTrain == True:\n    tmp_params = dict(vars(config))\n    del tmp_params['__module__'],tmp_params['__dict__'],tmp_params['__weakref__'],tmp_params['__doc__']\n\ndef get_oversampled_df(df):\n    \n    new_df = [df]\n\n    low_sample_birds = df[\"primary_label\"].value_counts()[df[\"primary_label\"].value_counts() < cfg.oversample_threthold].index\n    for bird in low_sample_birds:\n        tmp = df[df[\"primary_label\"] == bird]\n        data_num = len(tmp)\n    \n        tiles = 1 + cfg.oversample_threthold // data_num\n    \n        tile_df = []\n        for i in range(tiles):\n            tile_df.append(tmp)\n    \n        tiled_df = pd.concat(tile_df)\n        piece = tiled_df[data_num:cfg.oversample_threthold]\n        new_df.append(piece)\n    \n    return pd.concat(new_df)","metadata":{"papermill":{"duration":0.042146,"end_time":"2024-05-27T08:24:08.546191","exception":false,"start_time":"2024-05-27T08:24:08.504045","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if isTrain == True:\n    set_random_seed(seed=42)\n    \n    \n    if cfg.wandb == True:\n        wandb.init(project='BirdCLEF_cv_ver2', name=f\"{name}\",\n                   config=tmp_params)\n        \n    # for fold in range(cfg.nfolds):\n    for fold in cfg.inference_folds:\n        train_ = train_csv.loc[train_csv[\"fold\"]!=fold]\n\n        if cfg.oversample == True:\n            train = get_oversampled_df(df=train_)\n        else:\n            train = train_\n        \n        augme_dataset = BirdCLEF_Dataset(df=train, augmentation=True, mode='train')\n        augme_dataloader = torch.utils.data.DataLoader(dataset=augme_dataset, batch_size=cfg.train_batchsize, shuffle=True)\n\n        train_dataset = BirdCLEF_Dataset(df=train, augmentation=False, mode='train')\n        train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=cfg.train_batchsize, shuffle=True)\n        \n        valid = train_csv.loc[train_csv[\"fold\"]==fold]\n        valid_dataset = BirdCLEF_Dataset(df=valid, augmentation=False, mode='valid')\n        valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=cfg.valid_batchsize, shuffle=False)\n    \n        model, optimizer, scheduler, scaler, loss_func =  initialization()\n    \n    \n        best_f1 = 0\n        best_auc = 0\n        best_loss = 1.00000\n        for e in range(cfg.max_epoch):\n            start_time = time.time()\n            if e < cfg.aug_epoch:\n                if cfg.aug_spec_mixup > np.random.random():\n                    model, optimizer, scaler, shcheduler, train_loss = mixup_one_loop(model=model,optimizer=optimizer,scaler=scaler, \n                                                                                          scheduler=scheduler,dataloader=augme_dataloader, loss_fn=loss_func)\n                else:\n                    model, optimizer, scaler, shcheduler, train_loss = train_one_loop(model=model,optimizer=optimizer,scaler=scaler, \n                                                                                          scheduler=scheduler,dataloader=augme_dataloader, loss_fn=loss_func)\n\n            else:\n                model, optimizer, scaler, shcheduler, train_loss = train_one_loop(model=model,optimizer=optimizer,scaler=scaler, \n                                                                                          scheduler=scheduler,dataloader=train_dataloader, loss_fn=loss_func)\n            \n            valid_loss, auc, f1, f1_03, f1_05, sk_f1_30, sk_f1_50 = evaluate_validation(model=model, dataloader=valid_dataloader, loss_fn=loss_func)\n            # print(f\"epoch {e} , train_loss is {train_loss}, valid_loss is {valid_loss}\")\n            \n            if best_loss > valid_loss:\n                end_time = time.time()\n                print(f\"[epoch {str(e).zfill(2)}] AUC{auc: .4f}, F1{f1: .4f}, F1_03{f1_03: .4f}, F1_05{f1_05: .4f}\")\n                print(f\"[epoch {str(e).zfill(2)}] SKF1_03{sk_f1_30: .4f}, SKF1_05{sk_f1_50: .4f}\")\n                print(f\"[epoch {str(e).zfill(2)}] valid_loss {valid_loss: .6f}\")\n                print(f\"[epoch {str(e).zfill(2)}] update loss {best_loss: .6f} --> {valid_loss: .6f} {(end_time - start_time): .1f}[s]\")\n                print(f\"[epoch {str(e).zfill(2)}] update auc score {best_auc: .6f} --> {auc: .6f} {(end_time - start_time): .1f}[s]\")\n                model_name = f'{name}/checkpoint/fold_{fold}_snapshot_epoch_{str(e).zfill(2)}.pth'\n                best_model = model\n                best_loss = valid_loss\n                best_auc = auc\n                best_f1 = f1\n            else:\n                end_time = time.time()\n                print(f\"[epoch {str(e).zfill(2)}] NOT update loss {best_loss: .6f} <-- {valid_loss: .6f} {(end_time - start_time): .1f}[s]\")\n                print(f\"[epoch {str(e).zfill(2)}] NOT update score {best_auc: .6f} <-- {auc: .6f} {(end_time - start_time): .1f}[s]\")\n\n        if cfg.wandb == True:\n            wandb.log({f\"best_loss\": best_loss,\n                       f\"best_f1\": best_f1,\n                       f\"best_auc\":best_auc})\n\n        torch.save(best_model.state_dict(), model_name)\n        \n        del model, best_model\n        gc.collect()\n        torch.cuda.empty_cache()\n        print(\"--\")\n# print(best_auc)\nprint(\"1\")\n","metadata":{"papermill":{"duration":0.05756,"end_time":"2024-05-27T08:24:08.683435","exception":false,"start_time":"2024-05-27T08:24:08.625875","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">15 | Model Loading For Inference 🐦</p>\n\n1. **Initializing Dictionaries:**\n   - Two dictionaries, `models` and `models_names`, are initialized to store loaded models and their corresponding names, respectively.\n\n2. **Iterating Over Folds:**\n   - The code iterates over each fold for which inference is required (`for fold in cfg.inference_folds:`).\n\n3. **Loading Trained Model:**\n   - The path to the best-performing model checkpoint for the current fold is obtained using `glob.glob`.\n   - The `BirdModel` class is instantiated to create a new model instance with the same architecture as the trained model.\n   - The model's state dictionary is loaded from the saved checkpoint file using `torch.load`.\n   - The loaded model is set to evaluation mode using `model.eval()`.\n\n4. **Storing Loaded Models and Names:**\n   - The loaded model is stored in the `models` dictionary with the fold index as the key.\n   - The name of the ONNX file for the model is generated from the checkpoint file path and stored in the `models_names` dictionary.\n\n5. **Printing Model Path and ONNX Name:**\n   - The path of the loaded model checkpoint file and the corresponding ONNX file name are printed for verification.\n\n","metadata":{"papermill":{"duration":0.025863,"end_time":"2024-05-27T08:24:08.735311","exception":false,"start_time":"2024-05-27T08:24:08.709448","status":"completed"},"tags":[]}},{"cell_type":"code","source":"models = dict()\nmodels_names = dict()\n# for fold in range(cfg.nfolds):\nfor fold in cfg.inference_folds:\n    bestmodel_path = sorted(glob.glob(f\"/kaggle/input/{name}/checkpoint/fold_{fold}*.pth\"))[-1]\n\n    print(bestmodel_path)\n    model = BirdModel(model_name=cfg.model_name, pretrained=False, in_channels=1, num_classes=len(LABELS))\n    model.load_state_dict(torch.load(bestmodel_path, map_location=torch.device('cpu')))\n    model = model.eval()\n    models[fold] = model\n\n    models_names[fold] = bestmodel_path.split(\".\")[0]+\".onnx\"\n    print(models_names[fold])","metadata":{"papermill":{"duration":0.614286,"end_time":"2024-05-27T08:24:09.375315","exception":false,"start_time":"2024-05-27T08:24:08.761029","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest_audio_dir = f\"{cfg.dir}test_soundscapes/\"\nfile_list = glob.glob(test_audio_dir+\"*.ogg\")\nfile_list = sorted(file_list)\n\n\ntest_dataset = BirdCLEF_Dataset(df=file_list, mode=\"test\")\ntest_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                              batch_size=1, \n                                              shuffle=False)\n\ninput_tensor = torch.randn((48, 1, cfg.n_mels, cfg.size_x+1))  # input shape\noutput_names=['output']\ninput_names=[\"x\"]\n\n\n# models_names = []\nmodels_names = dict()\n# for fold in range(cfg.nfolds):\nfor fold in cfg.inference_folds:\n    onnxmodel_path = sorted(glob.glob(f\"/kaggle/input/{name}/checkpoint/fold_{fold}*.onnx\"))[-1]\n\n    print(onnxmodel_path)\n\n    models_names[fold] = onnxmodel_path\n    \n    \nonnx_sessions = dict()\n# for fold in range(cfg.nfolds):\nfor fold in cfg.inference_folds:\n\n    onnx_model = onnx.load(models_names[fold])\n    onnx_model_graph = onnx_model.graph\n    onnx_session = ort.InferenceSession(onnx_model.SerializeToString())\n\n    onnx_sessions[fold] = onnx_session    ","metadata":{"papermill":{"duration":0.449231,"end_time":"2024-05-27T08:24:09.902461","exception":false,"start_time":"2024-05-27T08:24:09.45323","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\npredictions = []\nfor data in tqdm(test_dataloader):\n    \n    preds = []\n    \n#     for fold, session in enumerate(onnx_sessions):\n    for fold in cfg.inference_folds:\n        session = onnx_sessions[fold]\n        pred = session.run(output_names, {input_names[0]: data[0].numpy()})[0]\n        \n        pred = torch.sigmoid(torch.tensor(pred))\n        preds.append(pred)\n    preds_per_batch = torch.stack(preds, axis=0).mean(axis=0)\n    \n    predictions.extend(preds_per_batch)\n    \nif len(predictions)>0:\n    predictions = torch.stack(predictions)\nelse:\n    predictions = predictions\nend_time = time.time()\nuse_time = end_time - start_time","metadata":{"papermill":{"duration":0.069935,"end_time":"2024-05-27T08:24:10.05298","exception":false,"start_time":"2024-05-27T08:24:09.983045","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"font-family: 'Amiri'; font-size: 3rem; color: Black; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3); background-color: #c9b68b; padding: 20px; border-radius: 20px; border: 7px solid Black; width:95%\">16 | Final Submission 📝</p>\n\n\n","metadata":{"papermill":{"duration":0.025943,"end_time":"2024-05-27T08:24:10.105111","exception":false,"start_time":"2024-05-27T08:24:10.079168","status":"completed"},"tags":[]}},{"cell_type":"code","source":"bird_cols = sample_submission.columns[1:]\ndf = pd.DataFrame(columns=['row_id']+list(bird_cols))\n\n\nrow_list = []\nfor file in file_list:\n    dataname = file.split(\"/\")[-1][:-4]\n    for i in range(int(4*60/5)):\n        row = f\"{dataname}_{(i+1)*5}\"\n        row_list.append(row)\n        \n        \n        \ndf['row_id'] = row_list        \n\nif len(predictions) < 1:\n    pass\nelse:\n    df[bird_cols] = predictions\n    \n    \ndf.to_csv(\"submission1.csv\", index=False)     ","metadata":{"papermill":{"duration":0.055183,"end_time":"2024-05-27T08:24:10.186739","exception":false,"start_time":"2024-05-27T08:24:10.131556","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## another","metadata":{}},{"cell_type":"code","source":"import sys, os\nsys.path.append('/kaggle/input/efficientnet-keras-dataset/efficientnet_kaggle')\n!pip install -q /kaggle/input/tensorflow-extra-lib-ds/tensorflow_extra-1.0.2-py3-none-any.whl --no-deps","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntf.get_logger().setLevel('ERROR')\ntf.autograph.set_verbosity(0)\nimport os\nimport pandas as pd\nimport numpy as np\nimport random\nfrom glob import glob\nfrom tqdm import tqdm\ntqdm.pandas()\nimport gc\nimport librosa\nimport sklearn\nimport time\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport librosa.display as lid\nimport IPython.display as ipd\n\nimport tensorflow as tf\ntf.config.optimizer.set_jit(True) # enable xla for speed up\nimport tensorflow_io as tfio\nimport tensorflow.keras.backend as K\n\nimport efficientnet.tfkeras as efn\nimport tensorflow_extra as tfe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    debug = False\n    verbose = 0\n    \n    device = 'CPU'\n    seed = 42\n    \n    # Input image size and batch size\n    img_size = [128, 384]\n    batch_size = 16\n    infer_bs = 2\n    tta = 1\n    drop_remainder = True\n    \n    # STFT parameters\n    duration = 5 # duration for test\n    train_duration = 10\n    sample_rate = 32000\n    downsample = 1\n    audio_len = duration*sample_rate\n    nfft = 2028\n    window = 2048\n    hop_length = train_duration*32000 // (img_size[1] - 1)\n    fmin = 20\n    fmax = 16000\n    normalize = True\n\n    # Data Preprocessing Settings\n    class_names = sorted(os.listdir('/kaggle/input/birdclef-2024/train_audio/'))\n    num_classes = len(class_names)\n    class_labels = list(range(num_classes))\n    label2name = dict(zip(class_labels, class_names))\n    name2label = {v:k for k,v in label2name.items()}\n    \n    target_col = ['target']\n    tab_cols = ['filename','common_name','rate']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.set_random_seed(CFG.seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_device():\n    \"Detect and intializes GPU/TPU automatically\"\n    # Check TPU category\n    tpu = 'local' if CFG.device=='TPU-VM' else None\n    try:\n        # Connect to TPU\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=tpu) \n        # Set TPU strategy\n        strategy = tf.distribute.TPUStrategy(tpu)\n        print(f'> Running on {CFG.device} ', tpu.master(), end=' | ')\n        print('Num of TPUs: ', strategy.num_replicas_in_sync)\n        device=CFG.device\n    except:\n        # If TPU is not available, detect GPUs\n        gpus = tf.config.list_logical_devices('GPU')\n        ngpu = len(gpus)\n         # Check number of GPUs\n        if ngpu:\n            # Set GPU strategy\n            strategy = tf.distribute.MirroredStrategy(gpus) # single-GPU or multi-GPU\n            # Print GPU details\n            print(\"> Running on GPU\", end=' | ')\n            print(\"Num of GPUs: \", ngpu)\n            device='GPU'\n        else:\n            # If no GPUs are available, use CPU\n            print(\"> Running on CPU\")\n            strategy = tf.distribute.get_strategy()\n            device='CPU'\n    return strategy, device, tpu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize GPU/TPU/TPU-VM\nstrategy, CFG.device, tpu = get_device()\nCFG.replicas = strategy.num_replicas_in_sync","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/birdclef-2024'\nGCS_PATH = BASE_PATH","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_audio_dir = '/kaggle/input/birdclef-2024/test_soundscapes/'\n\ntest_paths = [test_audio_dir+f for f in sorted(os.listdir(test_audio_dir))]\nif len(test_paths)==1:\n    test_audio_dir = '/kaggle/input/birdclef-2024/unlabeled_soundscapes/'\n\n    test_paths = [test_audio_dir+f for f in sorted(os.listdir(test_audio_dir))][:2]\n    \ntest_df = pd.DataFrame(test_paths, columns=['filepath'])\ntest_df['filename'] = test_df.filepath.map(lambda x: x.split('/')[-1].replace('.ogg',''))\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.io.gfile.exists(test_df.filepath.iloc[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_audio(filepath, sr=32000, normalize=True):\n    audio, orig_sr = librosa.load(filepath, sr=None)\n    if sr!=orig_sr:\n        audio = librosa.resample(y, orig_sr, sr)\n    audio = audio.astype('float32').ravel()\n    audio = tf.convert_to_tensor(audio)\n    return audio\n\n@tf.function(jit_compile=True)\ndef MakeFrame(audio, duration=5, sr=32000):\n    frame_length = int(duration * sr)\n    frame_step = int(duration * sr)\n    chunks = tf.signal.frame(audio, frame_length, frame_step, pad_end=True)\n    return chunks","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_audio(row):\n    # Caption for viz\n    caption = f'Id: {row.filename}'\n    # Read audio file\n    audio = load_audio(row.filepath)\n    # Keep fixed length audio\n    audio = audio[:CFG.audio_len]\n    # Display audio\n    print(\"# Audio:\")\n    display(ipd.Audio(audio.numpy(), rate=CFG.sample_rate))\n    print('# Visualization:')\n    plt.figure(figsize=(12, 3))\n    plt.title(caption)\n    # Waveplot\n    lid.waveshow(audio.numpy(),\n                 sr=CFG.sample_rate,)\n                 \n    plt.xlabel('');\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display_audio(test_df.iloc[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n# Directory of checkpoint\nCKPT_DIR = '/kaggle/input/birdclef2024training'\n\n# Get file paths of all trained models in the directory\nCKPT_PATHS = sorted([x for x in glob(f'{CKPT_DIR}/fold-*keras')])\nprint(\"Checkpoints: \", CKPT_PATHS)\n\n\n# Define a writable directory\nWRITABLE_DIR = '/kaggle/working/models/'\n\n# Create the writable directory if it does not exist\nif not os.path.exists(WRITABLE_DIR):\n    os.makedirs(WRITABLE_DIR)\n\n\n# Copy the model files to the writable directory\nfor ckpt_path in CKPT_PATHS:\n    shutil.copy(ckpt_path, WRITABLE_DIR)\n\n# Update the checkpoint paths to the writable directory\nCKPT_PATHS = sorted([f'{WRITABLE_DIR}/{os.path.basename(x)}' for x in glob(f'{CKPT_DIR}/fold-*keras')])\n\n# Load all the models in memory to speed up\nCKPTS = [tf.keras.models.load_model(x, compile=False) for x in tqdm(CKPT_PATHS, desc=\"Loading ckpts \")]\n# Num of ckpt to use\nNUM_CKPTS = 1\n\n# Submit or Interactive mode\n#SUBMIT = pd.read_csv('/kaggle/input/birdclef-2024/sample_submission.csv').shape[0] != 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start stopwatch\ntick = time.time()\n\n# Initialize empty list to store ids\nids = []\n# Initialize empty array to store predictions\npreds = np.empty(shape=(0, 182), dtype='float32')\n\n# Iterate over each audio file in the test dataset\nfor filepath in tqdm(test_df.filepath.tolist(), 'test '):\n    # Extract the filename without the extension\n    filename = filepath.split('/')[-1].replace('.ogg','')\n    \n    # Load audio from file and create audio frames, each recording will be a batch input\n    audio = load_audio(filepath)\n    chunks = MakeFrame(audio)\n    \n    # Predict bird species for all frames in a recording using all trained models\n    chunk_preds = np.zeros(shape=(len(chunks), 182), dtype=np.float32)\n    for model in CKPTS[:NUM_CKPTS]:\n        # Get the model's predictions for the current audio frames\n        rec_preds = model(chunks, training=False).numpy()\n        # Ensemble all prediction with average\n        chunk_preds += rec_preds/len(CKPTS)\n    \n    # Create a ID for each frame in a recording using the filename and frame number\n    rec_ids = [f'{filename}_{(frame_id+1)*5}' for frame_id in range(len(chunks))]\n    \n    # Concatenate the ids\n    ids += rec_ids\n    # Concatenate the predictions\n    preds = np.concatenate([preds, chunk_preds], axis=0)\n    \n# Stop stopwatch\ntock = time.time()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submit prediction\npred_df = pd.DataFrame(ids, columns=['row_id'])\npred_df.loc[:, CFG.class_names] = preds\npred_df.to_csv('submission2.csv',index=False)\npred_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 合并数据","metadata":{}},{"cell_type":"code","source":"import glob\n\nfolder_path = \"/kaggle/working/\"\noutput_path = \"/kaggle/working/\"\nfile_pattern = \"submission*.csv\"\n\nfile_paths = glob.glob(os.path.join(folder_path, file_pattern))\n\ncombined_data = pd.DataFrame()\n\nfor file_path in file_paths:\n    df = pd.read_csv(file_path)\n    df_filtered = df[df != 0]\n    combined_data = pd.concat([combined_data, df_filtered])\n    \naverage_values = combined_data.groupby('row_id').mean()\n\noutput_file = \"submission.csv\"\naverage_values.to_csv(os.path.join(output_path,output_file), index=True)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"average_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_time = (tock-tick)*550 # ~1100 recording on the test data\nsub_time = time.gmtime(sub_time)\nsub_time = time.strftime(\"%H hr: %M min : %S sec\", sub_time)\nprint(f\">> Time for submission: ~ {sub_time}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}